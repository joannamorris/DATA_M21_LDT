---
title: "m21_202303"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output: pdf
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Compute PCA

Following Andrews and Lo (2013) this script computes a PCA for our spelling and vocabulary measures. Because the standardised spelling and vocabulary scores were  correlated, to facilitate interpretation, two orthogonal measures of individual differences were derived from a principal components analysis. Analysis based on [this tutorial](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/)

```{r}
library(readr)
library(dplyr)
library(datawizard)
sv_202303 <- read_csv("m21_spell_vocab_202303.csv")
sv_202303.na <- na.omit(sv_202303)
sv_202303.na <- mutate(sv_202303.na, z_ART = standardise(ART_correct), z_vocab = standardise(vocab_correct), z_spell = standardise(spell_correct))

cor.test(sv_202303.na$z_vocab, sv_202303.na$z_spell)
```


By default, the function PCA() [in FactoMineR], standardizes the data automatically during the PCA; so you donâ€™t need do this transformation before the PCA.
- X: a data frame. Rows are individuals and columns are numeric variables
- scale.unit: a logical value. If TRUE, the data are scaled to unit variance before the analysis. This standardization to the same scale avoids some variables to become dominant just because of their large measurement units. It makes variable comparable.
- ncp: number of dimensions kept in the final results.
- graph: a logical value. If TRUE a graph is displayed.

The plot shows the relationships between all variables. It can be interpreted as follow:

Positively correlated variables are grouped together.
Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants).
The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map.

```{r}

library(FactoMineR)
library(factoextra)

res.pca <- PCA(sv_202303.na[,3:4], scale.unit = TRUE, ncp = 2, graph = TRUE)
```

The eigenvalues measure the amount of variation retained by each principal component. Eigenvalues are large for the first PCs and small for the subsequent PCs. That is, the first PCs corresponds to the directions with the maximum amount of variation in the data set.

We examine the eigenvalues to determine the number of principal components to be considered

```{r}
(eig.val <- get_eigenvalue(res.pca))
```


The quality of representation of the variables on factor map is called cos2 (square cosine, squared coordinates). A high cos2 indicates a good representation of the variable on the principal component. In this case the variable is positioned close to the circumference of the correlation circle.  A low cos2 indicates that the variable is not perfectly represented by the PCs. In this case the variable is close to the center of the circle.  For a given variable, the sum of the cos2 on all the principal components is equal to one.  If a variable is perfectly represented by only two principal components (Dim.1 & Dim.2), the sum of the cos2 on these two PCs is equal to one. In this case the variables will be positioned on the circle of correlations.

```{r}
res.pca$var$cos2
```

The contributions of variables in accounting for the variability in a given principal component are expressed in percentage. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. Variables that do not correlated with any PC or correlated with the last dimensions are variables with low contribution and might be removed to simplify the overall analysis.

```{r}
res.pca$var$contrib
(res.desc <- dimdesc(res.pca, axes = c(1,2), proba = 0.05))
```

The fviz_pca_ind() is used to produce the graph of individuals.

```{r}
fviz_pca_ind(res.pca)
```

```{r}
sv_202303.na<-bind_cols(sv_202303.na,res.pca$ind$coord)
```


Loads RT data and join to PCA dataset


```{r}
cw_frq <- read_csv("CW_frq.csv")
nw_frq <- read_csv("NW_frq.csv")

CW_rt <- read_csv("CW_rt_2.csv")
CW_rt$cw_target <- NULL
CW_rt <- rename(CW_rt, cw_target = target_lower)

NW_rt <- read_csv("NW_rt_2.csv")
NW_rt$nw_target <- NULL
NW_rt <- rename(NW_rt, nw_target = target_lower)

cw_rt_pca <- inner_join(sv_202303.na, CW_rt, by = "SubjID")  #join subject PCA data
nw_rt_pca <- inner_join(sv_202303.na, NW_rt, by = "SubjID")

cw <- left_join(cw_rt_pca, cw_frq, by = c("cw_target")) #join word frequency data
nw <- left_join(nw_rt_pca, nw_frq, by = c("nw_target"))



rm(CW_rt)  #remove original rt file after joining neuropsych data
rm(NW_rt)

```

Removes  rts for errors (column rt.err) and then imputes missing values with the mean for the dataset (column "rt.err.imp") then creates a new column with inverse RTs

```{r replacewithmean}
library(tidyr)
cw <- cw |> mutate(rt.err = response_time * correct)  # convert error rts to 0
cw <- cw |> mutate(rt.err = na_if(rt.err, 0))         # convert 0 rts to NA
cw.mean <- mean(cw$rt.err, na.rm = TRUE)  # get mean rt excluding errors
cw <- cw |> mutate(rt.err.imp = ifelse(is.na(rt.err), cw.mean, rt.err))  # replace missing values with mean
cw <- cw |> mutate(inv.rt = 1/rt.err.imp)  # creates new column with inverse RTs


nw <- nw |> mutate(rt.err = response_time * correct)  # convert error rts to 0
nw <- nw |> mutate(rt.err = na_if(rt.err, 0))         # convert 0 rts to NA
nw.mean <- mean(nw$rt.err, na.rm = TRUE)  # get mean rt excluding errors
nw <- nw |> mutate(rt.err.imp = ifelse(is.na(rt.err), nw.mean, rt.err))  # replace missing values with mean
nw <- nw |> mutate(inv.rt = 1/rt.err.imp)  # creates new column with inverst RTs
```

Divide participants based on median split of Dim2.  Higher values on this factor indicate that spelling scores were relatively higher than vocabulary, 

```{r}
cw.median <- median(cw$Dim.2)
(cw <- cw |>
    mutate(lang_type = case_when(
      Dim.2 <= cw.median ~ "Semantic",
      Dim.2 > cw.median ~ "Orthographic"
    )))

nw.median <- median(nw$Dim.2)
(nw <- nw |>
    mutate(lang_type = case_when(
      Dim.2 <= nw.median ~ "Semantic",
      Dim.2 > nw.median ~ "Orthographic"
    )))

```


With RT as dependent variable

```{r}
library(ez)
library(car)
(m.cw <- ezANOVA(cw, 
        dv = rt.err.imp,
        wid = SubjID,
        within = cw_famsize,
        between = lang_type))

(m.nw <- ezANOVA(nw, 
        dv = rt.err.imp,
        wid = SubjID,
        within = .(nw_famsize, complexity),
        between = lang_type))
```

Define standard error of the means function

```{r}
sem <- function(x) sd(x)/sqrt(length(x))
```


Get condition means

```{r}
(cw.cond.means <- cw |> group_by(cw_famsize, lang_type) |> summarise(mean = mean(rt.err.imp), se = sem(rt.err.imp)))
(nw.cond.means <- nw |> group_by(nw_famsize, complexity, lang_type) |> summarise(mean = mean(rt.err.imp), se = sem(rt.err.imp)))
```


