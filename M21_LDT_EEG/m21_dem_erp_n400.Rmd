---
title: "m21 LDT ERP analysis N400"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "||")
```

# Load libraries

Load libraries
```{r}
library(ez)
library(pander)
library(kableExtra)
library(afex)
library(gridExtra)
library(ggplot2)
library(emmeans)
library(tidyverse)
library(dplyr)
library(RColorBrewer)
library(wesanderson)
library(ggsci)
```



# Set `ggplot2` parameters
Before we begin, let's set some general parameters for `ggplot2`. We will set a general theme using the `theme_set()` function. We will use the 'classic' theme which gives us clean white background rather than the default grey with white grid lines. And we will position the legend at the top of the graph rather than at the right side which is the default.

```{r theme}
theme_set(theme_minimal() +  theme(legend.position = "bottom"))
```

#Define standard error of the mean function
```{r}
sem <- function(x) sd(x)/sqrt(length(x))
```


# Load and format data files
First we load the data files
```{r, echo = FALSE}
erpdat_1 <- read_csv("m21_ldt_mea_300500_200000_1.csv")
langdat_1 <- read_csv("m21_langdat_1_pca.csv")

erpdat_2 <- read_csv("m21_ldt_mea_300500_200000_2.csv")
langdat_2 <- read_csv("m21_langdat_2_pca.csv")
```

Now we extract `SubjID` from the `ERPset` column
```{r, echo = FALSE}

# Remove '_LDT_diff_waves' from each string in the ERPset column
# This code first renames the column and then applies the `str_replace` function 
# to the newly renamed column.
erpdat_1 <- erpdat_1 %>%
  rename(SubjID = ERPset) %>%
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", "")) |>
  select(-mlabel)

erpdat_2 <- erpdat_2 %>%
  rename(SubjID = ERPset) %>%
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", ""))|>
  select(-mlabel)
```

We then join the ERP data, and language into a single data frame
```{r, echo = FALSE}
# Join the three data frames, specifying different join columns for the third join
n400_1 <- erpdat_1 %>%
  full_join(langdat_1, by = "SubjID")

n400_2 <- erpdat_2 %>%
  full_join(langdat_2, by = "SubjID") 
```

Divide into word, non-word and difference wave dataframes
```{r}
n400_1_words <- n400_1 |> filter(bini %in% c(1:2))
n400_1_nonwords <- n400_1 |> filter(bini %in% c(3:6))
n400_1_diff <- n400_1 |> filter(bini %in% c(9:11))

n400_2_words <- n400_2 |> filter(bini %in% c(1:2))
n400_2_nonwords <- n400_2 |> filter(bini %in% c(3:6))
n400_2_diff <- n400_2 |> filter(bini %in% c(9:11))
```



Then we do some more formatting and cleanup of the dataframes.We  create separate columns, one for each independent variable (anteriority, laterality, morphological family size). To do this we have to use `seperate` function from the `stringr` package. Run `vignette("programming", package = "dplyr")` to see more about `tidy-selection` and `tidy-evaluation`.

```{r, echo = FALSE}
library(stringr)
library(tidyr)

n400_1_nonwords <- n400_1_nonwords %>%
  separate(binlabel, into = c("trial_type", "family_size", "complexity"), sep = "_", remove = TRUE) |>
  select(-trial_type)
n400_1_words <- n400_1_words %>%
  separate(binlabel, into = c("trial_type", "family_size"), sep = "_", remove = TRUE) |>
  select(-trial_type)

n400_2_nonwords <- n400_2_nonwords %>%
  separate(binlabel, into = c("trial_type", "family_size", "complexity"), sep = "_", remove = TRUE) |>
  select(-trial_type)
n400_2_words <- n400_2_words %>%
  separate(binlabel, into = c("trial_type", "family_size"), sep = "_", remove = TRUE) |>
  select(-trial_type)
```

Now we need to  extract just the bins and channels that we intend to analyse. For this analysis we will use 9 channels:  F3, Fz, F4, C3, Cz, C4, P3, Pz, P4 . We will use the`mutate` function from the `dplyr` package along with the `case_when` function. The `case_when` function  is a sequence of two-sided formulas. The left hand side determines which values match this case. The right hand side provides the replacement value.

```{r, echo=FALSE}

channels_1 <-  c(3, 2, 25, 7, 20, 21, 12, 11, 16)
channels_2 <-  c(3, 2, 29, 8, 23, 24, 14, 13, 19)

n400_1_nonwords <- n400_1_nonwords %>%
  filter(chindex %in% channels_1) %>% 
  mutate(anteriority = case_when(grepl("F", chlabel) ~ "Frontal",
                                 grepl("C", chlabel) ~ "Central",
                                 grepl("P", chlabel) ~ "Parietal"),
         laterality = case_when(grepl("3", chlabel) ~ "Left",
                                grepl("z", chlabel) ~ "Midline",
                                grepl("Z", chlabel) ~ "Midline",
                                grepl("4", chlabel) ~ "Right"))
n400_1_nonwords$anteriority <- factor(n400_1_nonwords$anteriority, levels = c("Frontal", 
                                                                              "Central", 
                                                                              "Parietal"))
n400_1_nonwords$laterality <- factor(n400_1_nonwords$laterality, levels = c("Left", 
                                                                            "Midline", 
                                                                            "Right"))


n400_2_nonwords <- n400_2_nonwords %>%
  filter(chindex %in% channels_2) %>% 
  mutate(anteriority = case_when(grepl("F", chlabel) ~ "Frontal",
                                 grepl("C", chlabel) ~ "Central",
                                 grepl("P", chlabel) ~ "Parietal"),
         laterality = case_when(grepl("3", chlabel) ~ "Left",
                                grepl("z", chlabel) ~ "Midline",
                                grepl("Z", chlabel) ~ "Midline",
                                grepl("4", chlabel) ~ "Right"))
n400_2_nonwords$anteriority <- factor(n400_2_nonwords$anteriority, levels = c("Frontal", 
                                                                              "Central", 
                                                                              "Parietal"))
n400_2_nonwords$laterality <- factor(n400_2_nonwords$laterality, levels = c("Left", 
                                                                            "Midline", 
                                                                            "Right"))
n400_2_nonwords$anteriority <- factor(n400_2_nonwords$anteriority, levels = c("Frontal", 
                                                                              "Central", 
                                                                              "Parietal"))
n400_2_nonwords$laterality <- factor(n400_2_nonwords$laterality, levels = c("Left", 
                                                                            "Midline", 
                                                                            "Right"))

```


# Now we can compute the ANOVA using `ezANOVA` and `aov_ez`

## Group 1
```{r}
anova_results.1a <- ezANOVA(n400_1_nonwords,
        dv = value,
        wid = SubjID,
        within = .(family_size, complexity, anteriority, laterality),
        between = .(lang_type_semantic, lang_type_ortho),
        type = 3) 
anova_results.1a$ANOVA
anova_results.1a$`Sphericity Corrections`

anova_results.1b <- aov_ez(id = "SubjID",
                          dv = "value",
                          data = n400_1_nonwords,
                          within = c("family_size",
                                     "complexity",
                                     "anteriority",
                                     "laterality"),
                          between = c("lang_type_semantic","lang_type_ortho"), 
                          type = 3)
anova_results.1b
```

## Group 2
```{r}
anova_results.2a <- ezANOVA(n400_2_nonwords,
        dv = value,
        wid = SubjID,
        within = .(family_size, complexity,anteriority, laterality),
        between = .(lang_type_semantic, lang_type_ortho),
        type = 3) 
anova_results.2a$ANOVA
anova_results.2a$`Sphericity Corrections`

anova_results.2b <- aov_ez(id = "SubjID",
                          dv = "value",
                          data = n400_2_nonwords,
                          within = c("family_size", 
                                     "complexity", 
                                     "anteriority", 
                                     "laterality"),
                          between = c("lang_type_semantic","lang_type_ortho"), 
                          type = 3)
anova_results.2b
```

# Examine and plot interactions
## Group 2 
### `Language Type Orthographic` by `Anteriority` Interaction

#### Pairwise Comparisons `lang_type_ortho | anteriority`
```{r}
emms <- emmeans(anova_results.1b, ~ lang_type_ortho | anteriority )
pairwise_results <- pairs(emms, by = c("anteriority"))
summary(pairwise_results)
```

#### Condition Means `lang_type_ortho | anteriority`
```{r}
(nw_ltortho_ant_1 <- n400_1_nonwords |> 
    na.omit()|>
   group_by(anteriority, lang_type_ortho) |> 
   summarise(mean = mean(value), 
             se = sem(value),
             num_stim = n()))
```

#### Diff Scores `lang_type_ortho | anteriority`
```{r}
(difference_scores_1.1 <- nw_ltortho_ant_1 %>%
  pivot_wider(names_from = lang_type_ortho, values_from = c(mean, se, num_stim)) %>%
  mutate(mean_diff = `mean_Low Orthographic` - `mean_High Orthographic`, 
         avg_se = mean(`se_Low Orthographic`,`se_High Orthographic`),
         total_num_stim = sum(`num_stim_Low Orthographic`, `num_stim_High Orthographic`)))
```


#### Plots`lang_type_ortho | anteriority` 
First we plot the raw scores then the difference scores
```{r, fig.width=5, fig.height=3}
# plot raw scores
p1.a <-  nw_ltortho_ant_1 |> ggplot(aes(x=anteriority, 
                                   y=mean, 
                                   fill = lang_type_ortho, 
                                   colour = lang_type_ortho,
                                   ymin = mean - se, 
                                   ymax = mean + se)) +
  coord_cartesian(xlim = NULL,ylim = c(-2, 4), expand = TRUE,default = FALSE,clip = "on") +
  geom_col(position = "dodge", width = .75,  alpha = .7)  +
  labs(y = "Voltage (microvolts)", x = "Anteriority")  +  
  geom_errorbar(width = .1, position = position_dodge(0.75)) + 
  theme_classic(base_size = 8) + 
   geom_text(aes(label = round(mean, digits = 2)),colour = "black",size = 2.5, vjust = -4, 
             position = position_dodge(.75))+
  guides(fill=guide_legend(title="Participant Reading Style"),
         colour= "none") +  
  theme(legend.position = "bottom")
p1.a + scale_fill_brewer(palette = "Paired")+
      scale_colour_brewer(palette = "Paired")

# plot diff scores
p1.b <- difference_scores_1.1 |> ggplot(aes(x = anteriority,
                                        y = mean_diff,
                                        ymin = mean_diff - avg_se,
                                        ymax = mean_diff + avg_se)) +
  coord_cartesian(xlim = NULL,ylim = c(-2, 4), expand = TRUE,default = FALSE,clip = "on") +
  geom_col(position = "dodge", width = 0.75, alpha = 0.7, 
           colour = "deepskyblue3", fill= "deepskyblue3") +
  labs(y = "N400 for Low Orthographic Minus High Orthographic", x = "Family Size") + 
  geom_errorbar(width = .08, position = position_dodge(0.75), colour = "deepskyblue3") + 
  theme_classic(base_size = 8) + 
  geom_text(aes(label = round(mean_diff, digits = 2)),colour = "black",size = 2.5, vjust = -4, 
             position = position_dodge(.75))+  
    guides(fill=guide_legend(title="Anteriority"),
           colour= "none") +  
  theme(legend.position = "bottom")
p1.b 
 
# grid.arrange(p1.a, p1.b, nrow = 1)
```




### `Language Type Semantic` by `Complexity` by `Anteriority`  Interaction
#### Simple Effects `complexity | lang_type_semantic * anteriority`
```{r}
# Examine the 2-way interaction between ` lang_type_semantics` and `complexity` 
# at each level of `Anteriority`
(se_frontal_1.1 <-n400_1_nonwords |> filter(anteriority == "Frontal")|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = complexity,
          between = lang_type_semantic))
(se_central_1.1 <-n400_1_nonwords |> filter(anteriority == "Central")|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = complexity,
          between = lang_type_semantic))
(se_parietal_1.1 <-n400_1_nonwords |> filter(anteriority == "Parietal")|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = complexity,
          between = lang_type_semantic))

# Examine `complexity` at each level of ` lang_type_semantic` at Frontal sites.
(se_frontal_hisem_1.1 <-n400_1_nonwords |> filter(anteriority == "Frontal" & 
                                                    lang_type_semantic == "High Semantic")|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = complexity))
(se_frontal_losem_1.1 <-n400_1_nonwords |> filter(anteriority == "Frontal" & 
                                                    lang_type_semantic == "Low Semantic")|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = complexity))
```

We found a marginally significant effect of complexity for low semantic readers at frontal sites $F(1,29)=3.554641,  p=0.069434$.

#### Pairwise Comparisons `complexity | lang_type_semantic * anteriority`
```{r}
emms <- emmeans(anova_results.1b, ~ complexity| lang_type_semantic * anteriority )
pairwise_results <- pairs(emms, by = c("lang_type_semantic", "anteriority"))
summary(pairwise_results)
```

#### Condition Means  `complexity | lang_type_semantic * anteriority`
```{r}
(nw_ltseman_cmplx_ant_1 <- n400_1_nonwords |> 
    na.omit()|>
   group_by(anteriority, lang_type_semantic, complexity) |> 
   summarise(mean = mean(value), 
             se = sem(value),
             num_stim = n()))
```

#### Diff Scores `complexity | lang_type_semantic * anteriority`
```{r}
(difference_scores_1.2 <- nw_ltseman_cmplx_ant_1 %>%
  pivot_wider(names_from = complexity, values_from = c(mean, se, num_stim)) %>%
  mutate(mean_diff = `mean_simple` - `mean_complex`, 
         avg_se = mean(`se_complex`,`se_simple`),
         total_num_stim = sum(`num_stim_complex`, `num_stim_simple`)))
```

#### Plots `complexity | lang_type_semantic * anteriority`
First we plot the raw scores then the difference scores

```{r, fig.width=5, fig.height=3}
# plot raw scores
# facet_wrap() wraps a 1d sequence of panels into 2d. Use vars() to supply faceting variables; 
# Control the number of rows and columns with nrow and ncol.


p2.a <-  nw_ltseman_cmplx_ant_1 |> ggplot(aes(x= lang_type_semantic, y=mean, 
                                             fill = complexity, colour = complexity,
                                             ymin = mean - se, ymax = mean + se)) +
  facet_wrap(vars(anteriority), ncol = 3, labeller = "label_value") +  
  coord_cartesian(xlim = NULL, ylim = c(-2, 4), expand = TRUE, default = FALSE,clip = "on") +
  geom_col(position = "dodge", width = 0.75, alpha = .7)  +
  labs(y = "Voltage (microvolts)", x = "Participant Reading Style")  + 
  geom_errorbar(width = .08, position = position_dodge(0.75)) + 
  theme_classic(base_size = 8) + 
   geom_text(aes(label = round(mean, digits = 1)), colour = "black",  
             size = 2.5,  vjust = -6, 
             position = position_dodge(.75))+
  guides(fill=guide_legend(title="Non-Word Complexity"),
         colour= "none") +  
  theme(legend.position = "bottom")
p2.a + scale_fill_brewer(palette = "Set1")+
      scale_colour_brewer(palette = "Set1")

# plot diff scores
p2.b <- difference_scores_1.2 |> ggplot(aes(x = anteriority, 
                                            y = mean_diff,
                                            fill = lang_type_semantic,
                                            colour = lang_type_semantic,
                                            ymin = mean_diff - avg_se, 
                                            ymax = mean_diff + avg_se)) +
  coord_cartesian(xlim = NULL,ylim = c(-2, 4), expand = TRUE,default = FALSE,clip = "on") +
  geom_col(position = "dodge", width = 0.75, alpha = 0.7) +
  labs(y = "N400 amplitude for Simple Minus Complex", x = "Anteriority") + 
  geom_errorbar(width = .08, position = position_dodge(0.75)) + 
  theme_classic(base_size = 8) + 
  geom_text(aes(label = round(mean_diff, digits = 2)),colour = "black",size = 2.5, vjust = -5.5, 
             position = position_dodge(.75))+  
  guides(fill=guide_legend(title="Participant Reading Style"),
         colour= "none") +  
  theme(legend.position = "bottom")
p2.b + scale_fill_brewer(palette = "Paired")+
      scale_colour_brewer(palette = "Paired")
 
# grid.arrange(p2.a, p2.b, nrow = 1)
```

### `Language Type Semantic` by `Complexity` by `Family Size` by `Anteriority` x` Laterality` Interaction
#### Simple Effects `complexity | lang_type_semantic * family_size * laterality * anteriority `
```{r}
# Examine the 4-way interaction between `anteriority`, `laterality`, ` complexity`, 
# and `lang_type_semantics` at each level of `family_size`
se_large_1.2 <-n400_1_nonwords |> filter(family_size == "small")|>
  ezANOVA(dv = value,
         wid = SubjID,
         within = .(complexity, anteriority, laterality),
         between = .(lang_type_semantic),
         type = 3)
se_large_1.2$`Sphericity Corrections`

se_small_1.2 <-n400_1_nonwords |> filter(family_size == "large")|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = .(complexity, anteriority, laterality),
          between = lang_type_semantic)
se_small_1.2$`Sphericity Corrections`

# Examine the 3-way interaction between ` complexity`, `anteriority` and `laterality` 
# at each level of `lang_type_semantics` for non-words from large families

se_large_hisem_1.2 <-n400_1_nonwords |> filter(family_size == "large" & 
                                                 lang_type_semantic == "High Semantic")|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = .(anteriority, laterality, complexity))
se_large_hisem_1.2$`Sphericity Corrections`

se_large_losem_1.2 <-n400_1_nonwords |> filter(family_size == "large" & 
                                                 lang_type_semantic == "Low Semantic")|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = .(complexity, anteriority, laterality))
se_large_losem_1.2$`Sphericity Corrections`


# Examine the 2-way interaction between complexity and anteriority 
# at each level of laterality for non-words from large families for high semantic readers

# left
se_large_hisem_left_1.2 <-n400_1_nonwords |> 
  filter(family_size == "large" & 
           lang_type_semantic == "High Semantic" & 
           laterality == "Left")|>
    ezANOVA(dv = value,
          wid = SubjID,
          within = .(complexity, anteriority))
se_large_hisem_left_1.2$`Sphericity Corrections`

# midline
se_large_hisem_mid_1.2 <-n400_1_nonwords |> 
    filter(family_size == "large" & 
             lang_type_semantic == "High Semantic" & 
             laterality == "Midline")|>
    ezANOVA(dv = value,
          wid = SubjID,
          within = .(complexity, anteriority))
se_large_hisem_mid_1.2$`Sphericity Corrections`

# right
se_large_hisem_right_1.2 <-n400_1_nonwords |> 
    filter(family_size == "large" & 
             lang_type_semantic == "High Semantic" & 
             laterality == "Right")|>
    ezANOVA(dv = value,
          wid = SubjID,
          within = .(complexity, anteriority))
se_large_hisem_right_1.2$`Sphericity Corrections`

# Finally we examine the simple effect of complexity at each level of anteriority 
# for non-words from large families for high semantic readers at left sites
# Frontal
se_large_hisem_left_frontal_1.2 <- n400_1_nonwords |>
      filter(family_size == "large" & 
               lang_type_semantic == "High Semantic" & 
               laterality == "Left" & 
               anteriority == "Frontal" )|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = .(complexity))
se_large_hisem_left_frontal_1.2$ANOVA

# Central
se_large_hisem_left_central_1.2 <- n400_1_nonwords |>
      filter(family_size == "large" & 
               lang_type_semantic == "High Semantic" & 
               laterality == "Left" & 
               anteriority == "Central" )|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = .(complexity))
se_large_hisem_left_central_1.2$ANOVA  


# Parietal
se_large_hisem_left_parietal_1.2 <- n400_1_nonwords |>
      filter(family_size == "large" & 
               lang_type_semantic == "High Semantic" & 
               laterality == "Left" & 
               anteriority == "Parietal" )|>
  ezANOVA(dv = value,
          wid = SubjID,
          within = .(complexity))
se_large_hisem_left_parietal_1.2$ANOVA  

```

We found a marginally significant effect of complexity for high semantic readers for large morphological families at left frontal sites $F(1,29) =3.014575, p=0.09313352$

#### Pairwise Comparisons  `complexity | lang_type_semantic * family_size * laterality * anteriority`
```{r}
emms <- emmeans(anova_results.1b,~complexity|lang_type_semantic*family_size*laterality*anteriority)
pairwise_results <- pairs(emms,by = c("laterality","anteriority","lang_type_semantic","family_size"))
summary(pairwise_results)
```

#### Condition Means `complexity | lang_type_semantic * family_size * laterality * anteriority`
```{r}
(nw_sem_famsize_lat_ant_cmplx_1 <- n400_1_nonwords |> 
    na.omit()|>
   group_by( lang_type_semantic, family_size, laterality, anteriority, complexity  ) |> 
   summarise(mean = mean(value), 
             se = sem(value),
             num_stim = n()))
```

#### Plots `complexity | lang_type_semantic * family_size * laterality * anteriority`
First we plot the raw scores then the difference scores
Diff Scores `complexity | lang_type_semantic * family_size * laterality * anteriority`
```{r}
(difference_scores_1.3 <- nw_sem_famsize_lat_ant_cmplx_1 %>%
  pivot_wider(names_from = complexity, values_from = c(mean, se, num_stim)) %>%
  mutate(mean_diff = `mean_simple` - `mean_complex`, 
         avg_se = mean(`se_complex`,`se_simple`),
         total_num_stim = sum(`num_stim_complex`, `num_stim_simple`)))
```


**Plot interaction `complexity | lang_type_semantic * family_size * laterality * anteriority ` Raw Scores**
`facet_wrap()` wraps a 1d sequence of panels into 2d. Use `vars()` to supply faceting variables;  Control the number of rows and columns with `nrow` and `ncol.` `labeller` options are "label_value" and "label_both".  The latter prints the name of the variable & its value.

 Plot raw scores
```{r, fig.width=7, fig.height=10}

p3.a <-  nw_sem_famsize_lat_ant_cmplx_1 |> ggplot(aes(x= lang_type_semantic, y=mean,
                                          fill = complexity, colour = complexity,
                                          ymin = mean - se, ymax = mean + se)) +
  facet_wrap(vars(family_size, anteriority, laterality),
             labeller = "label_value", ncol = 3) +  
  coord_cartesian(xlim = NULL, ylim = c(-3.5, 3.5), expand=TRUE, default=FALSE, clip="on") +
  geom_col(position = "dodge", width = 0.75, alpha = 0.7)  +
  labs(y = "Voltage (microvolts)", x = "Participant Reading Style")  +  
  geom_errorbar(width = .08, position = position_dodge(0.75)) + 
  theme_classic(base_size = 8) + 
   geom_text(aes(label = round(mean, digits = 1)),
             colour = "black", 
             size = 2.5, 
             vjust = -2, 
             position = position_dodge(.75))+
  guides(fill=guide_legend(title="Complexity"),
         colour= "none") +  
  theme(legend.position = "bottom")
p3.a + scale_fill_brewer(palette = "Set1")+
      scale_colour_brewer(palette = "Set1")
```

Plot diff scores
```{r, fig.width=7, fig.height=6}

p3.b <- difference_scores_1.3 |> ggplot(aes(x = family_size, y = mean_diff,
                                        fill = lang_type_semantic, colour = lang_type_semantic,
                                        ymin = mean_diff - avg_se, ymax = mean_diff + avg_se)) +
      facet_wrap(vars(anteriority, laterality),
             labeller = "label_value", ncol = 3) + 
  coord_cartesian(xlim = NULL,ylim = c(-4, 4), expand = TRUE,default = FALSE,clip = "on") +
  geom_col(position = "dodge", width = 0.75, alpha = 0.7) +
  labs(y = "N400 amplitude for Simple Minus Complex", x = "Family Size") + 
  geom_errorbar(width = .08, position = position_dodge(0.75)) + 
  theme_classic(base_size = 8) + 
  geom_text(aes(label = round(mean_diff, digits = 2)),colour = "black",size = 2.5, vjust = -5.5, 
             position = position_dodge(.75))+  
  guides(fill=guide_legend(title="Participant Reading Style"),
         colour= "none") +  
  theme(legend.position = "bottom")
p3.b + scale_fill_brewer(palette = "Paired")+
      scale_colour_brewer(palette = "Paired")
```


## Group 2 
### `Family Size` by `Complexity` Interaction
#### Pairwise Comparisons `complexity | family_size`
```{r}
emms <- emmeans(anova_results.2b, ~ complexity | family_size )
pairwise_results <- pairs(emms, by = c("family_size"))
summary(pairwise_results)
```

#### Condition Means `complexity | family_size`
```{r}
(nw_cmplx_famsize_2 <- n400_2_nonwords |> 
    na.omit()|>
   group_by(family_size, complexity) |> 
   summarise(mean = mean(value), 
             se = sem(value),
             num_stim = n()))
```

#### Diff Scores `complexity | family_size`
```{r}
(difference_scores_2.1 <- nw_cmplx_famsize_2 %>%
  pivot_wider(names_from = complexity, values_from = c(mean, se, num_stim)) %>%
  mutate(mean_diff = `mean_simple` - `mean_complex`, 
         avg_se = mean(`se_simple`,`se_complex`),
         total_num_stim = sum(`num_stim_simple`, `num_stim_complex`)))
```

#### Plots`complexity | family_size`
First we plot the raw scores then the difference scores
```{r, fig.width=5, fig.height=3}
# plot raw scores
p2.a <-  nw_cmplx_famsize_2 |> ggplot(aes(x=family_size, 
                                   y=mean, 
                                   fill = complexity, 
                                   colour = complexity,
                                   ymin = mean - se, 
                                   ymax = mean + se)) +
  coord_cartesian(xlim = NULL,ylim = c(-1, 2.5), expand = TRUE,default = FALSE,clip = "on") +
  geom_col(position = "dodge", width = .75,  alpha = .7)  +
  labs(y = "Voltage (microvolts)", x = "Family Size")  +  
  geom_errorbar(width = .1, position = position_dodge(0.75)) + 
  theme_classic(base_size = 8) + 
   geom_text(aes(label = round(mean, digits = 2)),colour = "black",size = 2.5, vjust = -6, 
             position = position_dodge(.75))+
  guides(fill=guide_legend(title="Complexity"),
         colour= "none") +  
  theme(legend.position = "bottom")
p2.a + scale_fill_brewer(palette = "Set1")+
      scale_colour_brewer(palette = "Set1")

# plot diff scores
p2.b <- difference_scores_2.1 |> ggplot(aes(x = family_size,
                                        y = mean_diff,
                                        ymin = mean_diff - avg_se,
                                        ymax = mean_diff + avg_se)) +
  coord_cartesian(xlim = NULL,ylim = c(-2, 2), expand = TRUE,default = FALSE,clip = "on") +
  geom_col(position = "dodge", width = 0.75, alpha = 0.7, 
           colour = "deepskyblue3", fill= "deepskyblue3") +
  labs(y = "N400 for Simple Minus Comlex Non-words", x = "Family Size") + 
  geom_errorbar(width = .08, position = position_dodge(0.75), colour = "deepskyblue3") + 
  theme_classic(base_size = 8) + 
  geom_text(aes(label = round(mean_diff, digits = 2)),colour = "black",size = 2.5, vjust = -6, 
             position = position_dodge(.75))+  
    guides(fill=guide_legend(title="Complexity Effect"),
           colour= "none") +  
  theme(legend.position = "bottom")
p2.b 
 
# grid.arrange(p1.a, p1.b, nrow = 1)
```

