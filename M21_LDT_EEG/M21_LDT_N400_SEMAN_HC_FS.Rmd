---
title: "M21 LDT ERP HC SEMANTIC SENSITIVITY N400 Family Size"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: inline
---

\scriptsize

# Set parameters {-}
Set chunk parameters
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "||")
options(width = 140)
```



Load libraries
```{r, echo=FALSE}
library(tidyverse)
library(ggeffects)
library(lme4)
library(afex)
library(gridExtra)
library(emmeans)
library(effectsize)
library(performance)
library(cowplot)  # for use with `plot_grid(x,x,ncol = x)` function
library(e1071) # for use with `skewness()` function
library(lmerTest)
```


Set ggplot parameters
```{r, echo=FALSE}
theme_set(theme_classic() +  
            theme(legend.position = "bottom", 
                  axis.text=element_text(size=8.5),
                  axis.title=element_text(size=9)))

my_theme <- theme(strip.text = element_text(size = 7),
                  axis.text.x = element_text(size = 7),
                  legend.text = element_text(size = 6),
                  legend.title = element_blank())

# Define a custom color palette
my_palette <- c("#A6CEE3",  "#FB9A99")
my_palette_2 <- c( "#1F78B4","#E31A1C" )
my_palette_3 <- c("#A6CEE3","#1F78B4","#FB9A99","#E31A1C")


# Create a function to apply this palette
scale_color_custom <- function() {
  scale_color_manual(values = my_palette_2)
}

scale_fill_custom <- function() {
  scale_fill_manual(values = my_palette_2)
}

# Combine theme and scales
my_style <- list(my_theme,scale_color_custom(),scale_fill_custom())
```


Define standard error of the mean function
```{r, echo=FALSE}
sem <- function(x) sd(x)/sqrt(length(x))
```


# Load data files 

```{r}
dir_path <- "CSV files"

erp_4A <- read_csv(file.path(dir_path, "fs_m21_ldt_mea_300500_050050_1_AB.csv"))
erp_4B <- read_csv(file.path(dir_path, "fs_m21_ldt_mea_300500_050050_1_BA.csv"))
dmg_lng_vsl <- read_csv(file.path(dir_path, "demo_lang_vsl_pca_hc.csv"))
```


```{r}
library(dplyr)

erp_4i <- bind_rows(
  erp_4A |> mutate(List = "AB"),
  erp_4B |> mutate(List = "BA")
)
```

Now we extract `SubjID` from the `ERPset` column
```{r, , echo=FALSE}

# Remove '_LDT_diff_waves' from each string in the ERPset column
# This code first renames the column and then applies the `str_replace` function 
# to the newly renamed column.
erp_4ii <- erp_4i |>
  rename(SubjID = ERPset) |>
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", "")) |>
  select(-mlabel)

```

We then join the ERP data and language into a single data frame

<br>

```{r, echo=FALSE}
erp_4iii <- erp_4ii |>
  left_join(dmg_lng_vsl, by = "SubjID") |>
  select(SubjID, everything()) 
```

# Format data files 

Divide into word, non-word and difference wave dataframes
```{r, echo=FALSE}

n400_words <- erp_4iii |> filter(bini %in% c(1:2))    # does not include BF data
n400_words_b <- erp_4iii |> filter(bini %in% c(9:12)) # includes BF data
n400_nonwords <- erp_4iii |> filter(bini %in% c(3:6))
```

Then we do some more formatting and cleanup of the dataframes.We  create separate columns, one for each independent variable (anteriority, laterality, morphological family size). To do this we have to use `seperate` function from the `stringr` package. Run `vignette("programming", package = "dplyr")` to see more about `tidy-selection` and `tidy-evaluation`.

```{r, echo=FALSE}
# Words
n400_words <- n400_words |>
  separate(binlabel, into = c("trial_type","family_size"), sep = "_", remove = TRUE) |>
  select(-trial_type)
n400_words_b <- n400_words_b |>
  separate(binlabel, into = c("trial_type", "family_size","tmp1", "base_freq", "tmp2"), sep = "_", remove = TRUE) |>
  select(-c(trial_type, tmp1, tmp2))

n400_words_b$family_size[n400_words_b$family_size == "large"] <- "Large"
n400_words_b$family_size[n400_words_b$family_size == "small"] <- "Small"


# Nonwords
n400_nonwords <- n400_nonwords |>
  separate(binlabel, into = c("trial_type", "family_size", "complexity"), sep = "_", remove = TRUE) |>
  select(-trial_type)

# Assuming your data frame is named 'df' and the column is named 'your_column'
n400_nonwords$complexity[n400_nonwords$complexity == "complex"] <- "Complex"
n400_nonwords$complexity[n400_nonwords$complexity == "simple"] <- "Simple"
n400_nonwords$family_size[n400_nonwords$family_size == "large"] <- "Large"
n400_nonwords$family_size[n400_nonwords$family_size == "small"] <- "Small"

# Format as factors
n400_nonwords <- n400_nonwords |>
  mutate(complexity = fct_relevel(complexity, "Simple", "Complex"))

n400_nonwords <- n400_nonwords |>
  mutate(family_size = fct_relevel(family_size, "Small", "Large"))
```

Now we need to  extract just the bins and channels that we intend to analyse. For this analysis we will use 9 channels:  F3, Fz, F4, C3, Cz, C4, P3, Pz, P4 . We will use the`mutate` function from the `dplyr` package along with the `case_when` function. The `case_when` function  is a sequence of two-sided formulas. The left hand side determines which values match this case. The right hand side provides the replacement value.

```{r, echo=FALSE}
channels_1 <-  c(3, 2, 25, 7, 20, 21, 12, 11, 16)

# Words
n400_words <- n400_words |>
 filter(chindex %in% channels_1) |> 
 select(-`Included VSL2`)

n400_words_b <- n400_words_b |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)

# Nonwords
n400_nonwords <- n400_nonwords  |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)
```


# N400 Word Data

```{r}
n400_words_b %>%
  count(family_size, base_freq, Semantic_Sensitivity)
```

## Nested ANOVA Model

```{r}
#Fit ANOVA model
anova_model_n400_words_b <- mixed(
    value ~ Semantic_Sensitivity * family_size * base_freq +
    (1 + family_size + base_freq | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n400_words_b,
  method = "KR"
)
anova_model_n400_words_b 

m1 <- anova_model_n400_words_b$full_model    # Extract the lmer model
ranova(m1) # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n400_words_b, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n400_words_b)
```

## Main Effects

The N400 data for words show no main effects,

## Interactions

There is a robust *family_size × base_freq* interaction, which is further modulated by *Semantic Sensitivity*. This is essentially the same structural pattern as for the N250, but the effect is larger and more reliable in the N400 window.


|                                     Effect|      df|          F| p.value||eta-sqrd|  
|-------------------------------------------|--------|-----------|--------|---------|
|                      family_size:base_freq| 1, 1498|  64.68 ***|   <.001|     0.04|
| Semantic_Sensitivity:family_size:base_freq| 1, 1498|  12.12 ***|   <.001| 8.02e-03|


### `Family Size x Base Frequency` Simple Contrasts

```{r}
# Estimated marginal means for the family_size × base frequency interaction
(emm1 <- emmeans(anova_model_n400_words_b, ~ family_size * base_freq))

# Get all pairwise contrasts
emm1_contrasts <- contrast(emm1,  method = "pairwise", by = NULL, adjust = "none")

# Keep only the contrasts you want
# Simple effects of family_size at each level of base_freq
# Simple effects of base_freq at each level of family_size
keep <- c("Large High - Small High",
          "Large Low - Small Low",
          "Large High - Large Low",
          "Small High - Small Low")
(emm1_contrasts_filtered <- subset(emm1_contrasts, contrast %in% keep))

# Get Confidence Intervals
(emm1_contrasts_filtered_ci <- confint(emm1_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs1 <- eff_size(emm1, sigma = sigma(m1), edf = df.residual(m1))

# Remove the two redundant rows (rows 3 and 4)
(effs1_filtered <- subset(effs1, !contrast %in% c("Large High - Small Low",
                                                  "Small High - Large Low")))
```


For large-family words, n400 amplitude is more negative when base frequency is high than when it is low. For small-family words, base frequency has little effect. For low-frequency bases, small-family words elicit more negative amplitudes than large-family words.

- At High base frequency: $\text{Large - Small }= \; -0.109; SE = 0.282; t = -0.386; p = 0.7005$.  NS.

- At Low base frequency: $\text{Large - Small } = 0.921 \; SE = 0.282; t = 3.205; p = 0.0017$. Significant

When base frequency is *low*, large vs small family_size differ significantly in predicted N400; when base frequency is *high*, they do not differ.

Next, contrasting High vs Low base_freq within each family_size: 

- Large family_size: $\text{High - Low }= \; -0.286; SE = 0.197; t = -1.456; p = 0.1496$. NS

- Small family_size: $\text{High - Low }= \; 0.743; SE = 0.197; t = 3.779; p = 0.0003$. Significant 

When family_size is *small*, high vs low base frequency yields a significant difference; when family_size is *large*, the difference is  not strong.

### `Family Size x Base Frequency` Interaction Contrasts

<br>

```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare base frequency effect in large vs small family)
contrast(emm1, interaction = "pairwise", by = NULL, adjust = "holm")

# Get confidence intervals, for each base frequency effect for each family size and then for interaction effect
confint(contrast(emmeans(m1, ~ family_size | base_freq), "pairwise"))
confint(contrast(emm1, interaction = c("pairwise", "pairwise")))
```

The final contrast tests whether the difference between Large vs Small family_size is itself different between High vs Low base_freq: 

$\text{Estimate }= \; -1.03$; $SE = 0.128$; $t = -8.029$; $p < .0001$

That is, the slope or effect of `family_size` depends strongly on the level of `base_freq` (consistent with your ANOVA). Put differently: the family size difference (Large - Small) is much more positive in the low base frequency condition than it is in the high base frequency condition. That difference of differences is highly significant.

### `Family Size x Base Frequency` Interaction Plots

```{r, , fig.width = 5.5, fig.height= 3 }
p1 <- emmip(anova_model_n400_words_b, family_size ~ base_freq) + my_style
p2 <- emmip(anova_model_n400_words_b, base_freq ~  family_size) + my_style

plot_grid(p1, p2, ncol = 2)
```

### `Sensitivity x Family Size x Base Frequency`  Simple Contrasts
<br>
```{r}
# Estimated marginal means for the family_size × base_freq interaction
(emm1b <- emmeans(anova_model_n400_words_b, ~ Semantic_Sensitivity * family_size * base_freq))

# Get all pairswise contrasts
emm1b_contrasts <- contrast(emm1b,  method = "pairwise", by = NULL, adjust = "none")

# Keep only the contrasts you want
# Simple effects of family_size at each level of base_freq
# Simple effects of base_freq at each level of family_size
keep1b <- c("High Semantic Large High - High Semantic Large Low",
          "High Semantic Small High - High Semantic Small Low",
          "Low Semantic Large High - Low Semantic Large Low",
          "Low Semantic Small High - Low Semantic Small Low",
          "High Semantic Large High - High Semantic Small High",
          "High Semantic Large Low - High Semantic Small Low",
          "Low Semantic Large High - Low Semantic Small High",
          "Low Semantic Large Low - Low Semantic Small Low",
          "High Semantic Large High - Low Semantic Large High",
          "High Semantic Small High - Low Semantic Small High",
          "High Semantic Large Low - Low Semantic Small Low",
          "High Semantic Small Low - Low Semantic Small Low")

(emm1b_contrasts_filtered <- subset(emm1b_contrasts, contrast %in% keep1b))

# Get Confidence Intervals
(emm1b_contrasts_filtered_ci <- confint(emm1b_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs1b <- eff_size(emm1b, sigma = sigma(m1), edf = df.residual(m1))

# Remove the  redundant rows 
(effs1b_filtered <- subset(effs1b, contrast %in% keep1b))
```

### `Sensitivity x Family Size x Base Frequency` Interaction Contrasts

The interaction contrast tests whether the difference in the *base frequency* effect for large vs small families differs across semantic sensitivity?

$$
 [ [(A_1 - A_2)\text{ in }B_1] - [(A_1 - A_2)\text{ in }B_2]
    \text{ in Condition }C_1]
  -
  [[(A_1 - A_2) in B_1] - [(A_1 - A_2) in B_2]
    \text{in Condition }C_2]
$$
<br>
```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare base_freq effect in large vs small family)
contrast(emm1b, interaction = "pairwise", by = NULL, adjust = "holm")
confint(contrast(emm1b, interaction = c("pairwise", "pairwise")))

# Compute the A1 - A2 difference within each combination of B × C
(base_freq_diff <- contrast(emm1b, method = "revpairwise", 
                            by = c("Semantic_sensitivity", "family_size"), 
                            simple = "base_freq"))

# Compute how that A-effect changes across the levels of B, separately for each level of C  
(family_size_base_freq_int_within_sensitivity <- contrast(base_freq_diff, 
                                                           method = "revpairwise",
                                                           by = "Semantic_sensitivity", simple = "family_size"))

# Get confidence intervals
confint(family_size_base_freq_int_within_sensitivity)
```
(a) Base-frequency effect within each Family Size × Sensitivity cell

| Semantic Sensitivity | Family Size    | High - Low Base Freq   | *t*      | *p*            | Interpretation |
|-----------------------|---------------|------------------------|----------|----------------|----------------|
| High Sensitivity      | Large Family  | -0.3850 µV             | -1.408   |   .1634 (n.s.) | Weak, non-sig. tendency: low-freq words slightly less negative. |
| Low Sensitivity       | Large Family  | -0.1878 µV             | -0.664   |   .5087 (n.s.) | Essentially flat. |
| High Sensitivity      | Small Family  |  1.0904 µV             |  3.987   | **.0002**      | More negative N400 for high-freq small-family words. |
| Low Sensitivity       | Small Family  |  0.3962 µV             |  1.401   |   .1654 (n.s.) | Modest, non-sig. version of same trend. |

**Pattern**:
Only high-sensitivity readers show a pronounced base-frequency effect—and only for small-family words, where high-frequency bases elicit larger (more negative) N400s.


(b) Difference of those frequency effects across family size (within each group)

| Semantic Sensitivity | (Small – Large Family)             | 95% CI            | *p*     | Interpretation |
|                      |  difference in base-freq effect    |                   |         |               |
|-----------------------|-----------------------------------|-------------------|---------|----------------|
| High Sensitivity      | –1.475 µV                          | [-1.825, -1.126] | <.0001  | the base-freq effect flips between large- and small-family words. |
| Low Sensitivity       | –0.584 µV                          | [-0.945, -0.223] |  0.0015 | Same pattern but weaker: a smaller differential between family sizes. |

Semantic Sensitivity	(Small – Large Family) difference in base-freq effect	95 % CI	p	Interpretation
High Sensitivity	–1.40 µV [–1.74, –1.05]	< .001	Very large difference: the base-freq effect flips between large- and small-family words.	
Low Sensitivity	–0.65 µV [–1.00, –0.29]	.0004	Same pattern but weaker: a smaller differential between family sizes.	

**Interpretation**:
Both groups show that the base-frequency effect differs by family size, but this contrast is about twice as strong in the high-sensitivity group.

(High – Low Sensitivity) × (Large – Small Family) × (High – Low Base Freq) = –0.75 µV [–1.25, –0.25], *p* = .003.

**Meaning**:
The difference between family-size patterns across base-frequency conditions is 0.75 µV larger in the high-sensitivity group than in the low-sensitivity group—confirming the N400 interaction found in the omnibus ANOVA.


### Plots `Sensitivity x Family Size x Base Frequency`
<br>
...
```{r, fig.width=8, fig.height=3.5}
p3 <- emmip(anova_model_n400_words_b, base_freq ~ family_size  |  Semantic_Sensitivity) + my_style
p4 <- emmip(anova_model_n400_words_b, family_size ~ base_freq | Semantic_Sensitivity) + my_style
plot_grid(p3, p4,  ncol = 2)
```

\newpage

# N400 Nonword Data

```{r, results='hide'}
n400_nonwords %>%
  count(family_size, complexity, Semantic_Sensitivity)
```

## Compute the ANOVA

```{r}
anova_model_n400_nonwords <- mixed(
    value ~ Semantic_Sensitivity * family_size * complexity +
    (1 + family_size + complexity | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n400_nonwords,
  method = "KR"
)
anova_model_n400_nonwords 

m2 <- anova_model_n400_nonwords$full_model    # Extract the lmer model
ranova(m2)    # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n400_nonwords, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n400_nonwords)
```
## Main Effects and Interactions

|     Effect|   df|      F| p.value| eta-sqrd|  
|-----------|------|-------|-------|---------|
| complexity| 1, 58| 4.15 *|   .046|     0.07|

Reponses to Simple words are more negative than those to Complex words.


|                                     Effect|      df|         F| p.value| eta-sqrd|  
|-------------------------------------------|--------|----------|--------|---------|
|                     family_size:complexity| 1, 1498|  8.08  **|    .005|5.36e-03 |
|Semantic_Sensitivity:family_size:complexity| 1, 1498| 34.43 ***|   <.001|    0.02 |


### `Family Size x Complexity` Simple Contrasts

(a) Effect of family_size within each level of complexity. Tests whether “*large vs. small family*” differs for simple and complex items separately. This helps you see where the interaction is coming from — e.g., if the family size effect flips between complexity levels.

(b) Effect of complexity within each level of family_size.  Tests whether “*complex vs. simple*” differs within large and small families.


```{r}
# Estimated marginal means for the family_size × complexity interaction
(emm2 <- emmeans(anova_model_n400_nonwords, ~ family_size * complexity))

# Get all pairswise contrasts
emm2_contrasts <- contrast(emm2,  method = "pairwise", by = NULL, adjust = "none")


# Keep only the contrasts you want
# Simple effects of family_size at each level of complexity
# Simple effects of complexity at each level of family_size
keep2 <- c("Small Simple - Large Simple",
          "Small Complex - Large Complex",
          "Small Simple - Small Complex",
          "Large Simple - Large Complex")
(emm2_contrasts_filtered <- subset(emm2_contrasts, contrast %in% keep2))

# Get Confidence Intervals
(emm2_contrasts_filtered_ci <- confint(emm2_contrasts_filtered))

# Get effect sizes
# Get all pairwise effect sizes
effs2 <- eff_size(emm2, sigma = sigma(m2), edf = df.residual(m2))

# Remove the two redundant rows (rows 3 and 4)
(effs2_filtered <- subset(effs2, contrast %in% keep2))
```


### `Family Size x Complexity` Interaction Contrasts

Is the difference in the effect of A across levels of B different at Complex vs. Simple levels?

Mathematically:

$$
 [ [(A_1 - A_2)\text{ in }B_1] - [(A_1 - A_2)\text{ in }B_2]
$$

```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare complexity effect in large vs small family)
contrast(emm2, interaction = "pairwise", by = NULL, adjust = "holm")

# Get confidence intervals, for each complexity effect for each family size and then for interaction effect
confint(contrast(emmeans(m2, ~ family_size | complexity), "pairwise"))
confint(contrast(emm2, interaction = c("pairwise", "pairwise")))
```


### `Family Size x Complexity` Plots 

```{r, fig.width=8, fig.height=3.5}
p5 <- emmip(anova_model_n400_nonwords, family_size ~ complexity) + my_style
p6 <- emmip(anova_model_n400_nonwords, complexity ~  family_size) + my_style
plot_grid(p5, p6, ncol = 2)
```

###  `Semantic Sensitivity x Family Size x Complexity` Simple Contrasts

Compare High vs Low Semantic Sensitivity within each combination of Family Size and Complexity  

<br>
```{r}
# Estimated marginal means for the family_size × complexity interaction
(emm3 <- emmeans(anova_model_n400_nonwords, ~ Semantic_Sensitivity * family_size * complexity))

# Get all pairswise contrasts
emm3_contrasts <- contrast(emm3,  method = "pairwise", by = NULL, adjust = "none")
# emm3_contrasts

# Keep only the contrasts you want
# Simple effects of family_size at each level of complexity
# Simple effects of complexity at each level of family_size
keep2 <- c("High Semantic Large Simple - High Semantic Large Complex",
           "High Semantic Small Simple - High Semantic Small Complex",
           "Low Semantic Large Simple - Low Semantic Large Complex",
           "Low Semantic Small Simple - Low Semantic Small Complex",
           "High Semantic Small Simple - High Semantic Large Simple",
           "High Semantic Small Complex - High Semantic Large Complex",
           "Low Semantic Small Simple - High Semantic Large Simple",
           "Low Semantic Small Complex - Low Semantic Large Complex",
           "High Semantic Large Simple - Low Semantic Large Simple",
           "High Semantic Large Complex - Low Semantic Large Complex",
           "High Semantic Small Simple - Low Semantic Small Simple",
           "High Semantic Small Complex - Low Semantic Small Complex")

(emm3_contrasts_filtered <- subset(emm3_contrasts, contrast %in% keep2))

# Get Confidence Intervals
(emm3_contrasts_filtered_ci <- confint(emm3_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs3 <- eff_size(emm3, sigma = sigma(m2), edf = df.residual(m2))

# Remove the  redundant rows 
(effs3_filtered <- subset(effs3, contrast %in% keep2))
```

### `Semantic Sensitivity x Family Size x Complexity` Interaction Contrasts

The interaction contrast tests whether the difference in the complexity effect for large vs small families differs across sensitivity?


$$
 [ [(A_1 - A_2)\text{ in }B_1] - [(A_1 - A_2)\text{ in }B_2]
    \text{ in Condition }C_1]
  -
  [[(A_1 - A_2) in B_1] - [(A_1 - A_2)\text{ in }B_2]
    \text{ in Condition }C_2]
$$

<br>
```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare complexity effect in large vs small family)
contrast(emm3, interaction = "pairwise", by = NULL, adjust = "holm")
confint(contrast(emm3, interaction = c("pairwise", "pairwise")))

# Compute the A1 - A2 difference within each combination of B × C
(complexity_diff <- contrast(emm3, method = "revpairwise", 
                            by = c("Semantic_sensitivity", "family_size"), 
                            simple = "complexity"))

# Compute how that A-effect changes across the levels of B, separately for each level of C  
(familysize_complexity_int_within_sensitivity <- contrast(complexity_diff, 
                                                           method = "revpairwise",
                                                           by = "Semantic_sensitivity", simple = "family_size"))

# Get confidence intervals
confint(familysize_complexity_int_within_sensitivity)
```


### `Semantic Sensitivity x Family Size x Complexity` Plots
<br>  
```{r, fig.width=8, fig.height=3.5}
p7 <- emmip(anova_model_n400_nonwords, family_size ~   complexity|   Semantic_Sensitivity)  + my_style
p8 <- emmip(anova_model_n400_nonwords, complexity ~ family_size  | Semantic_Sensitivity)  + my_style
plot_grid(p7, p8, ncol = 2)
```


 - High-semantic participants: show a greater complexity effect for small families than for large families.
 - Low-semantic participants: show the opposite pattern—a stronger complexity effect for large-family items and no effect for small ones.

Summary (N400)
 - High-semantic participants: familiarity (large families) neutralizes complexity effects.
 - Low-semantic participants: familiarity amplifies complexity effects.





