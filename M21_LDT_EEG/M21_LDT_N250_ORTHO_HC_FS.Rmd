---
title: "M21 LDT ERP HC ORTHOGRAPIC SENSITIVITY N250 Family Size"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: inline
---

\scriptsize

# Set parameters {-}
Set chunk parameters
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "||")
options(width = 140)
```



Load libraries
```{r, echo=FALSE}
library(tidyverse)
library(ggeffects)
library(lme4)
library(afex)
library(gridExtra)
library(emmeans)
library(effectsize)
library(performance)
library(cowplot)  # for use with `plot_grid(x,x,ncol = x)` function
library(e1071) # for use with `skewness()` function
library(lmerTest)
```


Set ggplot parameters
```{r, echo=FALSE}
theme_set(theme_classic() +  
            theme(legend.position = "bottom", 
                  axis.text=element_text(size=8.5),
                  axis.title=element_text(size=9)))

my_theme <- theme(strip.text = element_text(size = 7),
                  axis.text.x = element_text(size = 7),
                  legend.text = element_text(size = 6),
                  legend.title = element_blank())

# Define a custom color palette
my_palette <- c("#A6CEE3",  "#FB9A99")
my_palette_2 <- c( "#1F78B4","#E31A1C" )
my_palette_3 <- c("#A6CEE3","#1F78B4","#FB9A99","#E31A1C")


# Create a function to apply this palette
scale_color_custom <- function() {
  scale_color_manual(values = my_palette_2)
}

scale_fill_custom <- function() {
  scale_fill_manual(values = my_palette_2)
}

# Combine theme and scales
my_style <- list(my_theme,scale_color_custom(),scale_fill_custom())
```

Define standard error of the mean function

```{r, echo=FALSE}
sem <- function(x) sd(x)/sqrt(length(x))
```


# Load data files 

```{r}
dir_path <- "CSV files"

erp_2A <- read_csv(file.path(dir_path, "fs_m21_ldt_mea_200300_050050_1_AB.csv"))
erp_2B <- read_csv(file.path(dir_path, "fs_m21_ldt_mea_200300_050050_1_BA.csv"))

dmg_lng_vsl <- read_csv(file.path(dir_path, "demo_lang_vsl_pca_hc.csv"))
```

```{r}
library(dplyr)

erp_2i <- bind_rows(
  erp_2A |> mutate(List = "AB"),
  erp_2B |> mutate(List = "BA")
)
```

Now we extract `SubjID` from the `ERPset` column
```{r, , echo=FALSE}

# Remove '_LDT_diff_waves' from each string in the ERPset column
# This code first renames the column and then applies the `str_replace` function 
# to the newly renamed column.
erp_2ii <- erp_2i |>
  rename(SubjID = ERPset) |>
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", "")) |>
  select(-mlabel)

# erp_4 <- erp_4 |>
#   rename(SubjID = ERPset) |>
#   mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
#   mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
#   mutate(binlabel = str_replace(binlabel, "_family", "")) |>
#   select(-mlabel)
```

We then join the ERP data and language into a single data frame
```{r, echo=FALSE}

erp_2iii <- erp_2ii |>
  left_join(dmg_lng_vsl, by = "SubjID") |>
  select(SubjID, everything()) 
# n400 <- erp_4 |>
#   left_join(dmg_lng_vsl, by = "SubjID") |>
#   select(SubjID, everything()) 
```

# Format data files 

Divide into word, non-word and difference wave dataframes
```{r, echo=FALSE}
n250_words <- erp_2iii |> filter(bini %in% c(1:2))    # does not include BF data
n250_words_b <- erp_2iii |> filter(bini %in% c(9:12)) # includes BF data
n250_nonwords <- erp_2iii |> filter(bini %in% c(3:6))

```

Then we do some more formatting and cleanup of the dataframes.We  create separate columns, one for each independent variable (anteriority, laterality, morphological family size). To do this we have to use `seperate` function from the `stringr` package. Run `vignette("programming", package = "dplyr")` to see more about `tidy-selection` and `tidy-evaluation`.
<br>  
```{r, echo=FALSE}
# Words
n250_words <- n250_words |>
  separate(binlabel, into = c("trial_type","family_size"), sep = "_", remove = TRUE) |>
  select(-trial_type)
n250_words_b <- n250_words_b |>
  separate(binlabel, into = c("trial_type", "family_size","tmp1", "base_freq", "tmp2"), sep = "_", remove = TRUE) |>
  select(-c(trial_type, tmp1, tmp2))

n250_words_b$family_size[n250_words_b$family_size == "large"] <- "Large"
n250_words_b$family_size[n250_words_b$family_size == "small"] <- "Small"


# Nonwords
n250_nonwords <- n250_nonwords |>
  separate(binlabel, into = c("trial_type", "family_size", "complexity"), sep = "_", remove = TRUE) |>
  select(-trial_type)

n250_nonwords$complexity[n250_nonwords$complexity == "complex"] <- "Complex"
n250_nonwords$complexity[n250_nonwords$complexity == "simple"] <- "Simple"
n250_nonwords$family_size[n250_nonwords$family_size == "large"] <- "Large"
n250_nonwords$family_size[n250_nonwords$family_size == "small"] <- "Small"

# Format as factors
n250_nonwords <- n250_nonwords |>
  mutate(complexity = fct_relevel(complexity, "Simple", "Complex"))

n250_nonwords <- n250_nonwords |>
  mutate(family_size = fct_relevel(family_size, "Small", "Large"))

# str(n250_nonwords)

```
<br>  
Now we need to  extract just the bins and channels that we intend to analyse. For this analysis we will use 9 channels:  F3, Fz, F4, C3, Cz, C4, P3, Pz, P4 . We will use the`mutate` function from the `dplyr` package along with the `case_when` function. The `case_when` function  is a sequence of two-sided formulas. The left hand side determines which values match this case. The right hand side provides the replacement value.
<br>  
```{r, echo=FALSE}

channels_1 <-  c(3, 2, 25, 7, 20, 21, 12, 11, 16)

# Words
n250_words <- n250_words |>
 filter(chindex %in% channels_1) |> 
 select(-`Included VSL2`)

n250_words_b <- n250_words_b |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)

# Nonwords
n250_nonwords <- n250_nonwords |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)
```
<br>  

# N250 Word Data

Statistical analysis.

Linear mixed-effects models were fit using the afex::mixed function (method = “KR”) to account for both subject-level and electrode-level variability. Each model included random intercepts for participants (SubjID) and electrodes nested within participants (SubjID:chlabel), as well as by-subject random slopes for within-subject factors ( Family Size, Complexity, or Base Frequency, depending on the analysis). When a significant interaction was obtained, we probed it using estimated marginal means from the fitted model (emmeans package) to clarify the source of the effect.
Because these follow-up contrasts were intended to interpret a significant higher-order interaction rather than to test independent hypotheses, we reported uncorrected p-values (adjust = "none") for interpretive clarity. The robustness of the overall pattern was verified using a Holm correction, which did not change the substantive conclusions.

## Nested ANOVA Model
<br>  
```{r}
#Fit ANOVA model
anova_model_n250_words_b <- mixed(
    value ~ Orthographic_Sensitivity * family_size * base_freq +
    (1 + family_size + base_freq | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n250_words_b,
  method = "KR"
)
anova_model_n250_words_b 

m1 <- anova_model_n250_words_b$full_model    # Extract the lmer model
ranova(m1) # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n250_words_b, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n250_words_b)
```
<br>  

## Main Effects

No significant main effects

## Interactions


|                            Effect|      df|         F| p.value||eta-sqrd|  
|----------------------------------|--------|----------|--------|---------|
|             family_size:base_freq| 1, 1498| 33.27 ***|   <.001|    0.02 |

### Simple Contrasts
<br>
```{r}
# Estimated marginal means for the family_size × base frequency interaction
(emm1 <- emmeans(anova_model_n250_words_b, ~ family_size * base_freq))

# Get all pairswise contrasts
emm1_contrasts <- contrast(emm1,  method = "pairwise", by = NULL, adjust = "none")
emm1_contrasts

# Keep only the contrasts you want
# Simple effects of family_size at each level of base_freq
# Simple effects of base_freq at each level of family_size
keep <- c("Large High - Small High",
          "Large Low - Small Low",
          "Large High - Large Low",
          "Small High - Small Low")
(emm1_contrasts_filtered <- subset(emm1_contrasts, contrast %in% keep))

# Get Confidence Intervals
(emm1_contrasts_filtered_ci <- confint(emm1_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs1 <- eff_size(emm1, sigma = sigma(m1), edf = df.residual(m1))

# Remove the two redundant rows (rows 3 and 4)
(effs1_filtered <- subset(effs1, !contrast %in% c("Large Family High Base Frequency - Small Family Low Base Frequency",
                                               "Small Family High Base Frequency - Large Family Low Base Frequency")))
```
<br>  
For large-family words, N250 amplitude is more negative when base frequency is high (-0.86) than when it is low (-0.35). For small-family words, base frequency has little effect (-0.82 for high vs -1.01 for low). For low-frequency bases, small-family words elicit more negative amplitudes (-1.01) than large-family words (-0.35). For high-frequency bases, family size has little effect (-0.86 for large vs -0.82 for small).


### Interaction Contrasts
<br>
```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare base frequency effect in large vs small family)
contrast(emm1, interaction = "pairwise", by = NULL, adjust = "holm")

# Get confidence intervals, for each base frequency effect for each family size and then for interaction effect
confint(contrast(emmeans(m1, ~ family_size | base_freq), "pairwise"))
confint(contrast(emm1, interaction = c("pairwise", "pairwise")))
```

There is a robust crossover interaction: the base-frequency effect is significant in opposite directions for large vs. small family words.

## Plots

```{r, fig.width = 5.5, fig.height= 2.5}
p1 <- emmip(anova_model_n250_words_b, family_size ~ base_freq) + my_style
p2 <- emmip(anova_model_n250_words_b, base_freq ~  family_size) + my_style

plot_grid(p1, p2, ncol = 2)
```


\newpage

# N250 Nonword Data
<br>  
```{r, results='hide'}
n250_nonwords %>%
  count(family_size, complexity, Orthographic_Sensitivity)

n250_nonwords |> filter(family_size == "complex")
```


## Compute the ANOVA
<br>  
```{r}
anova_model_n250_nonwords <- mixed(
    value ~ Orthographic_Sensitivity * family_size * complexity +
    (1 + family_size + complexity | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n250_nonwords,
  method = "KR"
)
anova_model_n250_nonwords 

m2 <- anova_model_n250_nonwords$full_model    # Extract the lmer model
ranova(m2)    # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n250_nonwords, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n250_nonwords)
```



## Main Effects and Interactions

No main effects emerged, but there are two marginal effects suggesting a subtle modulation by orthographic sensitivity:

  - A trend for `Orthographic_Sensitivity × Complexity`, $F(1,58) = 3.13$, $p = .082$

  - A trend for `Orthographic_Sensitivity × Family_Size × Complexity`, $F(1,1498) = 3.27$, $p = .071$

These trends imply that the complexity effect (and perhaps its relation to family size) may differ between participants high vs. low in orthographic sensitivity.
<br>  
```{r}
# Estimated marginal means for the Orthographic_Sensitivity * complexity interaction
(emm2 <- emmeans(anova_model_n250_nonwords, ~ Orthographic_Sensitivity  * complexity))

# Get all pairwise contrasts
emm2_contrasts <- contrast(emm2,  method = "pairwise", by = NULL, adjust = "none")
# emm2_contrasts

# Keep only the contrasts you want
# Simple effects of family_size at each level of complexity
# Simple effects of complexity at each level of family_size
keep2 <- c("High Orthographic Complex - High Orthographic Simple",
           "Low Orthographic Complex - Low Orthographic Simple",
           "High Orthographic Complex - Low Orthographic Complex",
           "High Orthographic Simple - Low Orthographic Simple")

(emm2_contrasts_filtered <- subset(emm2_contrasts, contrast %in% keep2))

# Get Confidence Intervals
(emm2_contrasts_filtered_ci <- confint(emm2_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs2 <- eff_size(emm2, sigma = sigma(m2), edf = df.residual(m2))

# Remove the  redundant rows 
(effs2_filtered <- subset(effs2, contrast %in% keep2))
```

Only **low-orthographic participants** show a reliable complexity effect: N250 amplitudes are more negative for simple nonwords than for complex ones, particularly for large-family items ($d \approx 0.7$).

**High-orthographic participants** show no difference, indicating greater normalization or automatic segmentation.

<br>  
<br>  

```{r}
# Estimated marginal means for the Orthographic_Sensitivity * family_size * complexity interaction
(emm3 <- emmeans(anova_model_n250_nonwords, ~ Orthographic_Sensitivity * family_size * complexity))

# Get all pairwise contrasts
emm3_contrasts <- contrast(emm3,  method = "pairwise", by = NULL, adjust = "none")
# emm3_contrasts

# Keep only the contrasts you want
# Simple effects of family_size at each level of complexity
# Simple effects of complexity at each level of family_size
keep3 <- c("High Orthographic Large Simple - High Orthographic Large Complex",
           "High Orthographic Small Simple - High Orthographic Small Complex",
           "Low Orthographic Large Simple - Low Orthographic Large Complex",
           "Low Orthographic Small Simple - Low Orthographic Small Complex",
           "High Orthographic Small Simple - High Orthographic Large Simple",
           "High Orthographic Small Complex - High Orthographic Large Complex",
           "Low Orthographic Small Simple - High Orthographic Large Simple",
           "Low Orthographic Small Complex - Low Orthographic Large Complex",
           "High Orthographic Large Simple - Low Orthographic Large Simple",
           "High Orthographic Large Complex - Low Orthographic Large Complex",
           "High Orthographic Small Simple - Low Orthographic Small Simple",
           "High Orthographic Small Complex - Low Orthographic Small Complex")

(emm3_contrasts_filtered <- subset(emm3_contrasts, contrast %in% keep3))

# Get Confidence Intervals
(emm3_contrasts_filtered_ci <- confint(emm3_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs3 <- eff_size(emm3, sigma = sigma(m2), edf = df.residual(m2))

# Remove the  redundant rows 
(effs3_filtered <- subset(effs3, contrast %in% keep3))
```

  - For low-orthographic participants, the N250 is more negative overall (especially for simple items).
<br>  
  - For high-orthographic participants, N250 amplitudes are relatively stable across *complexity* and *family size*.
 <br>   
  - The trend-level `Orthographic_Sensitivity × Complexity` effect arises because low-sensitivity participants show a stronger complexity contrast (more negative for simple than complex), while high-sensitivity participants do not.
<br>  


### Interaction Contrasts

The interaction contrast tests whether the difference in the complexity effect for large vs small families differs across sensitivity?


$$
 [ [(A_1 - A_2)\text{ in }B_1] - [(A_1 - A_2)\text{ in }B_2]
    \text{ in Condition }C_1]
  -
  [[(A_1 - A_2) in B_1] - [(A_1 - A_2) in B_2]
    \text{in Condition }C_2]
$$

<br>
```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare complexity effect in large vs small family)
contrast(emm2, interaction = "pairwise", by = NULL, adjust = "holm")
confint(contrast(emm2, interaction = c("pairwise", "pairwise")))

# Compute the A1 - A2 difference within each combination of B × C
(complexity_diff <- contrast(emm2, method = "revpairwise", 
                            by = c("Orthographic_sensitivity", "family_size"), 
                            simple = "complexity"))

# Compute how that A-effect changes across the levels of B, separately for each level of C  
(family_size_complexity_int_within_sensitivity <- contrast(complexity_diff, 
                                                           method = "revpairwise",
                                                           by = "Orthographic_sensitivity", simple = "family_size"))

# Get confidence intervals
confint(family_size_complexity_int_within_sensitivity)
```



	1.	Within the **high-orthographic group**, N250 amplitudes are fairly stable. There are no consistent effects of complexity or family size. All amplitudes hover around -0.5 µV to -0.6 µV.  Their responses are uniformly moderate across all conditions.

	2.	Within the **low-orthographic group**, there is a clear complexity effect. 
	
	- Large-family nonwords: Simple = -1.13 µV vs. Complex = -0.13 µV --> strong difference (~1 µV).

	- Small-family nonwords: Simple = -1.24 µV vs. Complex = -0.50 µV → moderate difference (~0.7 µV).
  
 The complexity effect (Simple < Complex) is present for both family sizes but is stronger for large-family items.

	3.	The complexity effect depends on both family size and sensitivity: For high-orthographic participants there is essentially no complexity effect at either family size. For low-orthographic participants there is aa complexity effect, especially for large-family nonwords.

That pattern—where the complexity effect appears only for low-orthographic participants and is amplified for large-family items—is what drives the marginal 3-way interaction.

N250 amplitude (µV, more negative = larger N250)

|                   |  Complex  |    Simple |  Delta(Simple–Complex)|
|-------------------|-----------|-----------|-----------------------|
|High-Ortho Large   |  -0.62    |     -0.48 |  +0.14   ($\approx 0$)|
|High-Ortho Small   |  -0.60    |     -0.63 |  -0.03   ($\approx 0$)|
|Low-Ortho  Large   |  -0.13    |     -1.13 |  -1.00   (large)      |
|Low-Ortho  Small   |  -0.50    |     -1.24 |  -0.74   (moderate)   |

<br>
The marginal three-way interaction reflects that the complexity effect on the N250 (more negative for simple than complex nonwords) occurs only among participants low in orthographic sensitivity, and this effect is strongest when the nonwords are derived from large morphological families.

In contrast, high-orthographic participants show similar N250 amplitudes across all combinations of family size and complexity, indicating more uniform, automatized form processing.

Thus, the three-way pattern suggests that individuals with weaker orthographic representations rely more heavily on morphological cues: when these cues are abundant (large family, complex form), processing is easier (less negative N250), but when such cues are sparse (large family, simple form), processing is effortful (more negative N250).

## Plots
<br>  
```{r, fig.width=8, fig.height=3}

p3 <- emmip(anova_model_n250_nonwords, Orthographic_Sensitivity ~ complexity | family_size) + my_style
p4 <- emmip(anova_model_n250_nonwords, family_size  ~ complexity | Orthographic_Sensitivity) + my_style
p5 <- emmip(anova_model_n250_nonwords, Orthographic_Sensitivity ~ family_size | complexity) + my_style

plot_grid(p3, p4, p5, ncol = 3)
```






