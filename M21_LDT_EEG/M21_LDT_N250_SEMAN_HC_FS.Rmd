---
title: "M21 LDT ERP HC SEMANTIC SENSITIVITY N250 Family Size"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: inline
---

\scriptsize

# Set parameters {-}
Set chunk parameters
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "||")
options(width = 140)
```



Load libraries
```{r, echo=FALSE}
library(tidyverse)
library(ggeffects)
library(lme4)
library(afex)
library(gridExtra)
library(emmeans)
library(effectsize)
library(performance)
library(cowplot)  # for use with `plot_grid(x,x,ncol = x)` function
library(e1071) # for use with `skewness()` function
library(lmerTest)
```


Set ggplot parameters
```{r, echo=FALSE}
theme_set(theme_classic() +  
            theme(legend.position = "bottom", 
                  axis.text=element_text(size=8.5),
                  axis.title=element_text(size=9)))

# Define a custom color palette
my_palette <- c("#A6CEE3",  "#FB9A99")
my_palette_2 <- c( "#1F78B4","#E31A1C" )
my_palette_3 <- c("#A6CEE3","#1F78B4","#FB9A99","#E31A1C")


# Create a function to apply this palette
scale_color_custom <- function() {
  scale_color_manual(values = my_palette_2)
}

scale_fill_custom <- function() {
  scale_fill_manual(values = my_palette_2)
}
```


Define standard error of the mean function
```{r, echo=FALSE}
sem <- function(x) sd(x)/sqrt(length(x))
```


# Load data files 

```{r}
dir_path <- "CSV files"

erp_2A <- read_csv(file.path(dir_path, "fs_m21_ldt_mea_200300_050050_1_AB.csv"))
erp_2B <- read_csv(file.path(dir_path, "fs_m21_ldt_mea_200300_050050_1_BA.csv"))
erp_4A <- read_csv(file.path(dir_path, "fs_m21_ldt_mea_300500_050050_1_AB.csv"))
erp_4B <- read_csv(file.path(dir_path, "fs_m21_ldt_mea_300500_050050_1_BA.csv"))
dmg_lng_vsl <- read_csv(file.path(dir_path, "demo_lang_vsl_pca_hc.csv"))
```

```{r}
library(dplyr)

erp_2i <- bind_rows(
  erp_2A |> mutate(List = "AB"),
  erp_2B |> mutate(List = "BA")
)
```

Now we extract `SubjID` from the `ERPset` column
```{r, , echo=FALSE}

# Remove '_LDT_diff_waves' from each string in the ERPset column
# This code first renames the column and then applies the `str_replace` function 
# to the newly renamed column.
erp_2ii <- erp_2i |>
  rename(SubjID = ERPset) |>
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", "")) |>
  select(-mlabel)

# erp_4 <- erp_4 |>
#   rename(SubjID = ERPset) |>
#   mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
#   mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
#   mutate(binlabel = str_replace(binlabel, "_family", "")) |>
#   select(-mlabel)
```

We then join the ERP data and language into a single data frame
```{r, echo=FALSE}

erp_2iii <- erp_2ii |>
  left_join(dmg_lng_vsl, by = "SubjID") |>
  select(SubjID, everything()) 
# n400 <- erp_4 |>
#   left_join(dmg_lng_vsl, by = "SubjID") |>
#   select(SubjID, everything()) 
```

# Format data files 

Divide into word, non-word and difference wave dataframes
```{r, echo=FALSE}
n250_words <- erp_2iii |> filter(bini %in% c(1:2))    # does not include BF data
n250_words_b <- erp_2iii |> filter(bini %in% c(9:12)) # includes BF data
n250_nonwords <- erp_2iii |> filter(bini %in% c(3:6))

```

Then we do some more formatting and cleanup of the dataframes.We  create separate columns, one for each independent variable (anteriority, laterality, morphological family size). To do this we have to use `seperate` function from the `stringr` package. Run `vignette("programming", package = "dplyr")` to see more about `tidy-selection` and `tidy-evaluation`.

```{r, echo=FALSE}
# Words
n250_words <- n250_words |>
  separate(binlabel, into = c("trial_type","family_size"), sep = "_", remove = TRUE) |>
  select(-trial_type)
n250_words_b <- n250_words_b |>
  separate(binlabel, into = c("trial_type", "family_size","tmp1", "base_freq", "tmp2"), sep = "_", remove = TRUE) |>
  select(-c(trial_type, tmp1, tmp2))


# Assuming your data frame is named 'df' and the column is named 'your_column'

n250_words_b$family_size[n250_words_b$family_size == "large"] <- "Large"
n250_words_b$family_size[n250_words_b$family_size == "small"] <- "Small"


# Nonwords
n250_nonwords <- n250_nonwords |>
  separate(binlabel, into = c("trial_type", "family_size", "complexity"), sep = "_", remove = TRUE) |>
  select(-trial_type)

# Assuming your data frame is named 'df' and the column is named 'your_column'
n250_nonwords$complexity[n250_nonwords$complexity == "complex"] <- "Complex"
n250_nonwords$complexity[n250_nonwords$complexity == "simple"] <- "Simple"
n250_nonwords$family_size[n250_nonwords$family_size == "large"] <- "Large"
n250_nonwords$family_size[n250_nonwords$family_size == "small"] <- "Small"

# Format as factors
n250_nonwords <- n250_nonwords |>
  mutate(complexity = fct_relevel(complexity, "Simple", "Complex"))

n250_nonwords <- n250_nonwords |>
  mutate(family_size = fct_relevel(family_size, "Small", "Large"))

# str(n250_nonwords)
```

Now we need to  extract just the bins and channels that we intend to analyse. For this analysis we will use 9 channels:  F3, Fz, F4, C3, Cz, C4, P3, Pz, P4 . We will use the`mutate` function from the `dplyr` package along with the `case_when` function. The `case_when` function  is a sequence of two-sided formulas. The left hand side determines which values match this case. The right hand side provides the replacement value.

```{r, echo=FALSE}

channels_1 <-  c(3, 2, 25, 7, 20, 21, 12, 11, 16)

# Words
n250_words <- n250_words |>
 filter(chindex %in% channels_1) |> 
 select(-`Included VSL2`)

n250_words_b <- n250_words_b |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)

# Nonwords
n250_nonwords <- n250_nonwords |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)
```


# N250 Word Data

Statistical analysis.

Linear mixed-effects models were fit using the afex::mixed function (method = “KR”) to account for both subject-level and electrode-level variability. Each model included random intercepts for participants (SubjID) and electrodes nested within participants (SubjID:chlabel), as well as by-subject random slopes for within-subject factors ( Family Size, Complexity, or Base Frequency, depending on the analysis). When a significant interaction was obtained, we probed it using estimated marginal means from the fitted model (emmeans package) to clarify the source of the effect.
Because these follow-up contrasts were intended to interpret a significant higher-order interaction rather than to test independent hypotheses, we reported uncorrected p-values (adjust = "none") for interpretive clarity. The robustness of the overall pattern was verified using a Holm correction, which did not change the substantive conclusions.

## Nested ANOVA Model
```{r}
#Fit ANOVA model
anova_model_n250_words_b <- mixed(
    value ~ Semantic_Sensitivity * family_size * base_freq +
    (1 + family_size + base_freq | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n250_words_b,
  method = "KR"
)
anova_model_n250_words_b 

m1 <- anova_model_n250_words_b$full_model    # Extract the lmer model
ranova(m1) # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n250_words_b, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n250_words_b)
```
## Main Effects

No significant main effects

## Interactions


|                                    Effect|      df|         F| p.value||eta-sqrd|  
|------------------------------------------|--------|----------|--------|---------|
|                     family_size:base_freq| 1, 1498| 32.72 ***|   <.001|    0.02 |
|Semantic_Sensitivity:family_size:base_freq| 1, 1498| 16.96 ***|   <.001|    0.01 |


### Simple Contrasts
<br>
```{r}
# Estimated marginal means for the family_size × base frequency interaction
(emm1 <- emmeans(anova_model_n250_words_b, ~ family_size * base_freq))

# Get all pairswise contrasts
emm1_contrasts <- contrast(emm1,  method = "pairwise", by = NULL, adjust = "none")

# Keep only the contrasts you want
# Simple effects of family_size at each level of base_freq
# Simple effects of base_freq at each level of family_size
keep <- c("Large High - Small High",
          "Large Low - Small Low",
          "Large High - Large Low",
          "Small High - Small Low")
(emm1_contrasts_filtered <- subset(emm1_contrasts, contrast %in% keep))

# Get Confidence Intervals
(emm1_contrasts_filtered_ci <- confint(emm1_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs1 <- eff_size(emm1, sigma = sigma(m1), edf = df.residual(m1))

# Remove the two redundant rows (rows 3 and 4)
(effs1_filtered <- subset(effs1, !contrast %in% c("Large High - Small Low",
                                                  "Small High - Large Low")))
```

For large-family words, N250 amplitude is more negative when base frequency is high than when it is low. For small-family words, base frequency has little effect. For low-frequency bases, small-family words elicit more negative amplitudes than large-family words.

- At **High Base Frequency**:  `Large` vs. `Small` family; no difference (-0.027, *p* = .92). Family size doesn’t matter when base frequency is high.

- At **Low Base Frequency**: `Large` vs. `Small` family; significant difference (0.66, *p* = .016). When base frequency is low, small-family words yield more negative amplitudes than large-family words

- Within **Small Family**: `High` vs. `Low` base frequency; not significant (0.18, *p* = .43). Small-family words are unaffected by base frequency.

- Within **Large Family**: High vs. Low base frequency → significant (-0.51, *p* = .029). Large-family words show more negative amplitudes when their base frequency is high.

### Interaction Contrasts
<br>
```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare base frequency effect in large vs small family)
contrast(emm1, interaction = "pairwise", by = NULL, adjust = "holm")

# Get confidence intervals, for each base frequency effect for each family size and then for interaction effect
confint(contrast(emmeans(m1, ~ family_size | base_freq), "pairwise"))
confint(contrast(emm1, interaction = c("pairwise", "pairwise")))
```

## Plots

```{r}
emm1_df <- as.data.frame(emm1)
p1<- ggplot(emm1_df,
       aes(x = base_freq, y = emmean,
           color = family_size, group = family_size)) +
  geom_line(position = position_dodge(0.2)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  labs(x = "Base Frequency", y = "Estimated N250 amplitude",
       color = "Family Size",
       title = "Family Size × Base Frequency") +
  scale_color_custom() +
  scale_fill_custom() 

p2 <- ggplot(emm1_df,
       aes(x = family_size, y = emmean,
           color = base_freq, group = base_freq)) +
  geom_line(position = position_dodge(0.2)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  labs(x = "Family Size", y = "Estimated N250 amplitude",
       color = "Base Frequency",
       title = "Base Frequency × Family Size") +
  scale_color_custom() +
  scale_fill_custom() 

plot_grid(p1, p2, ncol = 2)
```


\newpage

# N250 Nonword Data

```{r, results='hide'}

n250_nonwords %>%
  count(family_size, complexity, Semantic_Sensitivity)

n250_nonwords |> filter(family_size == "complex")
```


## Compute the ANOVA


```{r}
anova_model_n250_nonwords <- mixed(
    value ~ Semantic_Sensitivity * family_size * complexity +
    (1 + family_size + complexity | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n250_nonwords,
  method = "KR"
)
anova_model_n250_nonwords 

m2 <- anova_model_n250_nonwords$full_model    # Extract the lmer model
ranova(m2)    # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n250_nonwords, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n250_nonwords)
```


## Main Effects

No main effects.  

## Interactions

A three way interaction between

  - `Sensitivity × Family Size × Complexity`: significant (*t* = 4.71, *p* =.03).

###  Simple Contrasts

Compare High vs Low Semantic Sensitivity within each combination of Family Size and Complexity  

This gives you: 4 contrasts: one for each `Family Size × Complexity` combination. Each shows whether `High` vs `Low Semantic Sensitivity` differs significantly

If simple effects aren’t significant, try looking at interaction contrasts, which test differences in the differences. You’re now asking: Does the effect of Sensitivity change more in some complexity/family combinations than others?
<br>
```{r}
# Estimated marginal means for the family_size × complexity interaction
(emm2 <- emmeans(anova_model_n250_nonwords, ~ Semantic_Sensitivity * family_size * complexity))

# Get all pairswise contrasts
emm2_contrasts <- contrast(emm2,  method = "pairwise", by = NULL, adjust = "none")
# emm2_contrasts

# Keep only the contrasts you want
# Simple effects of family_size at each level of complexity
# Simple effects of complexity at each level of family_size
keep2 <- c("High Semantic Large Simple - High Semantic Large Complex",
           "High Semantic Small Simple - High Semantic Small Complex",
           "Low Semantic Large Simple - Low Semantic Large Complex",
           "Low Semantic Small Simple - Low Semantic Small Complex",
           "High Semantic Small Simple - High Semantic Large Simple",
           "High Semantic Small Complex - High Semantic Large Complex",
           "Low Semantic Small Simple - High Semantic Large Simple",
           "Low Semantic Small Complex - Low Semantic Large Complex",
           "High Semantic Large Simple - Low Semantic Large Simple",
           "High Semantic Large Complex - Low Semantic Large Complex",
           "High Semantic Small Simple - Low Semantic Small Simple",
           "High Semantic Small Complex - Low Semantic Small Complex")

(emm2_contrasts_filtered <- subset(emm2_contrasts, contrast %in% keep2))

# Get Confidence Intervals
(emm2_contrasts_filtered_ci <- confint(emm2_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs2 <- eff_size(emm2, sigma = sigma(m2), edf = df.residual(m2))

# Remove the  redundant rows 
(effs2_filtered <- subset(effs2, contrast %in% keep2))
```

### Interaction Contrasts

The interaction contrast tests whether the difference in the complexity effect for large vs small families differs across sensitivity?


$$
 [ [(A_1 - A_2)\text{ in }B_1] - [(A_1 - A_2)\text{ in }B_2]
    \text{ in Condition }C_1]
  -
  [[(A_1 - A_2) in B_1] - [(A_1 - A_2)\text{ in }B_2]
    \text{ in Condition }C_2]
$$

<br>
```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare complexity effect in large vs small family)
contrast(emm2, interaction = "pairwise", by = NULL, adjust = "holm")
confint(contrast(emm2, interaction = c("pairwise", "pairwise")))

# Compute the A1 - A2 difference within each combination of B × C
(complexity_diff <- contrast(emm2, method = "revpairwise", 
                            by = c("Semantic_sensitivity", "family_size"), 
                            simple = "complexity"))

# Compute how that A-effect changes across the levels of B, separately for each level of C  
(family_size_complexity_int_within_sensitivity <- contrast(complexity_diff, 
                                                           method = "revpairwise",
                                                           by = "Semantic_sensitivity", simple = "family_size"))

# Get confidence intervals
confint(family_size_complexity_int_within_sensitivity)
```


Compute the effect of Complexity (Complex - Simple) within each Semantic Sensitivity × Family Size combination.


High Sensitivity- Small Family: `Complex - Simple = -0.743 - (-1.002) = +0.256`

High Sensitivity- Large Family: `Complex - Simple = -0.615 - (-0.629) = +0.014`

Low Sensitivity - Small Family: `Complex - Simple = -0.4267 - (-0.78) = +0.4277`

Low Sensitivity - Large Family: `Complex - Simple = -0.902 - (-0.902) = +0.709`


Compute the difference of differences:  compare how the effect of complexity differs across sensitivity groups:`(High Sensitivity complexity effect) - (Low Sensitivity complexity effect)`

For Large Family: 
```
High:  +0.114
Low:   +0.106
Difference: 0.114 - 0.106 = +0.008
```

For Small Family: 
```
High: -0.314  
Low:  +0.197  
Difference: -0.314 - (+0.197) = -0.511
```

This is a reversal of the complexity effect between High and Low sensitivity participants for Small Family nonwords — and that’s the core of your significant 3-way interaction.

Now take the difference of these differences (Small - Large):  `-0.511 - 0.008 = -0.519`.  That’s the interaction contrast estimate: ` -0.52, p = .0325`

	-	$SE = 0.243$, $df = 1523$, $t = 2.140$ --> yields $p = 0.0325$, so it is statistically significant (given Bonferroni correction, etc.).

The three-way interaction reflects the fact that High and Low sensitivity participants show opposite complexity effects — but only in the Small Family condition. In Large families, their complexity effects are essentially the same.

In `Small families`, `High sensitivity` participants respond more negatively to `complex items`, white `Low sensitivity` participants respond more negatively to `simple items`.

This crossover in the complexity effect is what drives the significant interaction — even though none of the simple effects are individually significant.

## Plots

## Plots
<br>  
```{r}
emmip(anova_model_n250_nonwords, Semantic_Sensitivity ~ family_size  | complexity)
```




Interpretation
 -	This is an interaction contrast (a “contrast of contrasts”) across your three factors (Semantic Sensitivity × Family Size × Complexity).
	
 -	Specifically, it is testing whether the difference (`Complex – Simple`)  for (`Large Family` vs. `Small Family`) differs between the two levels of `Semantic Sensitivity`. 
	
The contrast is asking: “Is the effect of `complexity`, in the contrast `Large` vs. `Small` family, different in `High Semantic` vs.` Low Semantic` participants?”

 -	The `estimate = 0.5`2 is the difference in differences (i.e. the slope difference) on your response metric (N250 amplitude).
	
 -	Because you used adjust = "bonferroni" and combine = TRUE, this contrast is part of a “family” of interaction contrasts that have been adjusted for multiple comparisons.

So in more conversational terms: you have evidence that High Semantic readers show a different `complexity × family size` effect than Low Semantic readers — in particular, in how the `complexity` effect (`Complex` vs. `Simple`) differs when comparing `Large` vs. `Small` family.
 
Suggests that sensitivity does influence the N250, but only in how it modulates the joint effect of family size and complexity. In other words: the way family size and complexity interact depends on whether participants are semantically sensitive or not.

  - Marginal $R^2 = 0.2%$ --> the fixed predictors (including sensitivity) account for very little variance overall.
	
  - Conditional $R^2 = 76%$ --> most variance is indeed explained by subjects and electrodes (as anticipated).

Most of the variability in N250 amplitude reflects differences across participants and electrode sites, as expected for ERP data. Semantic sensitivity did not produce an overall shift in N250 responses, but it did moderate the combined influence of family size and morphological complexity. This interaction was statistically significant but accounted for only a very small portion of the variance. Thus, semantic sensitivity may play a role in how multiple lexical factors are integrated during early morphological processing, though the effect is subtle.


