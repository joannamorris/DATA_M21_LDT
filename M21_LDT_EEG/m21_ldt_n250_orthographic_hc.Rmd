---
title: "M21 LDT ERP HC ORTHOGRAPIC SENSITIVITY"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

\scriptsize

# Set parameters {-}
Set chunk parameters
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "||")
options(width = 140)
```



Load libraries
```{r, echo=FALSE}
library(tidyverse)
library(ggeffects)
library(lme4)
library(afex)
library(gridExtra)
library(emmeans)
library(effectsize)
library(performance)
library(cowplot)  # for use with `plot_grid(x,x,ncol = x)` function
library(e1071) # for use with `skewness()` function
library(lmerTest)
```


Set ggplot parameters
```{r, echo=FALSE}
theme_set(theme_classic() +  
            theme(legend.position = "bottom", 
                  axis.text=element_text(size=8.5),
                  axis.title=element_text(size=9)))

# Define a custom color palette
my_palette <- c("#A6CEE3",  "#FB9A99")
my_palette_2 <- c( "#1F78B4","#E31A1C" )
my_palette_3 <- c("#A6CEE3","#1F78B4","#FB9A99","#E31A1C")


# Create a function to apply this palette
scale_color_custom <- function() {
  scale_color_manual(values = my_palette_2)
}

scale_fill_custom <- function() {
  scale_fill_manual(values = my_palette_2)
}
```


Define standard error of the mean function
```{r, echo=FALSE}
sem <- function(x) sd(x)/sqrt(length(x))
```


# Load and format data files 

```{r}
dir_path <- "CSV files"

erp_2 <- read_csv(file.path(dir_path, "m21_ldt_mea_200300_050050_1.csv"))
erp_4 <- read_csv(file.path(dir_path, "m21_ldt_mea_300500_050050_1.csv"))
dmg_lng_vsl <- read_csv(file.path(dir_path, "demo_lang_vsl_pca_hc.csv"))
```


Now we extract `SubjID` from the `ERPset` column
```{r, , echo=FALSE}

# Remove '_LDT_diff_waves' from each string in the ERPset column
# This code first renames the column and then applies the `str_replace` function 
# to the newly renamed column.
erp_2 <- erp_2 |>
  rename(SubjID = ERPset) |>
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", "")) |>
  select(-mlabel)

erp_4 <- erp_4 |>
  rename(SubjID = ERPset) |>
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", "")) |>
  select(-mlabel)
```

We then join the ERP data and language into a single data frame
```{r, echo=FALSE}

n250 <- erp_2 |>
  left_join(dmg_lng_vsl, by = "SubjID") |>
  select(SubjID, everything()) 
n400 <- erp_4 |>
  left_join(dmg_lng_vsl, by = "SubjID") |>
  select(SubjID, everything()) 
```

Divide into word, non-word and difference wave dataframes
```{r, echo=FALSE}
n250_words <- n250 |> filter(bini %in% c(1:2))    # does not include BF data
n250_words_b <- n250 |> filter(bini %in% c(9:12)) # includes BF data
n250_nonwords <- n250 |> filter(bini %in% c(3:6))

```

Then we do some more formatting and cleanup of the dataframes.We  create separate columns, one for each independent variable (anteriority, laterality, morphological family size). To do this we have to use `seperate` function from the `stringr` package. Run `vignette("programming", package = "dplyr")` to see more about `tidy-selection` and `tidy-evaluation`.

```{r, echo=FALSE}
# Words
n250_words <- n250_words |>
  separate(binlabel, into = c("trial_type","family_size"), sep = "_", remove = TRUE) |>
  select(-trial_type)
n250_words_b <- n250_words_b |>
  separate(binlabel, into = c("trial_type", "family_size","tmp1", "base_freq", "tmp2"), sep = "_", remove = TRUE) |>
  select(-c(trial_type, tmp1, tmp2))


# Assuming your data frame is named 'df' and the column is named 'your_column'
n250_words_b$Orthographic_Sensitivity[n250_words_b$Orthographic_Sensitivity == "Low"] <- "Low Sensitivity"
n250_words_b$Orthographic_Sensitivity[n250_words_b$Orthographic_Sensitivity == "High"] <- "High Sensitivity"
n250_words_b$base_freq[n250_words_b$base_freq == "Low"] <- "Low Base Frequency"
n250_words_b$base_freq[n250_words_b$base_freq == "High"] <- "High Base Frequency"
n250_words_b$family_size[n250_words_b$family_size == "large"] <- "Large Family"
n250_words_b$family_size[n250_words_b$family_size == "small"] <- "Small Family"


# Nonwords
n250_nonwords <- n250_nonwords |>
  separate(binlabel, into = c("trial_type", "family_size", "complexity"), sep = "_", remove = TRUE) |>
  select(-trial_type)

# Assuming your data frame is named 'df' and the column is named 'your_column'
n250_nonwords$Orthographic_Sensitivity[n250_nonwords$Orthographic_Sensitivity == "Low"] <- "Low Sensitivity"
n250_nonwords$Orthographic_Sensitivity[n250_nonwords$Orthographic_Sensitivity == "High"] <- "High Sensitivity"
n250_nonwords$complexity[n250_nonwords$complexity == "complex"] <- "Complex"
n250_nonwords$complexity[n250_nonwords$complexity == "simple"] <- "Simple"
n250_nonwords$family_size[n250_nonwords$family_size == "large"] <- "Large Family"
n250_nonwords$family_size[n250_nonwords$family_size == "small"] <- "Small Family"

```

Now we need to  extract just the bins and channels that we intend to analyse. For this analysis we will use 9 channels:  F3, Fz, F4, C3, Cz, C4, P3, Pz, P4 . We will use the`mutate` function from the `dplyr` package along with the `case_when` function. The `case_when` function  is a sequence of two-sided formulas. The left hand side determines which values match this case. The right hand side provides the replacement value.

```{r, echo=FALSE}

channels_1 <-  c(3, 2, 25, 7, 20, 21, 12, 11, 16)

# Words
n250_words <- n250_words |>
 filter(chindex %in% channels_1) |> 
 select(-`Included VSL2`)

n250_words_b <- n250_words_b |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)

# Nonwords
n250_nonwords <- n250_nonwords |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)
```


# N250 Word Data

## Nested ANOVA Model
```{r}
#Fit ANOVA model
anova_model_n250_words_b <- mixed(
    value ~ Orthographic_Sensitivity * family_size * base_freq +
    (1 + family_size + base_freq | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n250_words_b,
  method = "KR"
)
anova_model_n250_words_b 

m1 <- anova_model_n250_words_b$full_model    # Extract the lmer model
ranova(m1) # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n250_words_b, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n250_words_b)
```


## Significant Effects

|                            Effect|      df|         F| p.value||eta-sqrd|  
|----------------------------------|--------|----------|--------|---------|
|             family_size:base_freq| 1, 1523| 35.14 ***|   <.001|6.76e-03 |


### Main Effects

No significant main effects

### Interactions

```{r}
# `base_freq` x `family_size` interaction

# Estimated marginal means for the family_size × base_freq interaction
emm <- emmeans(anova_model_n250_words_b, ~ family_size * base_freq)

# Look at the table of estimated means
emm

# Simple effects of family_size at each level of base_freq
contrast(emm, method = "pairwise", by = "base_freq", adjust = "holm")

# Simple effects of base_freq at each level of family_size
contrast(emm, method = "pairwise", by = "family_size", adjust = "holm")

# Interaction contrasts (e.g., difference of differences)
contrast(emm, interaction = "pairwise", adjust = "holm")

```

For large-family words, N250 amplitude is more negative when base frequency is high than when it is low. For small-family words, base frequency has little effect. For low-frequency bases, small-family words elicit more negative amplitudes than large-family words.

- At **High Base Frequency**:  `Large` vs. `Small` family → no difference (*p* = .74). Family size doesn’t matter when base frequency is high.

- Within **Small Family**: `High` vs. `Low` base frequency → not significant (*p* = .60). Small-family words are unaffected by base frequency.

- At **Low Base Frequency**: Large vs. Small family → significant difference (*p* = .022). Small-family words yield more negative amplitudes than large-family words, but only when base frequency is low.

- Within **Large Family**: High vs. Low base frequency → significant (*p* = .012). Large-family words show more negative amplitudes when their base frequency is high.

## Plots

```{r}
# Get estimated marginal means for the interaction
emm <- emmeans(anova_model_n250_words_b, ~ family_size * base_freq)

# Convert to data frame for plotting
emm_df <- as.data.frame(emm)

# Plot barchart
p1 <- ggplot(emm_df, aes(x = base_freq, y = emmean,
                   color = family_size, fill = family_size)) +
  geom_col(alpha = .4, position = position_dodge(0.9)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
              width = 0.1, position = position_dodge(0.9)) +
    geom_text(aes(label = round(emmean, digits = 2)), colour = "black", size = 2.5, vjust = -12,
            position = position_dodge(.9)) +
  labs(x = "Base Frequency",
      y = "Estimated Marginal Mean (N250 amplitude)",
      title = "Family Size × Base Frequency \n Interaction (N250)") +
  scale_color_custom() +
  scale_fill_custom() +
  coord_cartesian(ylim = c(-1.3, .15)) +
  theme(legend.title = element_blank())


p2 <- ggplot(emm_df, aes(x = family_size, y = emmean,
                   color = base_freq, fill = base_freq)) +
  geom_col(alpha = .4, position = position_dodge(0.9)) +
  geom_text(aes(label = round(emmean, digits = 2)), colour = "black", size = 2.5, vjust = -12,
            position = position_dodge(.9)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
              width = 0.1, position = position_dodge(0.9)) +
  labs(x = "Family Size",
      y = "Estimated Marginal Mean (N250 amplitude)",
      title = "Family Size × Base Frequency \n Interaction (N250)") +
  scale_color_custom() +
  scale_fill_custom() +
  coord_cartesian(ylim = c(-1.3, .15)) +
  theme(legend.title = element_blank())

plot_grid(p1, p2, ncol = 2)
```

# N250 Nonword Data

## Compute the ANOVA

```{r}
anova_model_n250_nonwords <- mixed(
    value ~ Orthographic_Sensitivity * family_size * complexity +
    (1 + family_size + complexity | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n250_nonwords,
  method = "KR"
)
anova_model_n250_nonwords 

m1 <- anova_model_n250_nonwords$full_model    # Extract the lmer model
ranova(m1)    # Run random effects comparison



# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n250_nonwords, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n250_nonwords)
```


## Main Effects

No main effects.  

## Interactions

A three way interaction between

  - `Sensitivity × Family Size × Complexity`: significant (*p* =.033).

### Compare High vs Low Orthographic Sensitivity within each combination of Family Size and Complexity  

This gives you: 4 contrasts: one for each `Family Size × Complexity` combination. Each shows whether `High` vs `Low Orthographic Sensitivity` differs significantly

```{r}
sensitivity_effects <- contrast(emm1,
  method = "pairwise",
  by = c("family_size", "complexity"),
  simple = "Orthographic_Sensitivity",
  adjust = "bonferroni")  # or "none" if you want raw p-values

summary(sensitivity_effects)
```
  
### Does Sensitivity Affect Simple More Than Complex?

This is asking: “Is the sensitivity effect (High − Low) bigger for Simple than Complex?” This tells you: whether the effect of `Sensitivity` is modulated by `Complexity` and whether that pattern differs between `Large` and `Small` families

```{r}
# Sensitivity × Complexity interaction within Family Size
interaction_contrasts <- contrast(emm,
  interaction = c("pairwise", "pairwise"),
  by = "family_size",   # run this separately for Large and Small
  adjust = "holm")

summary(interaction_contrasts)
```

  
```{r}
# 1. Get the EMM grid for all combinations
(emm1 <- emmeans(anova_model_n250_nonwords, ~ Orthographic_Sensitivity * family_size * complexity))


# “Simple effects” contrasts:
#    a) Effect of complexity within each Sensitivity × family_size cell
(contrast(emm1, method = "pairwise", by = c("Orthographic_Sensitivity", "family_size"),
         simple = "complexity", adjust = "holm"))

#    b) Effect of family_size within each Sensitivity × complexity cell
(contrast(emm1, method = "pairwise", by = c("Orthographic_Sensitivity", "complexity"),
         simple = "family_size", adjust = "holm"))

#    c) Effect of Sensitivity within each family_size × complexity cell
(contrast(emm1, method = "pairwise", by = c("family_size", "complexity"),
         simple = "Orthographic_Sensitivity", adjust = "holm"))

#  Interaction contrasts (difference-of-differences)
#    Compare (complexity effect in large vs small family) across sensitivity
(contrast(emm1, interaction = c("pairwise", "pairwise"), by = NULL, adjust = "holm"))

(contrast(emm1, interaction = c("pairwise", "pairwise"), combine = TRUE, adjust = "bonferroni"))

# Optionally: get confidence intervals
(confint(contrast(emm1, interaction = c("pairwise", "pairwise"))))
```

```{r}
all_contr <- contrast(emm1, interaction = c("pairwise", "pairwise"), combine = TRUE, adjust = "bonferroni")

# 1. Summarize contrasts
(sumc <- summary(all_contr))

# Construct a contrast name string
contrast_name <- paste( sumc$Orthographic_Sensitivity_pairwise, sumc$family_size_pairwise,
  sumc$complexity_pairwise,sep = " — ")  # or whatever separator you prefer

# 2. Extract values
est <- sumc$estimate
se <- sumc$SE
df_contr <- sumc$df

# 3. Use the sigma you found
lm_mod <- anova_model_n250_nonwords$full_model
sigma_val <- sigma(lm_mod)

# 4. Compute d and its SE
d <- est / sigma_val
se_d <- se / sigma_val

# 5. Confidence intervals for d (t critical)
alpha <- 0.05
tcrit <- qt(1 - alpha/2, df_contr)
ci_low <- d - tcrit * se_d
ci_high <- d + tcrit * se_d

# 6. Make table
d_table <- data.frame( contrast = contrast_name, 
                       d = d, se_d = se_d, df = df_contr, ci_low = ci_low, ci_high = ci_high)
d_table
```

```{r}
# 6. Plot the interaction
library(ggplot2)

emm1_df <- as.data.frame(emm1)
ggplot(emm1_df,
       aes(x = complexity, y = emmean,
           color = family_size, group = family_size)) +
  facet_wrap(~ Orthographic_Sensitivity) +
  geom_line(position = position_dodge(0.2)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  labs(x = "Complexity", y = "Estimated N250 amplitude",
       color = "Family Size",
       title = "Interaction: Orthographic Sensitivity × Family Size × Complexity") +
  theme_bw() +
  scale_color_custom() +
  scale_fill_custom()
```



Interpretation
 -	This is an interaction contrast (a “contrast of contrasts”) across your three factors (Orthographic Sensitivity × Family Size × Complexity).
	
 -	Specifically, it is testing whether the difference (`Complex – Simple`) in `Complexity` for (`Large Family` vs. `Small Famil`y) differs between the two levels of `Orthographic Sensitivity`. 
	
The contrast is asking: “Is the effect of `complexity`, in the contrast `Large` vs. `Small` family, different in `High Orthographic` vs.` Low Orthographic` participants?”

 -	The `estimate = 0.5`2 is the difference in differences (i.e. the slope difference) on your response metric (N250 amplitude).
	
	-	*SE* = 0.243, *df* = 1523, *t* = 2.140 → yields *p* = 0.0325, so it is statistically significant (given Bonferroni correction, etc.).
	
 -	Because you used adjust = "bonferroni" and combine = TRUE, this contrast is part of a “family” of interaction contrasts that have been adjusted for multiple comparisons.

So in more conversational terms: you have evidence that High Orthographic readers show a different `complexity × family size` effect than Low Orthographic readers — in particular, in how the `complexity` effect (`Complex` vs. `Simple`) differs when comparing `Large` vs. `Small` family.
 
 

 

Suggests that sensitivity does influence the N250, but only in how it modulates the joint effect of family size and complexity. In other words: the way family size and complexity interact depends on whether participants are orthographically sensitive or not.

  - Marginal *R²* = 0.2% → the fixed predictors (including sensitivity) account for very little variance overall.
	
  - Conditional *R²* = 76% → most variance is indeed explained by subjects and electrodes (as anticipated).

Most of the variability in N250 amplitude reflects differences across participants and electrode sites, as expected for ERP data. Orthographic sensitivity did not produce an overall shift in N250 responses, but it did moderate the combined influence of family size and morphological complexity. This interaction was statistically significant but accounted for only a very small portion of the variance. Thus, orthographic sensitivity may play a role in how multiple lexical factors are integrated during early morphological processing, though the effect is subtle.


