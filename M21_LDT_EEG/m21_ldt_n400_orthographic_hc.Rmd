---
title: "M21 LDT ERP HC ORTHOGRAPIC SENSITIVITY N400"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

\scriptsize

# Set parameters {-}
Set chunk parameters
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "||")
options(width = 140)
```



Load libraries
```{r, echo=FALSE}
library(tidyverse)
library(ggeffects)
library(lme4)
library(afex)
library(gridExtra)
library(emmeans)
library(effectsize)
library(performance)
library(cowplot)  # for use with `plot_grid(x,x,ncol = x)` function
library(e1071) # for use with `skewness()` function
library(lmerTest)
```


Set ggplot parameters
```{r, echo=FALSE}
theme_set(theme_classic() +  
            theme(legend.position = "bottom", 
                  axis.text=element_text(size=8.5),
                  axis.title=element_text(size=9)))

# Define a custom color palette
my_palette <- c("#A6CEE3",  "#FB9A99")
my_palette_2 <- c( "#1F78B4","#E31A1C" )
my_palette_3 <- c("#A6CEE3","#1F78B4","#FB9A99","#E31A1C")


# Create a function to apply this palette
scale_color_custom <- function() {
  scale_color_manual(values = my_palette_2)
}

scale_fill_custom <- function() {
  scale_fill_manual(values = my_palette_2)
}
```


Define standard error of the mean function
```{r, echo=FALSE}
sem <- function(x) sd(x)/sqrt(length(x))
```


# Load and format data files 

```{r}
dir_path <- "CSV files"

erp_2 <- read_csv(file.path(dir_path, "m21_ldt_mea_200300_050050_1.csv"))
erp_4 <- read_csv(file.path(dir_path, "m21_ldt_mea_300500_050050_1.csv"))
dmg_lng_vsl <- read_csv(file.path(dir_path, "demo_lang_vsl_pca_hc.csv"))
```


Now we extract `SubjID` from the `ERPset` column
```{r, , echo=FALSE}

# Remove '_LDT_diff_waves' from each string in the ERPset column
# This code first renames the column and then applies the `str_replace` function 
# to the newly renamed column.
erp_2 <- erp_2 |>
  rename(SubjID = ERPset) |>
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", "")) |>
  select(-mlabel)

erp_4 <- erp_4 |>
  rename(SubjID = ERPset) |>
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", "")) |>
  select(-mlabel)
```

We then join the ERP data and language into a single data frame
```{r, echo=FALSE}

n250 <- erp_2 |>
  left_join(dmg_lng_vsl, by = "SubjID") |>
  select(SubjID, everything()) 
n400 <- erp_4 |>
  left_join(dmg_lng_vsl, by = "SubjID") |>
  select(SubjID, everything()) 
```

Divide into word, non-word and difference wave dataframes
```{r, echo=FALSE}

n400_words <- n400 |> filter(bini %in% c(1:2))    # does not include BF data
n400_words_b <- n400 |> filter(bini %in% c(9:12)) # includes BF data
n400_nonwords <- n400 |> filter(bini %in% c(3:6))
```

Then we do some more formatting and cleanup of the dataframes.We  create separate columns, one for each independent variable (anteriority, laterality, morphological family size). To do this we have to use `seperate` function from the `stringr` package. Run `vignette("programming", package = "dplyr")` to see more about `tidy-selection` and `tidy-evaluation`.

```{r, echo=FALSE}
# Words

n400_words <- n400_words |>
  separate(binlabel, into = c("trial_type","family_size"), sep = "_", remove = TRUE) |>
  select(-trial_type)
n400_words_b <- n400_words_b |>
  separate(binlabel, into = c("trial_type", "family_size","tmp1", "base_freq", "tmp2"), sep = "_", remove = TRUE) |>
  select(-c(trial_type, tmp1, tmp2))

# Assuming your data frame is named 'df' and the column is named 'your_column'

n400_words_b$Orthographic_Sensitivity[n400_words_b$Orthographic_Sensitivity == "Low"] <- "Low Sensitivity"
n400_words_b$Orthographic_Sensitivity[n400_words_b$Orthographic_Sensitivity == "High"] <- "High Sensitivity"
n400_words_b$base_freq[n400_words_b$base_freq == "Low"] <- "Low Base Frequency"
n400_words_b$base_freq[n400_words_b$base_freq == "High"] <- "High Base Frequency"
n400_words_b$family_size[n400_words_b$family_size == "large"] <- "Large Family"
n400_words_b$family_size[n400_words_b$family_size == "small"] <- "Small Family"

# Nonwords
n400_nonwords <- n400_nonwords |>
  separate(binlabel, into = c("trial_type", "family_size", "complexity"), sep = "_", remove = TRUE) |>
  select(-trial_type)

# Assuming your data frame is named 'df' and the column is named 'your_column'

n400_nonwords$Orthographic_Sensitivity[n400_nonwords$Orthographic_Sensitivity == "Low"] <- "Low Sensitivity"
n400_nonwords$Orthographic_Sensitivity[n400_nonwords$Orthographic_Sensitivity == "High"] <- "High Sensitivity"
n400_nonwords$complexity[n400_nonwords$complexity == "complex"] <- "Complex"
n400_nonwords$complexity[n400_nonwords$complexity == "simple"] <- "Simple"
n400_nonwords$family_size[n400_nonwords$family_size == "large"] <- "Large Family"
n400_nonwords$family_size[n400_nonwords$family_size == "small"] <- "Small Family"
```

Now we need to  extract just the bins and channels that we intend to analyse. For this analysis we will use 9 channels:  F3, Fz, F4, C3, Cz, C4, P3, Pz, P4 . We will use the`mutate` function from the `dplyr` package along with the `case_when` function. The `case_when` function  is a sequence of two-sided formulas. The left hand side determines which values match this case. The right hand side provides the replacement value.

```{r, echo=FALSE}
channels_1 <-  c(3, 2, 25, 7, 20, 21, 12, 11, 16)

# Words
n400_words <- n400_words |>
 filter(chindex %in% channels_1) |> 
 select(-`Included VSL2`)

n400_words_b <- n400_words_b |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)

# Nonwords
n400_nonwords <- n400_nonwords  |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)
```


# n400 Word Data

## Nested ANOVA Model
```{r}
#Fit ANOVA model
anova_model_n400_words_b <- mixed(
    value ~ Orthographic_Sensitivity * family_size * base_freq +
    (1 + family_size + base_freq | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n400_words_b,
  method = "KR"
)
anova_model_n400_words_b 

m1 <- anova_model_n400_words_b$full_model    # Extract the lmer model
ranova(m1) # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n400_words_b, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n400_words_b)
```

## Main Effects

No significant main effects

## Interactions

A two-way interaction betweeen Family Size and Base Frequency


|                            Effect|      df|         F| p.value||eta-sqrd|  
|----------------------------------|--------|----------|--------|---------|
|             family_size:base_freq| 1, 1523| 61.18 ***|   <.001|    0.04 |


### Simple Contrasts

```{r}
# Estimated marginal means for the family_size × base_freq interaction
emm <- emmeans(anova_model_n400_words_b, ~ family_size * base_freq)

# Look at the table of estimated means
emm

# Simple effects of family_size at each level of base_freq
contrast(emm, method = "pairwise", by = "base_freq", adjust = "holm")

# Simple effects of base_freq at each level of family_size
contrast(emm, method = "pairwise", by = "family_size", adjust = "holm")

```


For large-family words, n400 amplitude is more negative when base frequency is high than when it is low. For small-family words, base frequency has little effect. For low-frequency bases, small-family words elicit more negative amplitudes than large-family words.

- At High base frequency: $\text{Large − Small }= \; –0.136; SE = 0.272; t = –0.501; p = 0.6184$.  This difference is not statistically significant ($p > .05$).

- At Low base frequency: $\text{Large − Small } = 0.866 \; SE = 0.272; t = 3.179; p = 0.0023$. This difference is statistically significant ($p = 0.0023$ after adjustment.

Thus: when base frequency is low, large vs small family_size differ significantly in predicted N400; but when base frequency is high, they do not differ significantly.

Next, contrasting High vs Low base_freq within each family_size: 

- Large family_size: $\text{High − Low }= \; –0.339; SE = 0.196; t = –1.726; p = 0.0886$. This is a trend ($p \sim .09$), but not conventionally significant.

- Small family_size: $\text{High − Low }= \; 0.663; SE = 0.196; t = 3.381; p = 0.0012$. Significant difference: base_freq level matters when family_size is small.

So: when family_size is small, high vs low base frequency yields a significant difference; when family_size is large, the difference is marginal / not strong.

### Interaction Contrasts

```{r}
# Interaction contrasts (e.g., difference of differences)
contrast(emm, interaction = "pairwise", adjust = "holm")
```


The final contrast tests whether the difference between Large vs Small family_size is itself different between High vs Low base_freq: $\text{Estimate }= \; –1.000$; $SE = 0.128$; $t = –7.821$; $p < .0001$

That is, the slope or effect of `family_size` depends strongly on the level of `base_freq` (consistent with your ANOVA). Put differently: the family size difference (Large − Small) is much more positive in the low base frequency condition than it is in the high base frequency condition. That difference of differences is highly significant.


## Plots

```{r}
emm_df <- as.data.frame(emm)
p1<- ggplot(emm_df,
       aes(x = base_freq, y = emmean,
           color = family_size, group = family_size)) +
  geom_line(position = position_dodge(0.2)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  labs(x = "Base Frequency", y = "Estimated N250 amplitude",
       color = "Family Size",
       title = "Family Size × Base Frequency") +
  scale_color_custom() +
  scale_fill_custom() 

p2 <- ggplot(emm_df,
       aes(x = family_size, y = emmean,
           color = base_freq, group = base_freq)) +
  geom_line(position = position_dodge(0.2)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  labs(x = "Family Size", y = "Estimated N250 amplitude",
       color = "Base Frequency",
       title = "Base Frequency × Family Size") +
  scale_color_custom() +
  scale_fill_custom() 

plot_grid(p1, p2, ncol = 2)
```

\newpage

# n400 Nonword Data

## Compute the ANOVA

```{r}
anova_model_n400_nonwords_a <- lmer(
    value ~ Orthographic_Sensitivity * family_size * complexity +
    (1 + family_size + complexity | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n400_nonwords
)
anova(anova_model_n400_nonwords_a)


anova_model_n400_nonwords <- mixed(
    value ~ Orthographic_Sensitivity * family_size * complexity +
    (1 + family_size + complexity | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n400_nonwords,
  method = "KR"
)
anova_model_n400_nonwords 

m1 <- anova_model_n400_nonwords$full_model    # Extract the lmer model
ranova(m1)    # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n400_nonwords, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n400_nonwords)
```

All partial $\eta^2$ values are extremely small (all < .01), confirming that the observed family_size × complexity interaction, although statistically significant, explains a very small proportion of variance.

The N400 responses to nonwords were largely unaffected by orthographic sensitivity, family size, or complexity considered separately. A small but reliable family_size × complexity interaction suggests some modulation of the N400 pattern, possibly because “complex” nonwords with large families evoke slightly different expectations or morphological parsing attempts than “simple” ones — but this effect is weak and not influenced by reader sensitivity. Overall, the nonword data show minimal systematic structure, consistent with the idea that participants did not engage lexical-semantic processing for these stimuli in a meaningful way.


## Main Effects

No main effects

## Interactions

There was a two-way interaction between family size and complexity; the effect of family size (large vs. small) reversed or changed in magnitude depending on whether the nonwords were simple or complex

  - `Family Size × Complexity`: significant ($t = 5.87$, $p =.016$).

###  Simple Contrasts

Simple effects ask: within one level of one factor, what is the effect of the other factor?

(a) Effect of family_size within each level of complexity. Tests whether “*large vs. small family*” differs for simple and complex items separately. This helps you see where the interaction is coming from — e.g., if the family size effect flips between complexity levels.

(b) Effect of complexity within each level of family_size.  Tests whether “*complex vs. simple*” differs within large and small families.

```{r}
# Estimated marginal means for the family_size × complexity interaction
emm1 <- emmeans(anova_model_n400_nonwords, ~ family_size * complexity)

# Look at the table of estimated means
emm1

# Simple effects of family_size at each level of base_freq
contrast(emm1, method = "pairwise", by = "complexity", adjust = "holm")

# Simple effects of base_freq at each level of family_size
contrast(emm1, method = "pairwise", by = "family_size", adjust = "holm")

```

### Interaction Contrasts

If simple effects aren’t significant, try looking at interaction contrasts, which test differences in the differences. You’re now asking: Does the effect of Sensitivity change more in some complexity/family combinations than others?

The interaction contrast tests:

Is the difference in the effect of A across levels of B different at Complex vs. Simple levels?

Mathematically:

$$
 [ [(A_1 - A_2)\text{ in }B_1] - [(A_1 - A_2)\text{ in }B_2]
$$

```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare complexity effect in large vs small family)
contrast(emm1, interaction = "pairwise", by = NULL, adjust = "holm")

eff_size(emm1, sigma = ?, edf = 67)


# Optionally: get confidence intervals
(confint(contrast(emm1, interaction = c("pairwise", "pairwise"))))

all_contr <- contrast(emm1, interaction = c("pairwise", "pairwise"), combine = TRUE, adjust = "bonferroni")

# 1. Summarize contrasts
sumc <- summary(all_contr)

# Construct a contrast name string
contrast_name <- paste( sumc$Orthographic_Sensitivity_pairwise, sumc$family_size_pairwise,
  sumc$complexity_pairwise,sep = " — ")  # or whatever separator you prefer

# 2. Extract values
est <- sumc$estimate
se <- sumc$SE
df_contr <- sumc$df

# 3. Use the sigma you found
lm_mod <- anova_model_n400_nonwords$full_model
sigma_val <- sigma(lm_mod)

# 4. Compute d and its SE
d <- est / sigma_val
se_d <- se / sigma_val

# 5. Confidence intervals for d (t critical)
alpha <- 0.05
tcrit <- qt(1 - alpha/2, df_contr)
ci_low <- d - tcrit * se_d
ci_high <- d + tcrit * se_d

# 6. Make table
d_table <- data.frame( contrast = contrast_name, 
                       d = d, se_d = se_d, df = df_contr, ci_low = ci_low, ci_high = ci_high)
d_table
```

Compute the effect of Complexity (Complex - Simple) within each Orthographic Sensitivity × Family Size combination.
High Sensitivity- Large Family: `Complex - Simple = -0.495 - (-0.609) = +0.114`

High Sensitivity- Small Family: `Complex - Simple = -0.785 - (-0.471) = -0.314`

Low Sensitivity - Large Family: `Complex - Simple = -0.607 - (-0.713) = +0.106`

Low Sensitivity - Small Family: `Complex - Simple = -0.632 - (-0.829) = +0.197`

Compute the difference of differences:  compare how the effect of complexity differs across sensitivity groups:`(High Sensitivity complexity effect) - (Low Sensitivity complexity effect)`

For Large Family: 
```
High:  +0.114
Low:   +0.106
Difference: 0.114 - 0.106 = +0.008
```

For Small Family: 
```
High: -0.314  
Low:  +0.197  
Difference: -0.314 - (+0.197) = -0.511
```

This is a reversal of the complexity effect between High and Low sensitivity participants for Small Family nonwords — and that’s the core of your significant 3-way interaction.

Now take the difference of these differences (Small - Large):  `-0.511 - 0.008 = -0.519`.  That’s the interaction contrast estimate: ` -0.52, p = .0325`

The three-way interaction reflects the fact that High and Low sensitivity participants show opposite complexity effects — but only in the Small Family condition. In Large families, their complexity effects are essentially the same.

In `Small families`, `High sensitivity` participants respond more negatively to `complex items`, white `Low sensitivity` participants respond more negatively to `simple items`.

This crossover in the complexity effect is what drives the significant interaction — even though none of the simple effects are individually significant.


```{r}
# 6. Plot the interaction
library(ggplot2)

emm1_df <- as.data.frame(emm1)
ggplot(emm1_df,
       aes(x = complexity, y = emmean,
           color = family_size, group = family_size)) +
  facet_wrap(~ Orthographic_Sensitivity) +
  geom_line(position = position_dodge(0.2)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  labs(x = "Complexity", y = "Estimated n400 amplitude",
       color = "Family Size",
       title = "Interaction: Orthographic Sensitivity × Family Size × Complexity") +
  scale_color_custom() +
  scale_fill_custom()
```



Interpretation
 -	This is an interaction contrast (a “contrast of contrasts”) across your three factors (Orthographic Sensitivity × Family Size × Complexity).
	
 -	Specifically, it is testing whether the difference (`Complex – Simple`)  for (`Large Family` vs. `Small Family`) differs between the two levels of `Orthographic Sensitivity`. 
	
The contrast is asking: “Is the effect of `complexity`, in the contrast `Large` vs. `Small` family, different in `High Orthographic` vs.` Low Orthographic` participants?”

 -	The `estimate = 0.5`2 is the difference in differences (i.e. the slope difference) on your response metric (n400 amplitude).
	
	-	*SE* = 0.243, *df* = 1523, *t* = 2.140 → yields *p* = 0.0325, so it is statistically significant (given Bonferroni correction, etc.).
	
 -	Because you used adjust = "bonferroni" and combine = TRUE, this contrast is part of a “family” of interaction contrasts that have been adjusted for multiple comparisons.

So in more conversational terms: you have evidence that High Orthographic readers show a different `complexity × family size` effect than Low Orthographic readers — in particular, in how the `complexity` effect (`Complex` vs. `Simple`) differs when comparing `Large` vs. `Small` family.
 
 
Suggests that sensitivity does influence the n400, but only in how it modulates the joint effect of family size and complexity. In other words: the way family size and complexity interact depends on whether participants are orthographically sensitive or not.

  - Marginal $R^2 = 0.2%$ --> the fixed predictors (including sensitivity) account for very little variance overall.
	
  - Conditional $R^2 = 76%$ --> most variance is indeed explained by subjects and electrodes (as anticipated).

Most of the variability in n400 amplitude reflects differences across participants and electrode sites, as expected for ERP data. Orthographic sensitivity did not produce an overall shift in n400 responses, but it did moderate the combined influence of family size and morphological complexity. This interaction was statistically significant but accounted for only a very small portion of the variance. Thus, orthographic sensitivity may play a role in how multiple lexical factors are integrated during early morphological processing, though the effect is subtle.


