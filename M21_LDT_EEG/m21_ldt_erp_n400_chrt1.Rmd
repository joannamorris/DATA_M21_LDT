---
title: "M21 LDT ERP  N250"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: inline
---

\scriptsize

# Set parameters {-}
Set chunk parameters
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "||")
```

Load libraries
```{r}
library(tidyverse)
library(ggeffects)
library(lme4)
library(afex)
library(gridExtra)
library(emmeans)
library(effectsize)
library(performance)
library(cowplot)  # for use with `plot_grid(x,x,ncol = x)` function
library(e1071) # for use with `skewness()` function
```


Set ggplot parameters
```{r}
theme_set(theme_classic() +  
            theme(legend.position = "bottom", 
                  axis.text=element_text(size=10),
                  axis.title=element_text(size=9)))

# Define a custom color palette
my_palette <- c("#A6CEE3",  "#FB9A99")
my_palette_2 <- c( "#1F78B4","#E31A1C" )
my_palette_3 <- c("#A6CEE3","#1F78B4","#FB9A99","#E31A1C")


# Create a function to apply this palette
scale_color_custom <- function() {
  scale_color_manual(values = my_palette_2)
}

scale_fill_custom <- function() {
  scale_fill_manual(values = my_palette_2)
}
```


Define standard error of the mean function
```{r}
sem <- function(x) sd(x)/sqrt(length(x))
```


# N400
## Load and format data files

```{r, echo = FALSE}
erpdat_1 <- read_csv("m21_ldt_mea_300500_050050_1.csv")
dmg_lng_vsl <- read_csv("demo_lang_vsl_pca.csv")

```

Now we extract `SubjID` from the `ERPset` column
```{r, echo = FALSE}

# Remove '_LDT_diff_waves' from each string in the ERPset column
# This code first renames the column and then applies the `str_replace` function 
# to the newly renamed column.
erpdat_1 <- erpdat_1 %>%
  rename(SubjID = ERPset) %>%
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", "")) |>
  select(-mlabel)
```

We then join the ERP data, and language into a single data frame
```{r, echo = FALSE}

n400_1 <- erpdat_1 |>
  left_join(dmg_lng_vsl, by = "SubjID") |>
  select(SubjID, everything()) |>
  rename(orthographic_sensitivity = lang_type_ortho)
```

Divide into word, non-word and difference wave dataframes
```{r}
n400_1_words <- n400_1 |> filter(bini %in% c(1:2))
n400_1_words_b <- n400_1 |> filter(bini %in% c(9:12))
n400_1_nonwords <- n400_1 |> filter(bini %in% c(3:6))
```

Then we do some more formatting and cleanup of the dataframes.We  create separate columns, one for each independent variable (anteriority, laterality, morphological family size). To do this we have to use `seperate` function from the `stringr` package. Run `vignette("programming", package = "dplyr")` to see more about `tidy-selection` and `tidy-evaluation`.

```{r, echo = FALSE}
n400_1_words <- n400_1_words %>%
  separate(binlabel, into = c("trial_type", "family_size"), sep = "_", remove = TRUE) |>
  select(-trial_type)

n400_1_words_b <- n400_1_words_b %>%
  separate(binlabel, into = c("trial_type", "family_size","tmp1", "base_freq", "tmp2"), sep = "_", remove = TRUE) |>
  select(-c(trial_type, tmp1, tmp2))

# Assuming your data frame is named 'df' and the column is named 'your_column'
n400_1_words_b$orthographic_sensitivity[n400_1_words_b$orthographic_sensitivity == "Low"] <- "Low Sensitivity"
n400_1_words_b$orthographic_sensitivity[n400_1_words_b$orthographic_sensitivity == "High"] <- "High Sensitivity"
n400_1_words_b$base_freq[n400_1_words_b$base_freq == "Low"] <- "Low Base Frequency"
n400_1_words_b$base_freq[n400_1_words_b$base_freq == "High"] <- "High Base Frequency"
n400_1_words_b$family_size[n400_1_words_b$family_size == "large"] <- "Large Family"
n400_1_words_b$family_size[n400_1_words_b$family_size == "small"] <- "Small Family"

n400_1_nonwords <- n400_1_nonwords %>%
  separate(binlabel, into = c("trial_type", "family_size", "complexity"), sep = "_", remove = TRUE) |>
  select(-trial_type)

# Assuming your data frame is named 'df' and the column is named 'your_column'
n400_1_nonwords$orthographic_sensitivity[n400_1_nonwords$orthographic_sensitivity == "Low"] <- "Low Sensitivity"
n400_1_nonwords$orthographic_sensitivity[n400_1_nonwords$orthographic_sensitivity == "High"] <- "High Sensitivity"
n400_1_nonwords$complexity[n400_1_nonwords$complexity == "complex"] <- "Complex"
n400_1_nonwords$complexity[n400_1_nonwords$complexity == "simple"] <- "Simple"
n400_1_nonwords$family_size[n400_1_nonwords$family_size == "large"] <- "Large Family"
n400_1_nonwords$family_size[n400_1_nonwords$family_size == "small"] <- "Small Family"
```

Now we need to  extract just the bins and channels that we intend to analyse. For this analysis we will use 9 channels:  F3, Fz, F4, C3, Cz, C4, P3, Pz, P4 . We will use the`mutate` function from the `dplyr` package along with the `case_when` function. The `case_when` function  is a sequence of two-sided formulas. The left hand side determines which values match this case. The right hand side provides the replacement value.

```{r, echo=FALSE}

channels_1 <-  c(3, 2, 25, 7, 20, 21, 12, 11, 16)
channels_2 <-  c(3, 2, 29, 8, 23, 24, 14, 13, 19)

n400_1_words <- n400_1_words %>%
  filter(chindex %in% channels_1) %>% 
  mutate(anteriority = case_when(grepl("F", chlabel) ~ "Frontal",grepl("C", chlabel) ~ "Central",
                                 grepl("P", chlabel) ~ "Parietal"),
         laterality = case_when(grepl("3", chlabel) ~ "Left",grepl("z", chlabel) ~ "Midline",
                                grepl("Z", chlabel) ~ "Midline",grepl("4", chlabel) ~ "Right"))
n400_1_words$anteriority <- factor(n400_1_words$anteriority, levels = c("Frontal", "Central","Parietal"))
n400_1_words$laterality <- factor(n400_1_words$laterality, levels = c("Left","Midline","Right"))

n400_1_words_b <- n400_1_words_b %>%
  filter(chindex %in% channels_1) %>% 
  mutate(anteriority = case_when(grepl("F", chlabel) ~ "Frontal",grepl("C", chlabel) ~ "Central",
                                 grepl("P", chlabel) ~ "Parietal"),
         laterality = case_when(grepl("3", chlabel) ~ "Left",grepl("z", chlabel) ~ "Midline",
                                grepl("Z", chlabel) ~ "Midline",grepl("4", chlabel) ~ "Right"))
n400_1_words_b$anteriority <- factor(n400_1_words_b$anteriority, levels = c("Frontal", "Central","Parietal"))
n400_1_words_b$laterality <- factor(n400_1_words_b$laterality, levels = c("Left","Midline","Right"))

n400_1_nonwords <- n400_1_nonwords %>%
  filter(chindex %in% channels_1) %>% 
  mutate(anteriority = case_when(grepl("F", chlabel) ~ "Frontal",grepl("C", chlabel) ~ "Central",
                                 grepl("P", chlabel) ~ "Parietal"),
         laterality = case_when(grepl("3", chlabel) ~ "Left",grepl("z", chlabel) ~ "Midline",
                                grepl("Z", chlabel) ~ "Midline", grepl("4", chlabel) ~ "Right"))
n400_1_nonwords$anteriority <- factor(n400_1_nonwords$anteriority, levels = c("Frontal", "Central", "Parietal"))
n400_1_nonwords$laterality <- factor(n400_1_nonwords$laterality, levels = c("Left",  "Midline",  "Right"))
```

## Real Word Data

### Compute the ANOVA
```{r}
anova_model_1a <- mixed(
  value ~ orthographic_sensitivity * family_size * base_freq  + 
    laterality * anteriority  +  # Nuisance variables
    (1 | SubjID), 
  data = n400_1_words_b, 
  method = "KR")  # Kenward-Roger approximation for accurate F-tests
# Print ANOVA results
anova_model_1a


# Partial Eta Squared
# Extract effect sizes from your ANOVA model
eta_squared(anova_model_1a , partial = TRUE)

# Compute Marginal (fixed effects) and Conditional (fixed + random effects) RÂ²
r2(anova_model_1a)
```
### Significant Effects

|                            Effect|      df|         F| p.value||eta-sqrd|  
|----------------------------------|--------|----------|--------|---------|
| orthographic_sensitivity         |1, 59   |  4.83 *  |   .032 |     0.08|
| family_size                      |1, 2121 | 11.20 ***|  <.001 | 5.25e-03|
| base_freq                        |1, 2121 |  3.09 +  |   .079 | 1.46e-03|
| family_size:base_freq            |1, 2121 |  4.75 ** |   .009 |     0.01|


```{r}
## `orthographic_sensitivity` main effect
# Get estimated marginal means for each level of complexity
pairs <- emmeans(anova_model_1a, pairwise ~ orthographic_sensitivity, adjust = "bonferroni", pbkrtest.limit = 6480)
pairs_df <- as.data.frame(pairs$contrasts)
cohensd <- as.data.frame(cohens_d(value ~ orthographic_sensitivity, data = n400_1_words_b))
(orthographic_sensitivity_contrasts_df <- bind_cols(pairs_df,cohensd))
(orthographic_sensitivity_means <- as.data.frame(pairs$emmeans))
```

```{r}
## `family_size` main effect
# Get estimated marginal means for each level of family size
pairs <- emmeans(anova_model_1a, pairwise ~ family_size, adjust = "bonferroni", pbkrtest.limit = 6480)
pairs_df <- as.data.frame(pairs$contrasts)
cohensd <- as.data.frame(cohens_d(value ~ family_size, data = n400_1_words_b))
(family_size_contrasts_df <- bind_cols(pairs_df,cohensd))
(family_size_means <- as.data.frame(pairs$emmeans))
```

```{r}
## `base_freq` main effect
# Get estimated marginal means for each level of base frequency
pairs <- emmeans(anova_model_1a, pairwise ~ base_freq, adjust = "bonferroni", pbkrtest.limit = 6480)
pairs_df <- as.data.frame(pairs$contrasts)
cohensd <- as.data.frame(cohens_d(value ~ base_freq, data = n400_1_words_b))
(base_frequency_contrasts_df <- bind_cols(pairs_df,cohensd))
(base_frequency_means <- as.data.frame(pairs$emmeans))
```

```{r}
# Test whether the interaction between family_size and base_freq improves model fit
reduced_model_int <- update(anova_model_1a,
  . ~ . - family_size:base_freq - orthographic_sensitivity:family_size:base_freq)

anova(anova_model_1a, reduced_model_int)

# Custom contrasts for family_size Ã base_freq Interaction
pairs <- emmeans(anova_model_1a, pairwise ~ family_size * base_freq, adjust = "bonferroni", pbkrtest.limit = 6480)
(pairs_df <- as.data.frame(pairs$contrasts))

selected_contrasts_basefrq <- pairs$contrasts[pairs_df$contrast %in% c("Large Family High Base Frequency - Large Family Low Base Frequency",
                                                                       "Small Family High Base Frequency - Small Family Low Base Frequency"),]
selected_contrasts_famsize <- pairs$contrasts[pairs_df$contrast %in% c("Large Family High Base Frequency - Small Family High Base Frequency",
                                                                      "Large Family Low Base Frequency - Small Family Low Base Frequency"), ]

selected_contrasts_basefrq_df <- as.data.frame(selected_contrasts_basefrq)  # Convert the emmGrid object to a dataframe
selected_contrasts_famsize_df <- as.data.frame(selected_contrasts_famsize)  

cohensd_hi_basefrq <- as.data.frame(cohens_d(value ~ family_size,
                                          data = subset(n400_1_words_b, base_freq == "High Base Frequency")))
cohensd_lo_basefrq <- as.data.frame(cohens_d(value ~ family_size, 
                                             data = subset(n400_1_words_b, base_freq == "Low Base Frequency")))
cohensd_lrg_famsize <- as.data.frame(cohens_d(value ~ base_freq, 
                                            data = subset(n400_1_words_b, family_size == "Large Family")))
cohensd_sml_famsize <- as.data.frame(cohens_d(value ~ base_freq,
                                           data = subset(n400_1_words_b, family_size == "Small Family")))

cohensd_basefrq <- bind_rows(hi_basefrq = cohensd_hi_basefrq,
                             lo_basefrq = cohensd_lo_basefrq,
                             .id = "base_freq")

cohensd_famsize <- bind_rows(hifamsize = cohensd_lrg_famsize,
                            lo_sensi = cohensd_sml_famsize,
                             .id = "family_size")

(basefreq_contrasts_df <- bind_cols(selected_contrasts_basefrq_df,cohensd_basefrq))
(family_size_contrasts_df <- bind_cols(selected_contrasts_famsize_df,cohensd_famsize))
(family_size.basefreq_means <- as.data.frame(pairs$emmeans))
```

```{r, fig.width=7, fig.height=3.5}

p1 <- ggplot(orthographic_sensitivity_means, aes(x = orthographic_sensitivity, y = emmean, 
                                                 fill = orthographic_sensitivity, colour = orthographic_sensitivity)) +
  geom_bar(stat = "identity", position = position_dodge(), alpha = .4) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.05, position = position_dodge(0.9)) +
  ylab("Mean ERP Amplitude (microvolts)") +
  geom_text(aes(label = round(emmean, digits = 2)), colour = "black", size = 2.5, vjust = -8,
            position = position_dodge(.9)) +
  coord_cartesian(ylim = c(-.5, 2.2)) +
  scale_color_custom() +
  scale_fill_custom() +
  labs(title = "Orthographic Sensitivity") +
  theme( plot.title = element_text(size = 10),
         legend.position = "none",
         axis.title.x = element_blank(),
         axis.text.x = element_text(size = 7))


p2 <- ggplot(base_frequency_means, aes(x = base_freq, y = emmean, fill = base_freq, colour = base_freq)) +
  geom_bar(stat = "identity", position = position_dodge(), alpha = .4) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.05, position = position_dodge(0.9)) +
  ylab("Mean ERP Amplitude (microvolts)") +
  geom_text(aes(label = round(emmean, digits = 2)), colour = "black", size = 2.5, vjust = 8,
            position = position_dodge(.9)) +
  coord_cartesian(ylim = c(-.5, 2.2)) +
  scale_color_custom() +
  scale_fill_custom() +
  labs(title = "Base Frequency") +
  theme( plot.title = element_text(size = 10),
         legend.position = "none",
         axis.title.x = element_blank(),
         axis.text.x = element_text(size = 6))


p3 <- ggplot(family_size_means, aes(x = family_size, y = emmean, fill = family_size, colour = family_size)) +
  geom_bar(stat = "identity", position = position_dodge(), alpha = .4) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.05, position = position_dodge(0.9)) +
  ylab("Mean ERP Amplitude (microvolts)") +
  geom_text(aes(label = round(emmean, digits = 2)), colour = "black", size = 2.5, vjust = 7,
            position = position_dodge(.9)) +
  coord_cartesian(ylim = c(-.5, 2.2)) +
  scale_color_custom() +
  scale_fill_custom() +
  labs(title = "Morphological Family Size" ) +
  theme( plot.title = element_text(size = 10),
         legend.position = "none",
         axis.title.x = element_blank(),
         axis.text.x = element_text(size = 7))

plot_grid(p1, p2, p3, ncol = 3)

```

Plot for `family_size` Ã `base_freq` Interaction


```{r,  echo=FALSE, fig.width=4.5, fig.height=3}

p2 <- family_size.basefreq_means|>
  ggplot(aes(x = family_size,
             y = emmean,
             fill = base_freq,
             colour = base_freq)) +
  geom_col(alpha = .4, position = position_dodge(.9)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE), 
                width = .15,
                position = position_dodge(.9)) +
  labs(y="Mean ERP amplitude (in microvolts)",
       x = "Family Size by Base Frequency") +
  geom_text(aes(label = round(emmean, digits = 2)),
                colour = "black",size = 2.5, vjust = -1.7, position = position_dodge(.65)) +
  scale_color_custom() +
  scale_fill_custom() +
  theme( plot.title = element_text(size = 10),
         legend.title = element_blank(),
         axis.text.x = element_text(size = 6))
p2
#plot_grid(p1, p1, ncol = 2)
```

## Nonword Data

### Compute the ANOVA

```{r}

# Fit the ANOVA/mixed model
anova_model_1b <- mixed(
  value ~ orthographic_sensitivity * family_size * complexity + 
    laterality * anteriority  +  # Nuisance variables
    (1 | SubjID), 
  data = n400_1_nonwords, 
  method = "KR"  # Kenward-Roger approximation for accurate F-tests
)

# Print ANOVA results
anova_model_1b 

# Partial Eta Squared
# Extract effect sizes from your ANOVA model
eta_squared(anova_model_1b , partial = TRUE)

# Compute Marginal (fixed effects) and Conditional (fixed + random effects) RÂ²
r2(anova_model_1b)
```

### Significant Effects

No significant effects of any of the experimental variables were found.