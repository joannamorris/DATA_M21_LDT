---
title: "M21 LDT ERP HC ORTHOGRAPIC SENSITIVITY N400 Family Size"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 4
editor_options: 
  chunk_output_type: inline
---

\scriptsize

# Set parameters {-}
Set chunk parameters
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "||")
options(width = 140)
```



Load libraries
```{r, echo=FALSE}
library(tidyverse)
library(ggeffects)
library(lme4)
library(afex)
library(gridExtra)
library(emmeans)
library(effectsize)
library(performance)
library(cowplot)  # for use with `plot_grid(x,x,ncol = x)` function
library(e1071) # for use with `skewness()` function
library(lmerTest)
```


Set ggplot parameters
```{r, echo=FALSE}
theme_set(theme_classic() +  
            theme(legend.position = "bottom", 
                  axis.text=element_text(size=8.5),
                  axis.title=element_text(size=9)))

# Define a custom color palette
my_palette <- c("#A6CEE3",  "#FB9A99")
my_palette_2 <- c( "#1F78B4","#E31A1C" )
my_palette_3 <- c("#A6CEE3","#1F78B4","#FB9A99","#E31A1C")


# Create a function to apply this palette
scale_color_custom <- function() {
  scale_color_manual(values = my_palette_2)
}

scale_fill_custom <- function() {
  scale_fill_manual(values = my_palette_2)
}
```


Define standard error of the mean function
```{r, echo=FALSE}
sem <- function(x) sd(x)/sqrt(length(x))
```


# Load data files 

```{r}
dir_path <- "CSV files"

erp_4A <- read_csv(file.path(dir_path, "fs_m21_ldt_mea_300500_050050_1_AB.csv"))
erp_4B <- read_csv(file.path(dir_path, "fs_m21_ldt_mea_300500_050050_1_BA.csv"))
dmg_lng_vsl <- read_csv(file.path(dir_path, "demo_lang_vsl_pca_hc.csv"))
```

```{r}
library(dplyr)

erp_4i <- bind_rows(
  erp_4A |> mutate(List = "AB"),
  erp_4B |> mutate(List = "BA")
)
```

Now we extract `SubjID` from the `ERPset` column
```{r, , echo=FALSE}

# Remove '_LDT_diff_waves' from each string in the ERPset column
# This code first renames the column and then applies the `str_replace` function 
# to the newly renamed column.
# erp_2 <- erp_2 |>
#   rename(SubjID = ERPset) |>
#   mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
#   mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
#   mutate(binlabel = str_replace(binlabel, "_family", "")) |>
#   select(-mlabel)

erp_4ii <- erp_4i |>
  rename(SubjID = ERPset) |>
  mutate(SubjID = str_replace(SubjID, "_LDT_diff_waves", "")) |>
  mutate(binlabel = str_replace(binlabel, "Critical_", "")) |>
  mutate(binlabel = str_replace(binlabel, "_family", "")) |>
  select(-mlabel)
```

We then join the ERP data and language into a single data frame
```{r, echo=FALSE}

# n250 <- erp_2 |>
#   left_join(dmg_lng_vsl, by = "SubjID") |>
#   select(SubjID, everything()) 
erp_4iii <- erp_4ii |>
  left_join(dmg_lng_vsl, by = "SubjID") |>
  select(SubjID, everything()) 
```

# Format data files 

Divide into word, non-word and difference wave dataframes
```{r, echo=FALSE}

n400_words <- erp_4iii |> filter(bini %in% c(1:2))    # does not include BF data
n400_words_b <- erp_4iii |> filter(bini %in% c(9:12)) # includes BF data
n400_nonwords <- erp_4iii |> filter(bini %in% c(3:6))
```

Then we do some more formatting and cleanup of the dataframes.We  create separate columns, one for each independent variable (anteriority, laterality, morphological family size). To do this we have to use `seperate` function from the `stringr` package. Run `vignette("programming", package = "dplyr")` to see more about `tidy-selection` and `tidy-evaluation`.

```{r, echo=FALSE}
# Words

n400_words <- n400_words |>
  separate(binlabel, into = c("trial_type","family_size"), sep = "_", remove = TRUE) |>
  select(-trial_type)
n400_words_b <- n400_words_b |>
  separate(binlabel, into = c("trial_type", "family_size","tmp1", "base_freq", "tmp2"), sep = "_", remove = TRUE) |>
  select(-c(trial_type, tmp1, tmp2))

n400_words_b$family_size[n400_words_b$family_size == "large"] <- "Large"
n400_words_b$family_size[n400_words_b$family_size == "small"] <- "Small"

# Nonwords
n400_nonwords <- n400_nonwords |>
  separate(binlabel, into = c("trial_type", "family_size", "complexity"), sep = "_", remove = TRUE) |>
  select(-trial_type)

n400_nonwords$complexity[n400_nonwords$complexity == "complex"] <- "Complex"
n400_nonwords$complexity[n400_nonwords$complexity == "simple"] <- "Simple"
n400_nonwords$family_size[n400_nonwords$family_size == "large"] <- "Large"
n400_nonwords$family_size[n400_nonwords$family_size == "small"] <- "Small"

# Format as factors
n400_nonwords <- n400_nonwords |>
  mutate(complexity = fct_relevel(complexity, "Simple", "Complex"))

n400_nonwords <- n400_nonwords |>
  mutate(family_size = fct_relevel(family_size, "Small", "Large"))

# str(n400_nonwords)
```

Now we need to  extract just the bins and channels that we intend to analyse. For this analysis we will use 9 channels:  F3, Fz, F4, C3, Cz, C4, P3, Pz, P4 . We will use the`mutate` function from the `dplyr` package along with the `case_when` function. The `case_when` function  is a sequence of two-sided formulas. The left hand side determines which values match this case. The right hand side provides the replacement value.

```{r, echo=FALSE}
channels_1 <-  c(3, 2, 25, 7, 20, 21, 12, 11, 16)

# Words
n400_words <- n400_words |>
 filter(chindex %in% channels_1) |> 
 select(-`Included VSL2`)

n400_words_b <- n400_words_b |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)

# Nonwords
n400_nonwords <- n400_nonwords  |>
  filter(chindex %in% channels_1) |> 
  select(-`Included VSL2`)
```


# N400 Word Data

## Nested ANOVA Model
<br>
```{r}
#Fit ANOVA model
anova_model_n400_words_b <- mixed(
    value ~ Orthographic_Sensitivity * family_size * base_freq +
    (1 + family_size + base_freq | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n400_words_b,
  method = "KR"
)
anova_model_n400_words_b 

m1 <- anova_model_n400_words_b$full_model    # Extract the lmer model
ranova(m1) # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n400_words_b, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n400_words_b)
```

## Main Effects

No significant main effects

## Interactions

A two-way interaction betweeen Family Size and Base Frequency


|                            Effect|         df|         F| p.value||eta-sqrd|  
|----------------------------------|-----------|----------|--------|---------|
|             family_size:base_freq| 1, 4738.00| 87.28 ***|   <.001|    0.02 |


### Simple Contrasts
<br>
```{r}
# Estimated marginal means for the family_size × base frequency interaction
(emm1 <- emmeans(anova_model_n400_words_b, ~ family_size * base_freq))

# Get all pairswise contrasts
emm1_contrasts <- contrast(emm1,  method = "pairwise", by = NULL, adjust = "none")

# Keep only the contrasts you want
# Simple effects of family_size at each level of base_freq
# Simple effects of base_freq at each level of family_size
keep <- c("Large High - Small High",
          "Large Low - Small Low",
          "Large High - Large Low",
          "Small High - Small Low")
(emm1_contrasts_filtered <- subset(emm1_contrasts, contrast %in% keep))


# Get Confidence Intervals
(emm1_contrasts_filtered_ci <- confint(emm1_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs1 <- eff_size(emm1, sigma = sigma(m1), edf = df.residual(m1))

# Remove the two redundant rows (rows 3 and 4)
(effs1_filtered <- subset(effs1, contrast %in% keep))
```
For low-frequency bases, small-family words elicit more negative amplitudes than large-family words. When base frequency is high, family size has no effect.
 
 - Low base frequency: $\text{Large - Small } = \; 0.9381 \; SE = 0.250; z = 3.747; p = 0.0002$. This difference is statistically significant.

 - High base frequency: $\text{Large - Small }= \; 0.0395 \; SE = 0.272; z = 0.158; p = 0.8748$.  This difference is NS.
 
 <br>

For small-family words, low base frequency bases elicit more negative responses than high base frequency bases; when family_size is large, the difference is marginal.


 - Small family_size: $\text{High - Low }= \; 0.5495; SE = 0.188; z = 2.926; p = 0.0034$. Significant difference
 
 - Large family_size: $\text{High - Low }= \; -0.3491; SE = 0.188; z = -1.859; p = 0.0631$. This is a trend but not significant.



### Interaction Contrasts
<br>
```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare base frequency effect in large vs small family)
contrast(emm1, interaction = "pairwise", by = NULL, adjust = "holm")

# Get confidence intervals, for the frequency effect for each family size and then for interaction effect
confint(contrast(emmeans(m1, ~ family_size | base_freq), "pairwise"))
confint(contrast(emm1, interaction = c("pairwise", "pairwise")))
```

The final contrast tests whether the difference between Large vs Small family_size is itself different between High vs Low base_freq: 

$\text{Estimate }= \; -0.899$; $SE = 0.0962$; $z = -9.342$; $p < .0001$

That is, the slope or effect of `family_size` depends strongly on the level of `base_freq` (consistent with your ANOVA). Put differently: the family size difference (Large - Small) is much more positive in the low base frequency condition than it is in the high base frequency condition. That difference of differences is highly significant.


## Plots

```{r}
emm_df <- as.data.frame(emm1)
p1<- ggplot(emm_df,
       aes(x = family_size, y = emmean,
           color = base_freq, group = base_freq)) +
  geom_line(position = position_dodge(0.2)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  labs(x = "Family Size", y = "Estimated N400 amplitude",
       color = "Base Frequency",
       title = "Base Frequency × Family Size") +
  scale_color_custom() +
  scale_fill_custom() 

p2 <- ggplot(emm_df,
       aes(x = base_freq, y = emmean,
           color = family_size, group = family_size)) +
  geom_line(position = position_dodge(0.2)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  labs(x = "Base Frequency", y = "Estimated N400 amplitude",
       color = "Family Size",
       title = "Family Size × Base Frequency") +
  scale_color_custom() +
  scale_fill_custom() 

plot_grid(p1, p2, ncol = 2)
```

\newpage

# N400 Nonword Data

## Compute the ANOVA
<br>
```{r}
anova_model_n400_nonwords <- mixed(
    value ~ Orthographic_Sensitivity * family_size * complexity +
    (1 + family_size + complexity | SubjID) +     # by-subject intercept + slopes
    (1 | SubjID:chlabel),                        # electrode nested within subject
  data   = n400_nonwords,
  method = "KR"
)
anova_model_n400_nonwords 

m2 <- anova_model_n400_nonwords$full_model    # Extract the lmer model
ranova(m2)    # Run random effects comparison


# Extract effect sizes from your ANOVA model
eta_squared(anova_model_n400_nonwords, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_n400_nonwords)
```


## Main Effects

No significant main effects

## Interactions


|                            Effec|      df|         F| p.value||eta-sqrd|  
|---------------------------------|--------|----------|--------|---------|
|           family_size:complexity| 2, 4736| 18.31 ***|   <.001| 7.67e-03|


###  Simple Contrasts

(a) Effect of family_size within each level of complexity. Tests whether “*large vs. small family*” differs for simple and complex items separately. This helps you see where the interaction is coming from — e.g., if the family size effect flips between complexity levels.

(b) Effect of complexity within each level of family_size.  Tests whether “*complex vs. simple*” differs within large and small families.


```{r}
# Estimated marginal means for the family_size × complexity interaction
(emm2 <- emmeans(anova_model_n400_nonwords, ~ family_size * complexity))

# Get all pairswise contrasts
emm2_contrasts <- contrast(emm2,  method = "pairwise", by = NULL, adjust = "none")

# Keep only the contrasts you want
# Simple effects of family_size at each level of complexity
# Simple effects of complexity at each level of family_size
keep2 <- c("Large Family Complex - Small Family Complex",
          "Large Family Simple - Small Family Simple",
          "Large Family Complex - Large Family Simple",
          "Small Family Complex - Small Family Simple")
(emm2_contrasts_filtered <- subset(emm2_contrasts, contrast %in% keep2))

# Get Confidence Intervals
(emm2_contrasts_filtered_ci <- confint(emm2_contrasts_filtered))

# Get effect sizes
# Get all pairwise effect sizes
effs2 <- eff_size(emm2, sigma = sigma(m2), edf = df.residual(m2))

# Remove the two redundant rows (rows 3 and 4)
(effs2_filtered <- subset(effs2, contrast %in% keep2))
```

### Interaction Contrasts

If simple effects aren’t significant, try looking at interaction contrasts, which test differences in the differences. You’re now asking: Does the effect of Sensitivity change more in some complexity/family combinations than others?

The interaction contrast tests:

Is the difference in the effect of A across levels of B different at Complex vs. Simple levels?

Mathematically:

$$
 [ [(A_1 - A_2)\text{ in }B_1] - [(A_1 - A_2)\text{ in }B_2]
$$

```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare complexity effect in large vs small family)
contrast(emm2, interaction = "pairwise", by = NULL, adjust = "holm")

# Get confidence intervals, for each complexity effect for each family size and then for interaction effect
confint(contrast(emmeans(m2, ~ family_size | complexity), "pairwise"))
confint(contrast(emm2, interaction = c("pairwise", "pairwise")))
```


## Plots
<br>
```{r}
# Plot the interaction
library(ggplot2)

emm2_df <- as.data.frame(emm2)

p3<- ggplot(emm2_df,
       aes(x = family_size, y = emmean,
           color = complexity, group = complexity)) +
  geom_line(position = position_dodge(0.2)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  labs(x = "Family Size", y = "Estimated N400 amplitude",
       color = "Complexity",
       title = "Complexity × Family Size") +
  scale_color_custom() +
  scale_fill_custom() 

p4 <- ggplot(emm2_df,
       aes(x = complexity, y = emmean,
           color = family_size, group = family_size)) +
  geom_line(position = position_dodge(0.2)) +
  geom_point(position = position_dodge(0.2)) +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  labs(x = "Complexity", y = "Estimated N400 amplitude",
       color = "Family Size",
       title = "Family Size × Complexity") +
  scale_color_custom() +
  scale_fill_custom() 

plot_grid(p3, p4, ncol = 2)
```





