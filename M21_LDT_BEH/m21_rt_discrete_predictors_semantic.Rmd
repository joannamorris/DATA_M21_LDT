---
title: "M21 RT Semantic Sensitivity"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  chunk_output_type: console
---


\scriptsize
# Setup  {-}
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "||",
                      fig.height = 4,
                      fig.width = 7)
options(width = 150)
```


Load libraries
```{r, echo=FALSE}
library(tidyverse)
library(ggeffects)
library(lme4)
library(afex)
library(gridExtra)
library(emmeans)
library(effectsize)
library(performance)
library(cowplot)  # for use with `plot_grid(x,x,ncol = x)` function
library(e1071) # for use with `skewness()` function
library(lmerTest)
```


1. Set `ggplot2` parameters
```{r theme, echo = FALSE}
theme_set(theme_classic() +  
            theme(legend.position = "bottom", 
                  axis.text=element_text(size=8.5),
                  axis.title=element_text(size=9)))

my_theme <- theme(strip.text = element_text(size = 7),
                  axis.text.x = element_text(size = 7),
                  legend.text = element_text(size = 6),
                  legend.title = element_blank())

# Define a custom color palette
my_palette <- c("#A6CEE3",  "#FB9A99")
my_palette_2 <- c( "#1F78B4","#E31A1C" )
my_palette_3 <- c("#A6CEE3","#1F78B4","#FB9A99","#E31A1C")


# Create a function to apply this palette
scale_color_custom <- function() {
  scale_color_manual(values = my_palette_2)
}

scale_fill_custom <- function() {
  scale_fill_manual(values = my_palette_2)
}

# Combine theme and scales
my_style <- list(my_theme,scale_color_custom(),scale_fill_custom())
```

# Load Files and Format Files  {-}

## Load Files
```{r}
#DIR <- "csv_files"
df_a <- read_csv("rt_data_hc_A.csv")
df_b <- read_csv( "rt_data_hc_B_fixed.csv")
frq_w <- read_csv("frq_cw.csv")
frq_nw <- read_csv("frq_nw.csv")
dmg <- read_csv("demo_lang_vsl_pca_hc.csv")
```


## Format Files
```{r}
# Concatenate datasets
rt <- bind_rows(AB = df_a, 
                BA = df_b,
               .id = "List")
rt_dmg<- right_join(dmg, rt, join_by(SubjID == subject_nr)) |>  # Join Participant Demographic and Lang Data
                    mutate(target = tolower(target)) |>
                    filter(correct == 1)

# Divide into Experimental and Filler Items
rt_fill <- rt_dmg |> filter(str_detect(targ_type, "^FILL"))
rt_exp <- rt_dmg |> filter(!str_detect(targ_type, "^FILL"))

# Add logFS to frequency datasets
frq_w <- frq_w |> mutate(Log10FS = log10(FS))
frq_nw <- frq_nw |> mutate(Log10FS = log10(FS))

# Define Factors and Conditions
rt_exp_format <- rt_exp |>
  separate(targ_type, into = c("trial_type", "family_size", "complexity"), sep = "_",
           remove = TRUE, extra = "drop", fill = "right") 

# Divide into Words and Nonwords
rt_words <- rt_exp_format |> filter(trial_type == "CW") |> select(- complexity)
rt_nwords <- rt_exp_format |> filter(trial_type == "NW")

# Join Stimulus Frequency Data
rt_words_frq <- left_join(rt_words, frq_w, join_by(target))|>
  select(-cond_trig.y, -word_trig.y) |>
  rename(cond_trig = cond_trig.x, word_trig = word_trig.x)  # remove duplicate columns
rt_nwords_frq <-  left_join(rt_nwords, frq_nw, join_by(target==word)) |>   
  select(-cond_trig.y, -word_trig.y) |>
  rename(cond_trig = cond_trig.x, word_trig = word_trig.x)  

# Rename BF_Split and FS_Split columns
rt_words_frq <- rt_words_frq |> rename(Base_Frequency = BF_Split, Family_Size = FS_Split) # Rename BF_Split and FS_Split columns
rt_nwords_frq <- rt_nwords_frq |> rename(Base_Frequency = BF_Split, Family_Size = FS_Split)

# Recode factor levels
# rt_words_frq <- rt_words_frq |>
#   mutate(Base_Frequency = case_match(Base_Frequency, "Low" ~ "Low BF", "High" ~ "High BF"),
#          Family_Size = case_match(Family_Size, "Small" ~ "Small Family", "Large" ~ "Large Family"))
# rt_nwords_frq <- rt_nwords_frq |> mutate(Base_Frequency = case_match(Base_Frequency, "Low" ~ "Low BF", "High" ~ "High BF"),
#                                          Family_Size = case_match(Family_Size, "Small" ~ "Small Family", "Large" ~ "Large Family"))
# 
# rt_words_frq$Semantic_Sensitivity[rt_words_frq$Semantic_Sensitivity == "Low"] <- "Low Sensitivity"
# rt_words_frq$Semantic_Sensitivity[rt_words_frq$Semantic_Sensitivity == "High"] <- "High Sensitivity"
```


# Word Data

Use `complete.cases()` to find which rows have missing data in the model-relevant variables:
```{r, results = "hide"}
# Specify only the variables used in the model
model_vars_w <- c("response_time",  "Log10BF","BF", "FS","Family_Size", "Base_Frequency", "Semantic_Sensitivity","SubjID")

# Identify incomplete rows cohort 1
incomplete_cases_words <- rt_words_frq[!complete.cases(rt_words_frq[, model_vars_w]), ]
rt_words_cmpl <- rt_words_frq[complete.cases(rt_words_frq[, model_vars_w]), ]
# View them
print(incomplete_cases_words)

# Standardize the predictors
rt_words_cmpl$Log10BF_std <- as.numeric(scale(rt_words_cmpl$Log10BF, center = TRUE, scale = TRUE))
rt_words_cmpl$FS_std <- as.numeric(scale(rt_words_cmpl$FS, center = TRUE, scale = TRUE))
rt_words_cmpl$Log10WF_std <- as.numeric(scale(rt_words_cmpl$Log10WF, center = TRUE, scale = TRUE))
rt_words_cmpl$Log10FS_std <- as.numeric(scale(rt_words_cmpl$Log10FS, center = TRUE, scale = TRUE))
rt_words_cmpl$Dim.2_std <- as.numeric(scale(rt_words_cmpl$Dim.2, center = TRUE, scale = TRUE))
```


```{r, echo=FALSE, results = "hide"}
rt_words_cmpl %>%
  summarise(
    n_subjects = n_distinct(SubjID),
    n_items = n_distinct(STRING))

# Count trials per subject
rt_words_cmpl %>%
  count(SubjID, name = "n_trials") %>%
  summarise(
    min_trials = min(n_trials),
    max_trials = max(n_trials),
    mean_trials = mean(n_trials))

(trial_count_by_subj <- rt_words_cmpl %>%
  count(SubjID, name = "n_trials") %>%
  arrange(desc(n_trials)))

rt_words_cmpl %>%
  count(Family_Size, Base_Frequency, Semantic_Sensitivity)
```


### Anova 
<br>
```{r}
anova_model_words <- mixed(
  response_time ~ Base_Frequency * Family_Size * Semantic_Sensitivity +
    (1 | SubjID) +
    (1 | STRING),
  data = rt_words_cmpl,
  method = "S")
anova_model_words

m1 <- anova_model_words$full_model    # Extract the lmer model
ranova(m1) # formally test whether adding each random effect improves fit

# Extract effect sizes from your ANOVA model
eta_squared(anova_model_words, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_words)
```

Concise Explanation

Models including random slopes for Base Frequency and Family Size by subject failed to converge or produced singular fits, indicating that the data did not support estimation of these additional variance components. Consequently, we report results from a simpler model with random intercepts for subjects and items (STRING), which converged cleanly and provided stable estimates.

Fuller explanation 

We initially attempted to fit a maximal random-effects structure following Barr et al. (2013), including random slopes for Base Frequency and Family Size by subject. However, these models yielded singular fits (zero variance estimates and perfect correlations among random effects). Because such structures can produce unreliable standard errors and inflated Type I error rates, we adopted the maximal non-singular model, containing random intercepts for both subjects and items (STRING). All reported statistics are based on this model.

 Brief 

(A more complex model including by-subject random slopes failed to converge; results from the non-singular intercept-only model are reported.)

### Main Findings

|                        Effect|       df |      F |p.value|
|------------------------------|----------|--------|-------|
|                Base_Frequency| 1, 92.29 |10.15 **|   .002|
|                   Family_Size| 1, 92.30 | 9.28 **|   .003|


### Plots


```{r, echo = FALSE, fig.height=3}
#### Main Effect of Base Frequency
# Compute means for each level of Base_Frequency
(emm_wd_bf_df <- as.data.frame(emmeans(anova_model_words, ~  Base_Frequency)))

p1<- emm_wd_bf_df |> ggplot(aes(y = emmean, x = Base_Frequency, colour = Base_Frequency, fill = Base_Frequency)) +
  geom_col( alpha = .4)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(550, 650)) +
  labs(title = "Base Frequency Effect (Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() 

#### Main Effect of Family Size
# Compute means for each level of Base_Frequency
(emm_wd_fs_df <- as.data.frame(emmeans(anova_model_words, ~  Family_Size )))

p2 <- emm_wd_fs_df |> ggplot(aes(y = emmean, x = Family_Size, colour = Family_Size, fill = Family_Size)) +
  geom_col( alpha = .4)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(550, 650)) +
  labs(title = "Family Size Effect (Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() 

plot_grid(p1, p2, ncol = 2, labels = "AUTO")
```

\newpage

# Non-word Data 

Use `complete.cases()` to find which rows had missing data in the model-relevant variables:

```{r, results = "hide"}
# Specify only the variables used in the model
# model_vars <- c("response_time", "Dim.2","SubjID")
model_vars_nw <- c("response_time","Complexity", "Family_Size", "Base_Frequency", 
                   "SubjID", "ItemID", "Semantic_Sensitivity")

# Identify incomplete rows 
incomplete_cases_nwords <- rt_nwords[!complete.cases(rt_nwords_frq[, model_vars_nw]), ]
rt_nwords_cmpl <- rt_nwords_frq[complete.cases(rt_nwords_frq[, model_vars_nw]), ]
# View them
print(incomplete_cases_nwords)

# str(rt_nwords_1_cmpl)
```

<br>

Standardize the predictors

<br>

```{r}
rt_nwords_cmpl$LogBF_std <- as.numeric(scale(rt_nwords_cmpl$LogBF, center = TRUE, scale = TRUE))
rt_nwords_cmpl$FS_std <- as.numeric(scale(rt_nwords_cmpl$FS, center = TRUE, scale = TRUE))
rt_nwords_cmpl$BF_std <- as.numeric(scale(rt_nwords_cmpl$BF, center = TRUE, scale = TRUE))
rt_nwords_cmpl$Dim.2_std <- as.numeric(scale(rt_nwords_cmpl$Dim.2, center = TRUE, scale = TRUE))
```

## Anova Family Size

<br>

```{r}

rt_nwords_cmpl %>%
  count(Complexity, Base_Frequency, Semantic_Sensitivity)

temp <- rt_nwords_cmpl |> filter(is.na(Complexity) & is.na(Base_Frequency))
# write_csv(temp, "temp.csv")
```

<br>

```{r}
anova_model_nwords_fs <- mixed(
  response_time ~ Complexity * Family_Size * Semantic_Sensitivity +
    (1 | SubjID) +
    (1 | ItemID),
  data = rt_nwords_cmpl,
  method = "S")
anova_model_nwords_fs

m2 <- anova_model_nwords_fs$full_model    # Extract the lmer model
ranova(m2) # Run random effects comparison
```

<br>

### Main Findings

|                                     Effect|          df |        F |p.value |
|-------------------------------------------|-------------|----------|--------|
|                                 Complexity|  1, 4528.29 |122.09 ***|   <.001|
|                                Family_Size|  1,   94.56 |  0.96    |    .329|
|                       Semantic_Sensitivity|  1,   63.44 |  0.00    |    .954|
|Complexity:Family_Size:Semantic_Sensitivity|  1, 4442.85 |  4.84   *|    .028|
	

### Interaction Effects

###  Simple Contrasts

Compare High vs Low Semantic Sensitivity within each combination of Family Size and Complexity  

This gives you: 4 contrasts: one for each `Family Size × Complexity` combination. Each shows whether `High` vs `Low Semantic Sensitivity` differs significantly

If simple effects aren’t significant, try looking at interaction contrasts, which test differences in the differences. You’re now asking: Does the effect of Sensitivity change more in some complexity/family combinations than others?

<br>

```{r}
# Estimated marginal means for the family_size × complexity interaction
(emm2 <- emmeans(anova_model_nwords_fs, ~ Semantic_Sensitivity * Family_Size * Complexity))

# Get all pairswise contrasts
emm2_contrasts <- contrast(emm2,  method = "pairwise", by = NULL, adjust = "none")
# emm2_contrasts 

# Keep only the contrasts you want
# Simple effects of family_size at each level of complexity
# Simple effects of complexity at each level of family_size
keep2 <- c("High Large Complex - High Large Simple",
          "High Small Complex - High Small Simple",
          "Low Large Complex - Low Large Simple",
          "Low Small Complex - Low Small Simple",
          "High Large Complex - High Small Complex",
          "High Large Simple - High Small Simple",
          "Low Large Complex - Low Small Complex",
          "Low Large Simple - Low Small Simple",
          "High Large Complex - Low Large Complex",
          "High Small Complex - Low Small Complex",
          "High Large Simple - Low Small Simple",
          "High Small Simple - Low Small Simple")

(emm2_contrasts_filtered <- subset(emm2_contrasts, contrast %in% keep2))

# Get Confidence Intervals
(emm2_contrasts_filtered_ci <- confint(emm2_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs2 <- eff_size(emm2, sigma = sigma(m2), edf = df.residual(m2))

# Remove the  redundant rows 
(effs2_filtered <- subset(effs2, contrast %in% keep2))
```

<br>

### Interaction Contrasts

The interaction contrast tests whether the difference in the complexity effect for large vs small families differs across sensitivity?


$$
 [ [(A_1 - A_2)\text{ in }B_1] - [(A_1 - A_2)\text{ in }B_2]
    \text{ in Condition }C_1]
  -
  [[(A_1 - A_2) in B_1] - [(A_1 - A_2) in B_2]
    \text{in Condition }C_2]
$$

<br>

```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare complexity effect in large vs small family)
contrast(emm2, interaction = "pairwise", by = NULL, adjust = "holm")
confint(contrast(emm2, interaction = c("pairwise", "pairwise")))

# Compute the A1 - A2 difference within each combination of B × C
(complexity_diff <- contrast(emm2, method = "revpairwise", 
                            by = c("Semantic_Sensitivity", "Family_Size"), 
                            simple = "Complexity"))

# Compute how that A-effect changes across the levels of B, separately for each level of C  
(family_size_complexity_int_within_sensitivity <- contrast(complexity_diff, 
                                                           method = "revpairwise",
                                                           by = "Semantic_Sensitivity", simple = "Family_Size"))

# Get confidence intervals
confint(family_size_complexity_int_within_sensitivity)
```

<br>

Responses were slower to *Complex* than *Simple* nonwords in every condition. The *Complexity effect* (`Complex - Simple`) varied with both *Family Size* and *Semantic Sensitivity*.

The Complexity effect (slower responses for complex vs. simple nonwords) is robust across all groups.

However, its magnitude varies:

  - Among **high-sensitivity participants**, the effect is larger for large families ($\approx 45$ ms) than small families ($\approx 27$ ms).

  - Among **low-sensitivity participants**, the pattern reverses slightly ($\approx 29$ ms vs. 38 ms).

<br>


The difference in the Complexity × Family Size interaction between high- and low-sensitivity participants is about 27 ms.

  - **High-sensitivity participants** showed a stronger complexity effect for large-family nonwords than for small-family ones.

  - **Low-sensitivity participants** showed the opposite or no difference.

This indicates that semantic sensitivity modulates how morphological family size influences the cost of morphological complexity in nonword processing. 

All groups show reliable complexity effects (complex slower than simple).
Only one cross-condition difference is significant: High-sensitivity participants respond faster to complex nonwords from small families than to complex nonwords from large-families.


Summary interpretation (for Results section):

Response times to morphologically complex nonwords were significantly slower than to simple nonwords, indicating greater processing cost for complexity. While overall family size and semantic sensitivity did not produce main effects, there was a significant `Complexity × Family Size × Semantic Sensitivity` interaction ($p = .028$).

Follow-up contrasts showed that for participants with high semantic sensitivity, the complexity effect was larger for large-family nonwords ($\approx 45$ ms) than for small-family nonwords ($\approx 27$ ms). In contrast, participants with low semantic sensitivity showed little difference or the reverse pattern. This suggests that individuals with greater semantic knowledge are more sensitive to morphological family size cues when processing novel morphological structures, showing amplified complexity costs when nonwords resemble rich morphological families.


### Plots

```{r, echo = FALSE, fig.height=3}
#### Main Effect of Complexity
# Compute means for each level of Complexity
(emm_nw_fs_cmp_df <- as.data.frame(emmeans(anova_model_nwords_fs, ~  Complexity )))

p3<- emm_nw_fs_cmp_df |> ggplot(aes(y = emmean, x = Complexity, 
                                   colour = Complexity, 
                                   fill = Complexity)) +
  geom_col( alpha = .4)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(650, 800)) +
  labs(title = "Complexity Effect (Non-Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() 

#### Main Effect of Semantic_Sensitivity
# Compute means for each level of Semantic_Sensitivity
(emm_nw_fs_os_df <- as.data.frame(emmeans(anova_model_nwords_fs, ~  Semantic_Sensitivity )))

p4 <- emm_nw_fs_os_df |> ggplot(aes(y = emmean, x = Semantic_Sensitivity, 
                              colour = Semantic_Sensitivity, 
                              fill = Semantic_Sensitivity)) +
  geom_col( alpha = .4)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(650, 800)) +
  labs(title = "Semantic Sensitivity Effect (Non-Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() 

plot_grid(p3, p4, ncol = 2)
```

\newpage

## Anova Base Frequency

<br>

```{r}
anova_model_nwords_bf <- mixed(
  response_time ~ Complexity * Base_Frequency * Semantic_Sensitivity +
    (1 | SubjID) +
    (1 | ItemID),
  data = rt_nwords_cmpl,
  method = "S")
anova_model_nwords_bf

m3 <- anova_model_nwords_bf$full_model    # Extract the lmer model
ranova(m3) # Run random effects comparison
```

<br>

### Main Findings

|                     Effect|          df |        F |p.value |
|---------------------------|-------------|----------|--------|
|                 Complexity|  1, 4533.26 |125.15 ***|   <.001|
|             Base_Frequency|  1,   95.24 | 12.70  **|   <.001|
|  Complexity:Base_Frequency|  1, 4535.47 |  3.92   *|    .048|


<br>

Participants responded more slowly to complex nonwords and to low-frequency-base nonwords.

<br>

###  Interaction Effects: `Complexity x Base_Frequency` 

#### Simple Contrasts

<br>

```{r}
# Estimated marginal means for the family_size × base frequency interaction
(emm1 <- emmeans(anova_model_nwords_bf, ~ Complexity * Base_Frequency))

# Get all pairswise contrasts
emm1_contrasts <- contrast(emm1,  method = "pairwise", by = NULL, adjust = "none")
emm1_contrasts

# Keep only the contrasts you want
# Simple effects of Complexity at each level of Base_Frequency
# Simple effects of Base_Frequency at each level of Complexity
keep <- c("Complex High - Simple High",
          "Complex Low - Simple Low",
          "Complex High - Complex Low",
          "Simple High - Simple Low")
(emm1_contrasts_filtered <- subset(emm1_contrasts, contrast %in% keep))

# Get Confidence Intervals
(emm1_contrasts_filtered_ci <- confint(emm1_contrasts_filtered))


# Get effect sizes
# Get all pairwise effect sizes
effs1 <- eff_size(emm1, sigma = sigma(m3), edf = df.residual(m3))
effs1

# Remove the two redundant rows (rows 3 and 4)
(effs1_filtered <- subset(effs1, !contrast %in% c("Complex High - Simple Low",
                                               "Simple High - Complex Low")))
```

<br>

#### Interaction Contrasts

<br>

```{r}
#  Interaction contrasts (difference-of-differences)
#    Compare base frequency effect in large vs small family)
contrast(emm1, interaction = "pairwise", by = NULL, adjust = "holm")

# Get confidence intervals, for each base frequency effect for each family size and then for interaction effect
confint(contrast(emmeans(m3, ~ Complexity | Base_Frequency), "pairwise"))
confint(contrast(emm1, interaction = c("pairwise", "pairwise")))
```

<br>

A small but reliable Complexity × Base Frequency interaction (p = .048) suggests that the complexity cost was smaller for nonwords derived from low-frequency bases.

Complexity|   Base Frequency | Mean RT (ms) |Interpretation               |
----------|------------------|--------------|-----------------------------|
Complex   |             High |        748   |slowest                      |
Simple    |             High |        707   |41 ms faster                 |
Complex   |             Low  |        721   |28 ms slower than Simple Low |
Simple    |             Low  |        692   |fastest                      |

<br>

Both complexity and base frequency affect RTs additively, but their combination reveals that high-frequency bases magnify the complexity cost.

  - The complexity effect (`Complex – Simple`) is larger for *high-frequency* bases (41 ms) than for *low-frequency* ones (29 ms).
  
  - The base-frequency advantage (`High – Low`) is stronger for *complex* items (27 ms) than for *simple* ones (15 ms).

  - Both effects are moderate in size (Cohen’s $d \approx 0.3–0.4$).

The complexity cost increases by about 12 ms when the base is high frequency rather than low frequency, confirming the small but significant interaction.

No effects involving Semantic Sensitivity were observed, indicating that this base-frequency modulation of complexity applies broadly across participants, independent of their semantic knowledge.

###  Main Effects Plots


```{r, echo = FALSE, fig.height=3}
#### Main Effect of Complexity
# Compute means for each level of Complexity
(emm_nw_bf_cmp_df <- as.data.frame(emmeans(anova_model_nwords_bf, ~  Complexity )))

p5<- emm_nw_bf_cmp_df |> ggplot(aes(y = emmean, x = Complexity, 
                                   colour = Complexity, 
                                   fill = Complexity)) +
  geom_col( alpha = .4, show.legend = FALSE)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(650, 800)) +
  labs(title = "Complexity Effect \n (Non-Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() + 
  theme(legend.position = "none")
# p5

#### Main Effect of Base_Frequency
# Compute means for each level of Base_Frequency
(emm_nw_bf_bf_df <- as.data.frame(emmeans(anova_model_nwords_bf, ~  Base_Frequency )))

p6<- emm_nw_bf_bf_df |> ggplot(aes(y = emmean, x = Base_Frequency, 
                                   colour = Base_Frequency, 
                                   fill = Base_Frequency)) +
  geom_col( alpha = .4, show.legend = FALSE)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(650, 800)) +
  labs(title = "Base Frequency Effect \n (Non-Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() + 
  theme(legend.position = "none")
# p6

#### Main Effect of Semantic_Sensitivity
# Compute means for each level of Semantic_Sensitivity
(emm_nw_bf_os_df <- as.data.frame(emmeans(anova_model_nwords_bf, ~  Semantic_Sensitivity )))

p7 <- emm_nw_fs_os_df |> ggplot(aes(y = emmean, x = Semantic_Sensitivity, 
                              colour = Semantic_Sensitivity, 
                              fill = Semantic_Sensitivity)) +
  geom_col( alpha = .4, show.legend = FALSE)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(650, 800)) +
  labs(title = "Semantic Sensitivity Effect \n (Non-Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() +
  theme(legend.position = "none")
# p7

plot_grid(p5, p6, p7, ncol = 3, labels = "AUTO")
```

###  Interaction Plots

```{r, fig.height=3}
p8 <- emmip(anova_model_nwords_bf, Complexity ~ Base_Frequency) + my_style
p9 <- emmip(anova_model_nwords_bf, Base_Frequency ~ Complexity) + my_style

plot_grid(p8, p8, ncol = 2)
```
