---
title: "M21 RT Orthographic Sensitivity"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  chunk_output_type: inline
---


\scriptsize
# Setup  {-}
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE, 
                      error = FALSE,
                      comment = "||",
                      fig.height = 4,
                      fig.width = 7)
options(width = 150)
```


Load libraries
```{r load_libraries, echo=FALSE}
library(tidyverse)
library(ggeffects)
library(lme4)
library(afex)
library(gridExtra)
library(emmeans)
library(effectsize)
library(performance)
library(cowplot)  # for use with `plot_grid(x,x,ncol = x)` function
library(e1071) # for use with `skewness()` function
library(lmerTest)
```


1. Set `ggplot2` parameters

<br>

```{r theme, echo = FALSE}
theme_set(theme_classic() +  
            theme(legend.position = "bottom", 
                  axis.text=element_text(size=8.5),
                  axis.title=element_text(size=9)))

my_theme <- theme(strip.text = element_text(size = 7),
                  axis.text.x = element_text(size = 7),
                  legend.text = element_text(size = 6),
                  legend.title = element_blank())

# Define a custom color palette
my_palette <- c("#A6CEE3",  "#FB9A99")
my_palette_2 <- c( "#1F78B4","#E31A1C" )
my_palette_3 <- c("#A6CEE3","#1F78B4","#FB9A99","#E31A1C")


# Create a function to apply this palette
scale_color_custom <- function() {
  scale_color_manual(values = my_palette_2)
}

scale_fill_custom <- function() {
  scale_fill_manual(values = my_palette_2)
}

# Combine theme and scales
my_style <- list(my_theme,scale_color_custom(),scale_fill_custom())
```

<br>

# Load Files and Format Files  {-}

## Load Files

<br>

```{r load_files}
#DIR <- "csv_files"
df_a <- read_csv("rt_data_hc_A.csv")
df_b <- read_csv( "rt_data_hc_B_fixed.csv")
frq_w <- read_csv("frq_cw.csv")
frq_nw <- read_csv("frq_nw.csv")
dmg <- read_csv("demo_lang_vsl_pca_hc.csv")
```

<br>

## Format Files

<br>

```{r format_files}
# Concatenate datasets
rt <- bind_rows(AB = df_a, 
                BA = df_b,
               .id = "List")
rt_dmg<- right_join(dmg, rt, join_by(SubjID == subject_nr)) |>  # Join Participant Demographic and Lang Data
                    mutate(target = tolower(target)) |>
                    filter(correct == 1)

# Divide into Experimental and Filler Items
rt_fill <- rt_dmg |> filter(str_detect(targ_type, "^FILL"))
rt_exp <- rt_dmg |> filter(!str_detect(targ_type, "^FILL"))

# Add logFS to frequency datasets
frq_w <- frq_w |> mutate(Log10FS = log10(FS))
frq_nw <- frq_nw |> mutate(Log10FS = log10(FS))

# Define Factors and Conditions
rt_exp_format <- rt_exp |>
  separate(targ_type, into = c("trial_type", "family_size", "complexity"), sep = "_",
           remove = TRUE, extra = "drop", fill = "right") 

# Divide into Words and Nonwords
rt_words <- rt_exp_format |> filter(trial_type == "CW") |> select(- complexity)
rt_nwords <- rt_exp_format |> filter(trial_type == "NW")

# Join Stimulus Frequency Data
rt_words_frq <- left_join(rt_words, frq_w, join_by(target))|>
  select(-cond_trig.y, -word_trig.y) |>
  rename(cond_trig = cond_trig.x, word_trig = word_trig.x)  # remove duplicate columns
rt_nwords_frq <-  left_join(rt_nwords, frq_nw, join_by(target==word)) |>   
  select(-cond_trig.y, -word_trig.y) |>
  rename(cond_trig = cond_trig.x, word_trig = word_trig.x)  

# Rename BF_Split and FS_Split columns
rt_words_frq <- rt_words_frq |> rename(Base_Frequency = BF_Split, Family_Size = FS_Split) # Rename BF_Split and FS_Split columns
rt_nwords_frq <- rt_nwords_frq |> rename(Base_Frequency = BF_Split, Family_Size = FS_Split)

# Recode factor levels
# rt_words_frq <- rt_words_frq |>
#   mutate(Base_Frequency = case_match(Base_Frequency, "Low" ~ "Low BF", "High" ~ "High BF"),
#          Family_Size = case_match(Family_Size, "Small" ~ "Small Family", "Large" ~ "Large Family"))
# rt_nwords_frq <- rt_nwords_frq |> mutate(Base_Frequency = case_match(Base_Frequency, "Low" ~ "Low BF", "High" ~ "High BF"),
#                                          Family_Size = case_match(Family_Size, "Small" ~ "Small Family", "Large" ~ "Large Family"))
# 
# rt_words_frq$Orthographic_Sensitivity[rt_words_frq$Orthographic_Sensitivity == "Low"] <- "Low Sensitivity"
# rt_words_frq$Orthographic_Sensitivity[rt_words_frq$Orthographic_Sensitivity == "High"] <- "High Sensitivity"
```

<br>

<br>

<br>

# Word Data

Use `complete.cases()` to find which rows have missing data in the model-relevant variables:

<br>

```{r remove_NA_words}
# Specify only the variables used in the model
model_vars_w <- c("response_time",  "Log10BF","BF", "FS","Family_Size", "Base_Frequency", "Orthographic_Sensitivity","SubjID")

# Identify incomplete rows cohort 1
incomplete_cases_words <- rt_words_frq[!complete.cases(rt_words_frq[, model_vars_w]), ]
rt_words_cmpl <- rt_words_frq[complete.cases(rt_words_frq[, model_vars_w]), ]
# View them
# print(incomplete_cases_words)
```

```{r standardize_predictors_words}
# Standardize the predictors
rt_words_cmpl$Log10BF_std <- as.numeric(scale(rt_words_cmpl$Log10BF, center = TRUE, scale = TRUE))
rt_words_cmpl$FS_std <- as.numeric(scale(rt_words_cmpl$FS, center = TRUE, scale = TRUE))
rt_words_cmpl$Log10WF_std <- as.numeric(scale(rt_words_cmpl$Log10WF, center = TRUE, scale = TRUE))
rt_words_cmpl$Log10FS_std <- as.numeric(scale(rt_words_cmpl$Log10FS, center = TRUE, scale = TRUE))
rt_words_cmpl$Dim.2_std <- as.numeric(scale(rt_words_cmpl$Dim.2, center = TRUE, scale = TRUE))
```

<br>

```{r get_condition_counts, echo=FALSE, results = "hide"}
rt_words_cmpl %>%
  summarise(
    n_subjects = n_distinct(SubjID),
    n_items = n_distinct(STRING))

# Count trials per subject
rt_words_cmpl %>%
  count(SubjID, name = "n_trials") %>%
  summarise(
    min_trials = min(n_trials),
    max_trials = max(n_trials),
    mean_trials = mean(n_trials))

(trial_count_by_subj <- rt_words_cmpl %>%
  count(SubjID, name = "n_trials") %>%
  arrange(desc(n_trials)))

rt_words_cmpl %>%
  count(Family_Size, Base_Frequency, Orthographic_Sensitivity)


```

<br>

### Anova 

<br>

```{r anova_words}
anova_model_words <- mixed(
  response_time ~ Base_Frequency * Family_Size * Orthographic_Sensitivity +
    (1 + Base_Frequency + Family_Size | SubjID) +
    (1 | STRING),
  data = rt_words_cmpl,
  method = "S")
anova_model_words

m1 <- anova_model_words$full_model    # Extract the lmer model 
ranova(m1) # formally test whether adding each random effect improves fit

# Extract effect sizes from your ANOVA model
eta_squared(anova_model_words, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(m1)
```

&nbsp;

### Main Effects

|                        Effect|       df |      F |p.value|
|------------------------------|----------|--------|-------|
|                Base_Frequency| 1, 93.84 |10.22 **|   .002|
|                   Family_Size| 1, 92.69 | 9.12 **|   .003|

#### Means


```{r words_means}
emmeans(anova_model_words, ~ Family_Size)
emmeans(anova_model_words, ~ Base_Frequency)
emmeans(anova_model_words, ~ Orthographic_Sensitivity)
```


### Plots

<br>

```{r words_main_effects_plots, echo = FALSE, fig.height=3.5, fig.width=8}
#### Main Effect of Base Frequency
# Compute means for each level of Base_Frequency
(emm_wd_bf_df <- as.data.frame(emmeans(anova_model_words, ~  Base_Frequency )))

p1<- emm_wd_bf_df |> ggplot(aes(y = emmean, x = Base_Frequency, colour = Base_Frequency, fill = Base_Frequency)) +
  geom_col( alpha = .4)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(550, 650)) +
  labs(title = "Base Frequency Effect (Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() 

#### Main Effect of Family Size
# Compute means for each level of Base_Frequency
(emm_wd_fs_df <- as.data.frame(emmeans(anova_model_words, ~  Family_Size )))

p2 <- emm_wd_fs_df |> ggplot(aes(y = emmean, x = Family_Size, colour = Family_Size, fill = Family_Size)) +
  geom_col( alpha = .4)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(550, 650)) +
  labs(title = "Family Size Effect (Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() 

plot_grid(p1, p2, ncol = 2, labels = "AUTO")
```

\newpage

# Non-word Data 

Use `complete.cases()` to find which rows had missing data in the model-relevant variables:

<br>

```{r remove_NA_nwords, echo=FALSE, results = "hide"}
# Specify only the variables used in the model
# model_vars <- c("response_time", "Dim.2","SubjID")
model_vars_nw <- c("response_time","Complexity", "Family_Size", "Base_Frequency", 
                   "SubjID", "ItemID", "Orthographic_Sensitivity")

# Identify incomplete rows 
incomplete_cases_nwords <- rt_nwords[!complete.cases(rt_nwords_frq[, model_vars_nw]), ]
rt_nwords_cmpl <- rt_nwords_frq[complete.cases(rt_nwords_frq[, model_vars_nw]), ]
# View them
print(incomplete_cases_nwords)

# str(rt_nwords_1_cmpl)
```

```{r standardize_predictors_nwords}
### Standardize the predictors

rt_nwords_cmpl$LogBF_std <- as.numeric(scale(rt_nwords_cmpl$LogBF, center = TRUE, scale = TRUE))
rt_nwords_cmpl$FS_std <- as.numeric(scale(rt_nwords_cmpl$FS, center = TRUE, scale = TRUE))
rt_nwords_cmpl$BF_std <- as.numeric(scale(rt_nwords_cmpl$BF, center = TRUE, scale = TRUE))
rt_nwords_cmpl$Dim.2_std <- as.numeric(scale(rt_nwords_cmpl$Dim.2, center = TRUE, scale = TRUE))
```

<br>

## Anova Family Size

<br>

```{r anova_nwords_fs}
anova_model_nwords_fs <- mixed(
  response_time ~ Complexity * Family_Size * Orthographic_Sensitivity +
    (1 + Complexity + Family_Size | SubjID) +
    (1 | ItemID),
  data = rt_nwords_cmpl,
  method = "S")
anova_model_nwords_fs

m2 <- anova_model_nwords_fs$full_model    # Extract the lmer model
ranova(m2) # Run random effects comparison

# Extract effect sizes from your ANOVA model
eta_squared(anova_model_nwords_fs, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_nwords_fs)
```

<br>

### Main Effects

|                     Effect|          df |        F |p.value |
|---------------------------|-------------|----------|--------|
|                 Complexity|  1,   61.93 | 89.98 ***|   <.001|
|   Orthographic_Sensitivity|  1,   63.58 |  5.33   *|    .024|
	
	
#### Main Effects Means
```{r nwords_fs_means}
emmeans(anova_model_nwords_fs, ~ Complexity)
emmeans(anova_model_nwords_fs, ~ Family_Size)
emmeans(anova_model_nwords_fs, ~ Orthographic_Sensitivity)
```
	

**Non-word complexity** had a robust effect; complex non-words (e.g., pseudoderived forms) elicited longer response times than simple ones. Participants with higher **orthographic sensitivity** responded significantly faster overall, suggesting more efficient processing of letter patterns even in non-words. **Morphological family size** did not modulate non-word RTs, nor did it interact with complexity or orthographic sensitivity.  Interpretation: In the absence of lexical representations, apparent “family size” (based on real-word analogues) does not measurably influence non-word recognition.

#### Main Effects Plots

...

```{r nwords_fs_main_effects_plots, echo = FALSE, fig.height=3.5, fig.width=8}
#### Main Effect of Complexity
# Compute means for each level of Complexity
emm_nw_fs_cmp_df <- as.data.frame(emmeans(anova_model_nwords_fs, ~  Complexity ))

p3<- emm_nw_fs_cmp_df |> ggplot(aes(y = emmean, x = Complexity, 
                                   colour = Complexity, 
                                   fill = Complexity)) +
  geom_col( alpha = .4)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(650, 800)) +
  labs(title = "Complexity Effect (Non-Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() 

#### Main Effect of Orthographic_Sensitivity
# Compute means for each level of Orthographic_Sensitivity
emm_nw_fs_os_df <- as.data.frame(emmeans(anova_model_nwords_fs, ~  Orthographic_Sensitivity ))

p4 <- emm_nw_fs_os_df |> ggplot(aes(y = emmean, x = Orthographic_Sensitivity, 
                              colour = Orthographic_Sensitivity, 
                              fill = Orthographic_Sensitivity)) +
  geom_col( alpha = .4)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(650, 800)) +
  labs(title = "Orthographic Sensitivity Effect (Non-Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() 

plot_grid(p3, p4, ncol = 2)
```

\newpage

## Anova Base Frequency

<br>

```{r anova_nwords_bf}
anova_model_nwords_bf <- mixed(
  response_time ~ Complexity * Base_Frequency * Orthographic_Sensitivity +
    (1 + Base_Frequency + Complexity | SubjID) +
    (1 | ItemID),
  data = rt_nwords_cmpl,
  method = "S")
anova_model_nwords_bf

m3 <- anova_model_nwords_bf$full_model    # Extract the lmer model
ranova(m3) # Run random effects comparison

# Extract effect sizes from your ANOVA model
eta_squared(anova_model_nwords_bf, partial = TRUE)

# Compute Marginal(fixed effects only) and Conditional(fixed + random effects) R²
r2(anova_model_nwords_bf)
```

<br>

### Main Findings

|                     Effect|          df |        F |p.value |
|---------------------------|-------------|----------|--------|
|                 Complexity|  1,   60.94 | 90.90 ***|   <.001|
|             Base_Frequency|  1,   93.14 | 11.47  **|   <.001|
|   Orthographic_Sensitivity|  1,   63.60 |  5.24   *|    .025|
|  Complexity:Base_Frequency|  1,  425.13 |  3.40    |    .066|

- Complexity: complex > simple non-words → slower responses.

- Base Frequency (: non-words derived from high-frequency bases were processed faster than those from low-frequency bases — an echo of lexical familiarity effects even though the items are illegal.

- Orthographic Sensitivity : same direction as before.

- Complexity × Base Frequency (marginal effect): The effect of complexity was larger for high-frequency bases than for low-frequency ones.


```{r means_nwords_bf}
emmeans(anova_model_nwords_bf, ~ Complexity)
emmeans(anova_model_nwords_bf, ~ Base_Frequency)
emmeans(anova_model_nwords_bf, ~ Orthographic_Sensitivity)
```

###  Interaction Effects: `Complexity x Base_Frequency` 

#### Simple Contrasts

<br>

```{r nwords_bf_simple_contrasts}
# Estimated marginal means for the family_size × base frequency interaction
(emm1 <- emmeans(anova_model_nwords_bf, ~ Complexity * Base_Frequency))

# Get all pairswise contrasts
emm1_contrasts <- contrast(emm1,  method = "pairwise", by = NULL, adjust = "none")
emm1_contrasts

# Keep only the contrasts you want
# Simple effects of Complexity at each level of Base_Frequency
# Simple effects of Base_Frequency at each level of Complexity
keep <- c("Complex High - Simple High",
          "Complex Low - Simple Low",
          "Complex High - Complex Low",
          "Simple High - Simple Low")
(emm1_contrasts_filtered <- subset(emm1_contrasts, contrast %in% keep))
```

###  Main Effects Plots

<br>

```{r nwords_bf_main_effects_plots, echo = FALSE, fig.height=3.5, fig.width=8}
#### Main Effect of Complexity
# Compute means for each level of Complexity
(emm_nw_bf_cmp_df <- as.data.frame(emmeans(anova_model_nwords_bf, ~  Complexity )))

p5<- emm_nw_bf_cmp_df |> ggplot(aes(y = emmean, x = Complexity, 
                                   colour = Complexity, 
                                   fill = Complexity)) +
  geom_col( alpha = .4, show.legend = FALSE)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(650, 800)) +
  labs(title = "Complexity Effect \n (Non-Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() + 
  theme(legend.position = "none")
# p5

#### Main Effect of Base_Frequency
# Compute means for each level of Base_Frequency
(emm_nw_bf_bf_df <- as.data.frame(emmeans(anova_model_nwords_bf, ~  Base_Frequency )))

p6<- emm_nw_bf_bf_df |> ggplot(aes(y = emmean, x = Base_Frequency, 
                                   colour = Base_Frequency, 
                                   fill = Base_Frequency)) +
  geom_col( alpha = .4, show.legend = FALSE)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(650, 800)) +
  labs(title = "Base Frequency Effect \n (Non-Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() + 
  theme(legend.position = "none")
# p6

#### Main Effect of Orthographic_Sensitivity
# Compute means for each level of Orthographic_Sensitivity
(emm_nw_bf_os_df <- as.data.frame(emmeans(anova_model_nwords_bf, ~  Orthographic_Sensitivity )))

p7 <- emm_nw_fs_os_df |> ggplot(aes(y = emmean, x = Orthographic_Sensitivity, 
                              colour = Orthographic_Sensitivity, 
                              fill = Orthographic_Sensitivity)) +
  geom_col( alpha = .4, show.legend = FALSE)  +
  geom_errorbar(aes(ymin = emmean - SE, ymax = emmean + SE),
                width = 0.1, position = position_dodge(0.2)) +
  coord_cartesian(ylim = c(650, 800)) +
  labs(title = "Orthographic Sensitivity Effect \n (Non-Words)", y = "RT in milliseconds") +
  scale_color_custom() +
  scale_fill_custom() +
  theme(legend.position = "none")
# p7

plot_grid(p5, p6, p7, ncol = 3, labels = "AUTO")
```

###  Interaction Plots

```{r nwords_bf_interaction_plots, fig.height=3, fig.width=8, echo=FALSE}
p8 <- emmip(anova_model_nwords_bf, Complexity ~ Base_Frequency) + my_style
p9 <- emmip(anova_model_nwords_bf, Base_Frequency ~ Complexity) + my_style

plot_grid(p8, p9, ncol = 2)
```
