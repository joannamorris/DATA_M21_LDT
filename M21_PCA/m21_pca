---
title: "m21_pca"
author: "Joanna Morris"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This script computes the PCA for  Morph21.  


1. First we load the libraries we need

```{r}
library(readr)
library(psych)
library(dplyr)
library(tidyr)
```



# Compute PCA

Following Andrews and Lo (2013) this script computes a PCA for our spelling and vocabulary measures. Because the standardised spelling and vocabulary scores were  correlated, to facilitate interpretation, two orthogonal measures of individual differences were derived from a principal components analysis. Analysis based on [this tutorial](http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/)

First we import the data, remove missing values adn standardize the scores.

```{r}
library(readr)
library(dplyr)
library(datawizard)
langdat_1_202410 <- read_csv("m21_langdat_1.csv")
langdat_2_202410 <- read_csv("m21_langdat_2.csv")

langdat_1.na <- na.omit(langdat_1_202410)
langdat_2.na <- na.omit(langdat_2_202410)

describe(langdat_1.na)
describe(langdat_2.na)

langdat_1.na <- mutate(langdat_1.na, 
                       z_ART = standardise(ART), 
                       z_Vocabulary = standardise(Vocabulary), 
                       z_Spelling = standardise(Spelling))

langdat_2.na <- mutate(langdat_2.na, 
                       z_ART = standardise(ART), 
                       z_Vocabulary = standardise(Vocabulary), 
                       z_Spelling = standardise(Spelling))
```

Now we can put the three standardized measures into a separate data frame and compute the correlations, using the `cor()` function.  NB.  A correlation coefficient is a standardized covariance statistic. We can run the `cov()` function on the standardized values or the `cor()` function on the unstandardized ones.  Both methods will give the same results.

```{r}

art_vcb_spl_raw_1 <- langdat_1.na |> select(Vocabulary, Spelling, ART)
art_vcb_spl_z_1 <- langdat_1.na |> select( z_Vocabulary, z_Spelling, z_ART) 

cor(art_vcb_spl_raw_1, use = "everything", method = "pearson") 
cov(art_vcb_spl_z_1, use = "everything", method = "pearson") 

art_vcb_spl_raw_2 <- langdat_2.na |> select(Vocabulary, Spelling, ART)
art_vcb_spl_z_2 <- langdat_2.na |> select( z_Vocabulary, z_Spelling, z_ART) 

cor(art_vcb_spl_raw_2, use = "everything", method = "pearson") 
cov(art_vcb_spl_z_2, use = "everything", method = "pearson") 

```

Once we have generated the correlation coefficients we can test them for statistical significance.  You can only test one correlation at a time using the `cor.test()` function, but the `corr.test()` function in the `psych` package will test a matrix of correlation coefficients.

```{r}
library(psych)
corr.test(art_vcb_spl_z_1) 
corr.test(art_vcb_spl_z_2) 
```

Now we can do the PCA.  It turns out that by default, the function `PCA()` in `FactoMineR`, standardizes the data automatically, so we didn't actually need do the standardization. Oh well. ¯\\\_(ツ)\_/¯

Here are the arguments to the `PCA()` function:

- `X`: a data frame. Rows are individuals and columns are numeric variables

- `scale.unit`: a logical value. If TRUE, the data are scaled to unit variance before the analysis. This standardization to the same scale avoids some variables to become dominant just because of their large measurement units. It makes variables comparable.

- `ncp`: number of dimensions kept in the final results.

- `graph`: a logical value. If TRUE a graph is displayed.

The plot shows the relationships between all variables. It can be interpreted as follow:

- Positively correlated variables are grouped together.

- Negatively correlated variables are positioned on opposite sides of the plot origin (opposed quadrants).

- The distance between variables and the origin measures the quality of the variables on the factor map. Variables that are away from the origin are well represented on the factor map.

```{r}

library(FactoMineR)
library(factoextra)

res.pca_1 <- PCA(langdat_1.na[,2:4], scale.unit = TRUE, ncp = 2, graph = FALSE)
plot(res.pca_1, choix = "varcor", graph.type = c("ggplot"))

res.pca_2 <- PCA(langdat_2.na[,2:4], scale.unit = TRUE, ncp = 2, graph = FALSE)
plot(res.pca_2, choix = "varcor", graph.type = c("ggplot"))
```

The eigenvalues measure the amount of variation retained by each principal component. Eigenvalues are large for the first PCs and small for the subsequent PCs. That is, the first PCs corresponds to the directions with the maximum amount of variation in the data set.

We examine the eigenvalues to determine the number of principal components to be considered. The sum of all the eigenvalues give a total variance of 3, the number of variables. An eigenvalue > 1 indicates that PCs account for more variance than accounted by one of the original variables in standardized data. This is commonly used as a cutoff point for which PCs are retained. This holds true only when the data are standardized.

```{r}
(eig.val_1 <- get_eigenvalue(res.pca_1))
(eig.val_2 <- get_eigenvalue(res.pca_2))

```


The quality of representation of the variables on factor map is called cos2 (square cosine, squared coordinates). *A high cos2 indicates a good representation of the variable on the principal component*. In this case the variable is positioned close to the circumference of the correlation circle.  *A low cos2 indicates that the variable is not perfectly represented by the PCs.* In this case the variable is close to the center of the circle. If a variable is perfectly represented by only two principal components (Dim.1 & Dim.2), the sum of the cos2 on these two PCs is equal to one. In this case the variables will be positioned on the circle of correlations.

```{r c4}
res.pca_1$var$cos2
res.pca_2$var$cos2
```

The contributions of variables in accounting for the variability in a given principal component are expressed in percentages. Variables that are correlated with PC1 (i.e., Dim.1) and PC2 (i.e., Dim.2) are the most important in explaining the variability in the data set. The larger the value of the contribution, the more the variable contributes to the component. It’s possible to use the function corrplot() [corrplot package] to highlight the most contributing variables for each dimension.

```{r}
library('corrplot')

res.pca_1$var$contrib
res.pca_2$var$contrib
corrplot(res.pca_1$var$contrib, is.corr=FALSE) 
corrplot(res.pca_2$var$contrib, is.corr=FALSE) 
```


The correlation between a variable and a principal component (PC) is used as the coordinates of the variable on the PC.

```{r}
(res.pca_1$var$coord)
(res.desc <- dimdesc(res.pca_1, axes = c(1,2), proba = 0.05))

(res.pca_2$var$coord)
(res.desc <- dimdesc(res.pca_2, axes = c(1,2), proba = 0.05))
```

The fviz_pca_ind() is used to produce the graph of individuals.

```{r c6}
ind.1 <- get_pca_ind(res.pca_1)
fviz_pca_ind(res.pca_1)

ind.2 <- get_pca_ind(res.pca_2)
fviz_pca_ind(res.pca_2)
```

```{r c7}
langdat_1.na<-bind_cols(langdat_1.na,res.pca_1$ind$coord)
langdat_2.na<-bind_cols(langdat_2.na,res.pca_2$ind$coord)

#Divide participants based on median split of Dim2.  Higher values on this factor indicate that spelling scores were relatively higher than vocabulary, 

langdat_1.na <- langdat_1.na |>
  mutate(lang_type_ortho = case_when(
    Dim.2 <= 0 ~ "Low Orthographic",
    Dim.2 > 0 ~ "High Orthographic"
  ))
langdat_1.na <- langdat_1.na |>
  mutate(lang_type_semantic = case_when(
    Dim.1 <= 0 ~ "Low Semantic",
    Dim.1 > 0 ~ "High Semantic"
  ))

langdat_2.na <- langdat_2.na |>
  mutate(lang_type_ortho = case_when(
    Dim.2 <= 0 ~ "Low Orthographic",
    Dim.2 > 0 ~ "High Orthographic"
  ))
langdat_2.na <- langdat_2.na |>
  mutate(lang_type_semantic = case_when(
    Dim.1 <= 0 ~ "Low Semantic",
    Dim.1 > 0 ~ "High Semantic"
  ))
```


We can then write the individual pca values to a file

```{r}
write_csv(langdat_1.na, "m21_langdat_1_pca.csv")
write_csv(langdat_2.na, "m21_langdat_2_pca.csv")
```


